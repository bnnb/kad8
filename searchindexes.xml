<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<search>
  
  <entry>
    <title>欢迎加入 VxWorks 俱乐部！</title>
    <url>/post/welcome-to-vxworks-club/</url>
    <categories><category>Announce</category>
    </categories>
    <tags>
      <tag>VxWorks</tag>
      <tag>WindRiver</tag>
    </tags>
    <content type="html"><![CDATA[欢迎来到 北南南北 文档站点！ 相关文章来源于 VxWorks 俱乐部  ，也可能发布于 AI 嵌入式开发  ，专注于技术分享和交流。
免责声明 所有资源均来自网络，版权归原作者，如有侵权，请联系删除！
欢迎投稿  欢迎广大网友投稿 欢迎加入网友微信群  ]]></content>
  </entry>
  
  <entry>
    <title>GPU市场分析：全球视野下的竞争格局与未来发展</title>
    <url>/post/datacenter/gpu-competitive-landscape-and-future-development-from-a-global-perspective.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>SR-IO</tag>
      <tag>GPU</tag>
    </tags>
    <content type="html"><![CDATA[谈到计算机与GPU的关系，就不得不提到IBM公司在1981年发布的世界上第一台个人电脑IBM5150，该电脑配备了黑白显示适配器和彩色图形适配器，这是最早的图形显示控制器。
前言 谈到计算机与GPU的关系，就不得不提到IBM公司在1981年发布的世界上第一台个人电脑IBM5150，该电脑配备了黑白显示适配器和彩色图形适配器，这是最早的图形显示控制器。
在20世纪80年代初期，以GE芯片为代表的图形处理器开始出现，它具备四位向量的浮点运算功能，可以完成图形渲染过程中的矩阵、裁剪、投影等运算，标志着计算机图形学进入以图形处理器为主导的阶段。随着GE等图形处理器功能的不断完善，图形处理功能也逐渐从CPU向GPU（前身）转移。随着时间进入上世纪90年代，如今GPU的王者英伟达进入个人电脑3D市场，并于1999年推出具有标志意义的图形处理器GeForce256，真正意义上的GPU第一次出现。
与以往的图形处理器相比GeForce256将T&amp;L硬件从CPU中分离，并将其单独组合成硬件，这种创新使得GPU可以独立进行三维顶点的空间坐标变换，从而将CPU从繁重的光照计算中解放出来。这意味着，即使是低端CPU，如果搭配了支持硬件T&amp;L的显卡，也可以流畅地玩游戏。这一技术革新使得NVIDIA在市场竞争中获得了较大的优势，并且其市占率也持续提升。
当今全球GPU市场分析 目前，全球局势动荡，前几年新冠疫情席卷全球，整个世界逐渐转入“线上”模式，随着家用电脑需求量的增加和因疫情等原因带来的芯片紧缺，加之中美冲突，当下北约内部矛盾，一度激化上演全武行至今也不见降温迹象。再到如今的后疫情时代，大语言模型AI的火爆，加之汽车智能化浪潮和智能驾驶对于AI算力的需求，一下子就把GPU推到了世界半导体硬件的舞台中央。
就当下而言，GPU应用市场可划分为三大应用领域分别为：PC市场（游戏）、AI&amp;数据中心和智能汽车。
首先，我们先来看看GPU的主战场，也就是PC领域。
根据Jon Peddie Research的数据，2023年的整体出货量包括NVIDIA、AMD和英特尔的出货量都呈现大幅下降的情况。根据该报告，2023年第一季GPU的总出货量达到了630万片，但与上一季相比下降了12.6%，与2022年同期相比则下降了38.2%。其中，2023年第一季出货量相较上一季下降了12.6%，低于 -4.9%的10年平均水平。而2023年的第二季度情况则更让人瞩目，GPU市场规模达到了6160万美元，但同比下降了惊人的23%。究其原因，笔者认为一是后疫情时代消费者普遍预算吃紧，消费意愿较低；二是NVIDIA和AMD都有较旧型号的库存，可用于吸引部分主流和预算型市场的游戏玩家，加上英特尔的高性比显卡的入局；三是虽然新的显卡已经推出，但在性价比明显不佳情况下，消费者对它们的兴趣不大，更多的消费者宁可选择前代3090系列或是英特尔的产品。
尽管整体趋势呈下降态势，但该报告还是给出了GPU市场乐观的预测：2022年至2026年期间，PC用GPU的复合年增长率将稳定在3.70%，并有望在2026年年末达到总规模2998万片。与此同时，JPR认为PC用独立显卡(DGPU)的渗透率有望增长到32%，为市场带来新的发展契机。
其次，我再来关注一下GPU拥有巨大潜力的市场，那便是2023年初被推上风口的AI&amp;数据中心领域。
在数据中心，GPU被广泛应用于人工智能的训练、推理、高性能计算（HPC）等领域。以ChatGPT、文心一言等为代表的生成式大语言模型AI的涌现，其展现出了高度的拟人化和智能化，背后离不开利用GPU进行的海量基础训练，自GPT-3模型推出后，大规模自然语言模型进入了拥有千亿个参数的时代。在2021年之后，出现了许多达到千亿级别的自然语言模型，这些模型的训练算力显著增加。例如，ChatGPT模型拥有1.75亿个参数，其训练所需的算力为3.14x10^23次浮点运算。
当前，各种预训练语言模型仍在不断快速更新和迭代，不断刷新自然语言处理任务的最佳表现记录。同时，单个模型的训练算力需求也不断创新高。以ChatGPT的训练为例，单次模型训练就可能需要超过2000张英伟达A100显卡持续不断地训练27天。不仅如此，大模型的日常运行中每一次用户调用都需要一定的算力和带宽作为支撑。还是以ChatGPT为例，近期，ChatGPT官网每日吸引的访客数量接近5000万，每小时平均访问人数约为210万人。在高峰时期，同时在线人数达到450万人。假设每个人在一小时内提问8个问题，每个问题的回答长度为200个字，那么需要使用14,000块英伟达A100芯片来提供日常算力支持。大模型在融入搜索引擎或以app形式提供其他商业化服务过程中，其AI芯片需求将得到进一步的显著拉动。
根据相关数据显示，2023年全球AI芯片市场规模达到了1500亿美元，预计到2028年将达到2500亿美元。全球范围内，企业在人工智能市场的技术投资从2019年的612.4亿美元增长至2021年的924.0亿美元，2022年同比增长26.6%至1,170.0亿美元，并有望到2025年突破2,000亿美元，增幅高于企业数字化转型（DX）支出整体增幅。
在数据中心领域，则是GPU市场保持持续增长的另一个强劲支柱，根据Omdia数据，2019年全球人工智能服务器市场规模为23亿美金，2026年将达到376亿美金，CAGR为49%。而根据IDC的数据显示，2020年中国数据中心用于人工智能推理的芯片的市场份额已经超过50%，预计到2025年，用于人工智能推理工作负载的芯片将达到60.8%。人工智能服务器在搭建时，一般选用CPU+各种加速处理器的组合。而常用的加速处理器有FPGA、ASIC、GPU、NPU等。而就目前来看，GPU凭借其出色的深度学习能力、并行计算能力还有其不俗的通用性，在数据中心加速芯片市场中占比一骑绝尘，超过90%的AI服务器将GPU作为了加速芯片。因此相关数据指出，在2026年AI服务器市场规模将达到376亿美元，随着AI服务器迅速增长GPU在这一部分市场也将迎来需求的大爆发。
最后，我们来看一下GPU在汽车电子中的应用情况。
在如今新能源汽车崛起高速增长的今天，随着汽车智能化浪潮的到来，自动驾驶和智能座舱是最具有发展前景的两个方向。
在自动驾驶领域，使用的芯片不计其数，各个类型的芯片都有涉及，根据Yole数据，全球自动驾驶市场2025年将达到780亿美金，其中用于自动驾驶的AI芯片超过100亿美元。在这之中GPU在自动驾驶的渗透率随着自动驾驶的等级的提升而提升。根据ICVTank的自动驾驶渗透数据：假设GPU在L2中的渗透率达到15%，在L3-L5中的渗透率达到50%，那么可以估算出GPU在自动驾驶领域的市场规模。整体规模预计将从2020年的7.1亿美元增长到2025年的44亿美元，年复合增长率为44%。
在智能座舱领域，独立GPU往往不会被应用，往往是以SoC的形式以集成GPU的形态被应用的。在一片完整的SoC中一般由GPU、CPU、AI引擎、DPU等组成。而目前的智能座舱往往向着一芯多屏的方向发展，这就是GPU部分在智能座舱中有着无可替代且越来越重要的优势。
国内外GPU相关企业概况 谈到GPU方面的市场份额，那么就不得不提目前几乎是一家独大的英伟达。在消费级独立PC市场上，根据 Jon Peddie Research 调查数据显示，2023 年第一季度，全球桌面级显卡销量约为 630 万块，其中英伟达显卡销量约为 529 万张，以 84%的市场份额占据领先地位，此外销量位居前三的 GPU 供应商还包括 AMD 及 Intel，其销量分别为 76 万张、25 万张。而国内方面则在消费级独立GPU上起步较晚，我国国产GPU厂商市场份额普遍较小，但也不乏如芯动科技、摩尔线程等勇于挑战国外厂商垄断地位的国内GPU新兴厂商，未来随着国内数据中心、智能驾驶及终端侧 GPU 市场需求的提升，国产 GPU 市场份额有望实现渗透。
而在盈利能力方面，英伟达凭借其绝对的市场地位遥遥领先，根据权威机构Jon Peddie Research GPU市场统计数据，2023年第二季度英伟达以68%的PC显卡市场份额位居市场第一。与此相对，2024财年第二季度游戏业务实现了24.9亿美元的营收，同比增长了22%。而老对手AMD的游戏业务则以15.81亿美元，同比下降4%，环比下降了10%，营业利润2.25亿美元的成绩紧随其后。
在数据中心市场，英伟达依旧是处于领导地位，根据研究机构TrendForce的预测，目前数据中心市场上，英伟达的GPU已成为主流芯片，市场份额约占60-70%。而国内在该领域布局者较多，产品性能逐渐向英伟达靠拢。比如，目前国内算力最高的产品是由壁仞科技推出的BR100P，其采用了台积电7nm工艺。在峰值状态下，它具有单精度浮点算力240TFLOPS和整型算力1920TOPS。BR100芯片的性能比英伟达的A100高出3倍以上，虽说对比英伟达的旗舰产品H100还有不小的差距，但是依旧未来可期。
在盈利方面，据英伟达 2024 财年 Q2 报告，其数据中心营收已达到 103.2 亿美元，同比增长171%，约占总营收比例为 76%，目前数据中心业务已经成为英伟达最高的收入来源。而我国壁仞科技的 BR100P 系列芯片同样由台积电代工，预计于 2023 年量产，若量产计划顺利推进，也有望在数据中心这一广阔的市场中分一杯羹。
在智能驾驶领域，国内大多数智能驾驶车型选用英伟达产品，大多都采用其旗下的ORIN 产品。但在智能驾驶领域，华为也在之中提供了自动驾驶的全栈解决方案，其发布的昇腾 610、MDC810 已经量产，MDC610平台，单组算力为200 TOPS，与英伟达 ORIN 产品差距较小。在市场结构方面，2023年上半年，中国市场乘用车自动驾驶计算方案市场份额中，英伟达以52.57%的份额位居第一，地平线占据30.71%的市场份额位居第二，华为海思占据4.05%的市场份额。
在盈利能力方面，英伟达的智能驾驶业务在2024财年第二季度的营收较第一季度出现了下滑，但与去年同期相比，仍然增长了15%，总营收达到2.53亿美元。
结语 就目前的GPU市场而言，英伟达在市场中凭借自己多年的技术积累大杀四方，占据了绝对的领导地位（80%以上的市场份额），而AMD和重新归来的英特尔在其后瓜分剩下的一点市场份额。而国内GPU厂商则是刚刚起步，正在完成从0到1的突破阶段，总体来看还有很长的路要走。但是在诸如智能驾驶等国内重点发展的领域也能从英伟达口中分得一点点市场份额。
未来看我国的GPU发展，作为数据处理的重要工具，GPU具有重要的战略地位，并受到国家的高度重视。在中美科技摩擦的背景下，自主研发和掌握GPU显得尤为重要。从成长性的角度来看，全球市场空间广阔，国内市场规模也相当庞大，并随着下游需求的增加而呈现加速增长的趋势。在数字化驱动的总需求提升背景下，结合国产化的发展趋势，为国产GPU产业提供了总量和份额双提升的机遇。因此，国产GPU厂商得到了快速发展的机会。在大市场需求下，GPU的国产化进程具有广阔的空间，而优秀厂商的稀缺性也日益凸显，有望实现加速成长，部分厂商甚至有望实现爆发式增长。
]]></content>
  </entry>
  
  <entry>
    <title>你以为你的SSD硬盘正在待机，实际上它快忙坏了</title>
    <url>/post/datacenter/background-io-for-enterprise-ssd.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Background IO</tag>
      <tag>Enterprise SSD</tag>
    </tags>
    <content type="html"><![CDATA[Background IO，从字面理解即后台IO任务，它和前台IO是相对应的关系。前台IO，就是通常我们所理解的CPU访问SSD盘内数据、写数据到 SSD  闪存颗粒等工作。
Background IO是什么？ 但因为，SSD使用了相对HDD来说更加脆弱和敏感的NAND闪存颗粒，所以导致了SSD主控芯片不仅要处理数据的读写，还需要做一些额外的后台工作来保障SSD的正常工作。
一般来说，一款标准的企业级SSD，它所需要做的Background IO主要有以下几种：
 PLP电容自检 BootLoader存放区域扫描 系统区域扫描（固件、坏块表存放区） 用户数据区域后台扫描 温度传感器采样 Log日志定时写入NAND Read disturb数据搬移 后台GC垃圾回收 磨损均衡  平时没怎么总结还没发现，原来SSD固件在后台需要处理的任务这么多了，而且还有一些这里没有列出来的。
为什么需要Background IO？ PLP电容自检 正常情况下，拆开一块企业级SSD，基本都可以看到下图中这一块黑色的上面写着1000uF的电容，当然也可以是很多个并联的小电容组成。
这块电容在SSD中充当着不可或缺的角色，因为企业级对异常掉电后的数据恢复是刚需，因此需要这块电容存储电荷，在掉电时再继续让SSD工作一段时间，将仍留在DRAM中的重要数据及时写到NAND颗粒中，保证了异常掉电下的数据安全。
但是，只要是铝电解电容都会有一些缺点，那就是高温敏感、寿命短、漏电流大。因为铝电解电容内部基本是液态的电解质，当温度过高时，内部电解质会被加热导致挥发甚至是爆浆（有兴趣的读者，可以网上搜索“铝电解电容爆炸的各种姿势，还是挺有意思的”）。而且，如果长期在高温下使用，可能会造成电解质挥发至干涸，最终电容变成一具“干尸”。
当然，做SSD设计时就已经考虑到了铝电解电容的这些特征，因此做了冗余设计，保证电容能够持续工作5年以上。但，基于铝电解电容的这些特性，还是需要定时去检查一下有没有问题，防止电容失效而引起数据丢失灾难。一般每隔24小时检查一次，当然你也可以设置更长或更短时间间隔。
这就像，我们日常开的车，刹车片虽然不用每次保养都去更换，但每次保养都要去检查一下磨损程度。同样的道理。
BootLoader存放区域扫描 Bootloader的用途，做产品研发的同学应该是清楚的。它是一段非常精简的代码，用来引导整个固件或者系统代码的启动。
因此，它真的非常重要！
所以，BootLoader代码一般都不会存放在NAND颗粒中，而是在一颗很小的SPI Flash中，这个SPI Flash的可靠性要远高于NAND闪存颗粒。
但，即使这样，对于BootLoader存放区域的介质健康检查不可省略，谁叫它这么重要呢？至于扫描的时间间隔，每家公司设计不同，可自行决定。
就像，你在家里的密码箱存放了100斤黄金，就算它足够安全不会被偷，你也还是想每天晚上睡觉前打开柜子看看确保安全呢。
系统区域扫描（固件、坏块表存放区） 系统区域，一般是指固件在NAND闪存内部开辟出来的多个高安全区域，这些区域的特点是，相比于其他区域，他们更加安全，更不容易出现bit error。
在这些最安全的系统区域，我们会用来存放固件代码和备份、坏块表等数据，因为固件代码和坏块表足够重要，所以会有很多的备份存放在多个不同的系统区域。
但是，最安全的系统区域，也只是相对安全，并非绝对安全。因此，对这些区域的扫描不可省略，一旦发现某些闪存块有问题，就要立马将数据搬到新的区域。至于扫描的时间间隔，一般是2-3周，或200-300个上下电循环，企业级一般是采用固定时间间隔很少采用上下电循环，消费类盘都可以。
用户数据区域后台扫描 用户数据区域，和刚刚说的系统区域，是一个相对的概念。
相对于系统区域，用户数据区域的数据安全性相对低一些，但也是相当重要的，毕竟用户就是上帝，上帝的数据绝不能丢。
但为什么需要定时去扫描用户的数据区域呢？因为所有的NAND闪存都有一个缺点，包括英特尔家的floating gate架构、或者其他家的charge trap架构，均存在电荷泄漏的问题。
意思是，当已经保存有数据的NAND闪存断电时，其cell内部的电子会逐渐逃逸出去，导致电荷泄漏，最终数据丢失。当然，不要过度恐慌，这是一个非常缓慢的过程。正常情况下，企业级SSD断电情况下，数据可以在40摄氏度温度下安全存放3个月，即使是生命末期。而消费类的SSD，则数据可以在30摄氏度的温度下安全存放1年的时间（所以，如果有好久没有用过且存有重要数据的SSD，要赶紧拿出来溜溜了，通上电跑一跑）。
它有个专业术语，叫数据保持-data retention。
因此，一旦我们的SSD掉电一段时间后，尤其是经历过高温的情况，之前保存的数据可能会陷入险境之中。这时候，一旦重新上电，SSD固件就要开始马不停蹄地去检查，哪些数据存在风险，要赶紧搬移到新的区域。
温度传感器采样 温度传感器采样，这个比较简单，因为很多情况下，系统需要获取到SSD内部的温度，来决定散热风速的转速。
因此，温度传感器的采用必不可少，一般都是每秒钟采样一次。
Log日志定时写入NAND 都知道Windows/Linux操作系统有各种各样的log日志，来记录系统的状态和debug用途。SSD也是一样，它内部也运行了一套庞大的类似系统一样的复杂逻辑和运算，因此也会存在各种各样的Log日志来记录系统状态、错误信息等内容。
尤其是在SSD研发早期阶段，大量的debug工作非常依赖于系统log。同时，在如今的大型数据中心，越来越多的用户开始关注SSD Log，因为对于数据中心的运维来说，健全的Log可以大大提高debug效率。
但，因为很多时候，最新Log其实是存放在DRAM中的，因为这样可以更快去刷新。而DRAM是易失性存储，即掉电数据丢失，因此，系统Log日志，同样也是需要定期维护刷写到非易失性的NAND闪存中的。至于刷写的时间间隔，因需求而异，可以设置5分钟，也可以设置半小时。
Read Disturb数据搬移 Read Disturb，读干扰，也是NAND闪存的其中一个缺点。具体的原理呢，会比较复杂，一时半会很难说清楚，后续有时间单独开一篇来讲。
这里，只简单说一下读干扰所带来的结果。因为现代NAND制程的进度，密度增加，导致cell之间的距离越来越小。而读干扰的现象也越来越明显，即当我们读取某个页数据非常多次数时（可能是200万~1000万次），会对它周边的页造成干扰，而影响到周边页的数据，严重时会造成周边页数据无法读取。
因此，当我们选择一款NAND开发SSD时，NAND厂商的FAE会告诉我们，read disturb建议的次数时xx次。
当固件发现某个页被连续读取了xx次时，触发了read disturb阈值，此时就要将周边页的数据进行搬移，防止数据丢失。
后台GC垃圾回收 后台GC垃圾回收，这个已经不用说太多了吧。
因此NAND闪存本身的特性，最小写入单位是页，而擦除最小单位是块。因此随着数据不断写入，每个块上都会随机散布着各种数据，这时候如果有新的数据再写进来，则可能没有足够的空间，这时候就需要擦除旧的数据块，并搬移有效数据到新的块。
当经历过了长时间的随机写数据时，这部分GC的工作量是非常大的，而且非常影响这个SSD的性能。
磨损均衡 二八定律无处不在，又或者说，正态分布才是自然的常态。
经过不正经的调查，当没有任何认为干预时，我们随机性地往SSD中写入数据。
当我们写满全盘时，没有太大发现，但是当写了很多次全盘之后，就会发现。
NAND闪存中仅有20%的区域被写入了超过80%的数据，而另外80%的区域却并没有被写入太多数据。
这样会造成一个问题，因为NAND闪存都是有寿命的，而这种写入方法会造成20%的NAND闪存块很快被写完，寿命殆尽。而剩下80%的NAND闪存块却还可以用很久。
但这20%的闪存块的退役，会导致整个SSD系统无法正常工作，相当于一个本来完整的木桶，突然出现了一块短板，导致桶里的水大量流失。
因此，Wear Leveling磨损均衡才被需要，而且相当重要，因为它决定了你的木桶到底能装多少水！
最后 写到这里，我虽意犹未尽，但也累得腰酸背痛想要休息片刻。
不知各位读者看完，作何感想，能写SSD固件代码固然不难，但如果要做好固件真心不易。
]]></content>
  </entry>
  
  <entry>
    <title>传三星斩获AMD部分订单</title>
    <url>/post/news/samsung-is-rumored-to-have-won-some-orders-from-AMD.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>Samsung</tag>
      <tag>Zen 5c</tag>
    </tags>
    <content type="html"><![CDATA[据报道，三星已获得 AMD  的订单，将使用其的4nm工艺技术，为AMD生产基于Zen 5c架构的处理器。目前，这一消息在多个渠道得到了证实。
如果这笔交易最终达成，这标志着AMD对其制造战略进行了调整，以实现多元化供应。其中，AMD的考量已经从工艺节点、生产良率、成本等因素，扩展到产能、生态链等多个角度。
据悉，AMD的“Prometheus”处理器将由三星和台积电共同生产。其中，三星将使用其先进的4nm工艺技术制造基础版本的“Prometheus”处理器，而台积电则将使用更先进的3nm工艺技术来生产更高级别的“Prometheus”处理器。
如今，三星正在积极扩展其4nm工艺，以能够从台积电那里获得更多订单。在关键的良率方面，业内人士普遍认为，台积电的4nm工艺良率达到了80%，而三星的4nm工艺已经从年初的50%提升到75%，与台积电相当。对此，外界猜测高通、英伟达等大客户可能会回流采用三星工艺。
据此前曝出的苹果高层会议纪录显示，台积电3nm制程的良率接近63%，报价比4nm高出一倍。因此，随着三星今年较大幅度改善良率和产能，再加上台积电今年决定涨价，很多大客户可能会从成本等角度考量，寻找第二供应商以分散外包生产订单。
]]></content>
  </entry>
  
  <entry>
    <title>GH200，来自英伟达的AI超级计算机</title>
    <url>/post/news/gh200-from-nvidia.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Nvidia</tag>
      <tag>GH200</tag>
    </tags>
    <content type="html"><![CDATA[Nvidia的GH200 AI超级计算机是一款巨大的进步，相比于之前的H100，其在使用TB级数据训练和部署AI模型方面的性能提升了3-7倍。
GH200拥有144TB的共享内存和141GB HBM3e内存，这个共享内存比标准的A100多出了500倍。
每一个GH200 GPU都能够访问其他GPU的内存，以及所有NVIDIA Grace CPU的扩展GPU内存，速度高达每秒900GB。这大大加快了大型语言模型（LLM）的训练速度，并显著降低了LLM推理的成本。有了这些超级计算机，就不再需要成千上万的机器来训练和运行LLM的推理。
有了GH200，就不再需要成千上万台机器来训练LLM和进行推理，这对初创公司和大型企业都是一个变革性的时刻。特别是对那些旨在训练、微调和部署LLM的初创公司来说，GH200为它们提供了以前只有大公司才能拥有的计算能力。
随着时间的推移，硬件将变得更加强大和便宜，不久的将来，我们甚至可能在我们的笔记本电脑上运行我们自己的强大的个人LLM。
]]></content>
  </entry>
  
  <entry>
    <title>英伟达发布H200, 高性价比全面升级</title>
    <url>/post/news/nvidia-launch-h200-gpu.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Nvidia</tag>
      <tag>H200</tag>
      <tag>H100</tag>
    </tags>
    <content type="html"><![CDATA[在11月13日的S23大会上， NVIDIA  宣布推出NVIDIA HGX H200，为生成式人工智能和高性能计算（HPC）工作负载提供强大支持。
根据介绍，新的H200 GPU是当前H100的升级产品，作为第一款搭载 HBM3e 的 GPU，H200 集成了141GB的内存，更大更快的内存推动生成式人工智能和大型语言模型（LLM）的加速，同时推进科学计算在 HPC 工作负载中的发展，在用于推理或生成问题答案时，性能较H100提高60%至90%。
H200主要在内存带宽和内存容量上进行提升 为了最大化计算性能，H200 是全球首款搭载 HBM3e 内存的 GPU，拥有4.8TB/s的内存带宽，较 H100 增加了1.4倍。H200 还将 GPU 的内存容量扩展到141GB，几乎是 H100 的80GB 的两倍。更快速和更大容量的 HBM 内存的结合，加速了计算密集型的生成式人工智能和高性能计算应用的性能，同时满足了不断增长的模型大小的需求。对比 H100 和 H200 的性能参数，在算力指标上H200和H100完全一样。H200 的所有改进都集中在更快速和更高容量的141GB HBM3e 的引入。
H200相较于H100在推理性能上最高提高至2倍 H200的推理表现相较于H100在各大不同的模型都具有明显的提升。与处理类似 Llama2（一个 700 亿参数的 LLM）的LLMs时相比，相较于 H100 GPU，H200 在推理速度上提高了最多 2 倍。
高内存带宽激发高性能计算表现 内存带宽对于高性能计算应用至关重要，因为它能够实现更快的数据传输，减少复杂的问题处理。对于内存密集型的高性能计算场景，如仿真、科学研究和AI，H200 的更高内存带宽确保数据可以被高效地访问和操作，从而在与 CPU 相比，实现高达 110 倍更快的生成结果时间。
降低能耗和总拥有成本（TCO） H200 的引入使得AI数据中心能源效率和TCO进一步提升。H200在性能相较H100具有明显提升的背景下，保持了与 H100 相同的功耗。以Llama2 70B这样的模型去推理衡量看，H200的性能是H100的两倍，在功耗相同的情况下，成本是原来单位成本的一半。
]]></content>
  </entry>
  
  <entry>
    <title>GPT4.0 终于可以免费使用了</title>
    <url>/post/news/gpt-4.0-is-finally-free-to-use.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>GPT 4.0</tag>
      <tag>Copilot</tag>
      <tag>Microsoft Ignite</tag>
    </tags>
    <content type="html"><![CDATA[2023年末将至，今年几乎每个产业都在经历着集体转型， AI  技术的发展为工作方式带来了新的变革。在最近的Microsoft Ignite技术大会上，微软展示了他们最新的技术和产品进展，让每个人都能体验到科技创新的力量。
Microsoft Ignite是一年一度针对开发者和IT专业人士举办的大会。AIGC无疑是本届大会的最大的热点。而最重磅的莫过于微软宣布，Bing Chat更名为Copilot，面向所有用户免费开启GPT-4功能。
Bing Chat正式更名为Copilot，品牌升级意味着Copilot正在成为一个独立的新产品，用户无需先导航到Bing就可以访问。Copilot现在可以通过Bing和Windows使用，与ChatGPT一样，微软Copilot也有了自己的独立域名：copilot.microsoft.com，但与ChatGPT不同，Copilot上像GPT-4、DALL-E 3等功能全部免费开放！你只需登录微软账户即可使用（而ChatGPT需要订阅会员，每月20刀）。
就连OpenAI上周发布的GPT自定义版本，微软也开始免费提供，命名为Copilot Studio。
此前，微软内部曾有十几种产品共享“Copilot”品牌，Copilot品牌起源于2021年的 GitHub Copilot，今年微软又推出了Dynamics 365 Copilot、Windows Copilot、Microsoft Security Copilot和Microsoft 365 Copilot等，似乎每项业务都有自己的“Copilot”。
那为什么要在Copilot家族中再添两名新成员？微软表示是为了给消费者和企业客户带来统一的Copilot体验。
现在，Bing Chat成为了微软的第六个Copilot，也可以视作是最通用的一个Copilot。未来，Bing将为其提供支持。另外，Copilot基于最新OpenAI模型，包括GPT-4和DALL-E 3，提供统一的文本和图像生成功能。
有分析认为，Bing Chat本身并未为Bing带来太多效益，所以微软将其从搜索引擎中剥离出来。此举颇为讽刺，因为微软此前大力利用AI产品来提升自家搜索引擎，意图从谷歌手中夺取市场份额。今年初，微软CEO纳德拉甚至称“谷歌是搜索领域800磅重的大猩猩”，微软将让它“跳舞”。但谷歌并未像微软那样急于在搜索中加入生成式AI。从现在看来，这种“不变”并未对谷歌造成什么不良影响：在Bing Chat推出近10个月后，谷歌搜索引擎的市场份额仍超过91%。
微软特别提到，从12月1日起，使用Microsoft Entra登录时，在Bing、Edge和Windows中使用Copilot的用户将享有商业数据保护功能。Copilot不会保存用户的prompt和回复，微软无法直接访问这些数据，也不会用于模型训练。
此外，微软还推出了其他Copilot产品：Microsoft Copilot Studio、Copilot for Azure、Copilot for Service和Copilot in Dynamics 365 Guides。
其中，Microsoft Copilot Studio对应OpenAI上周发布的GPTs产品。该新平台提供将微软AI应用与第三方数据连接的工具，企业可以用它创建定制的Copilot或集成自定义的ChatGPT对话机器人。该产品已向现有Copilot for Microsoft 365订阅用户开放公测版。
现在，无论是Edge、Chrome、Safari，还是移动端，都可以使用Copilot。需要注意的是，虽然Copilot仅需要登录微软账户就可免费使用，但Microsoft 365等其他Copilot产品仍需付费。
与OpenAI GPTs相比，微软Copilot Studio还有些不同。它的主要设计目的是扩展Microsoft 365 Copilot。用户可以利用该应用自定义集成不同数据集和自动化流程的Copilot。
除了将Bing Chat改名Copilot，微软还发布了期待已久的自研芯片。这是为云基础设施设计的两款高端定制芯片，分别是Azure Maia 100 AI芯片和Cobalt 100 CPU。两款芯片均由微软内部构建，深度针对整个云服务器堆栈进行了优化，在性能、功耗和成本方面具有竞争优势，计划2024年推出。
]]></content>
  </entry>
  
  <entry>
    <title>恒流电路的三种设计方案</title>
    <url>/post/hardware/three-design-options-for-constant-current-circuits.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>Circuit Design</tag>
      <tag>Constant Current</tag>
    </tags>
    <content type="html"><![CDATA[作为硬件研发工程师相信对恒流电路不会陌生，本文介绍下三种恒流电路的原理图。
三极管恒流电路 三极管的恒流电路，主要是利用Q2三极管的基级导通电压为0.6~0.7V这个特性；当Q2三极管导通，Q1三极管基级电压被拉低而截止，负载R1不工作；负载R1流过的电流等于R6电阻的电流（忽略Q1与Q2三极管的基级电流），R6电阻的电流等于R6电阻两端的0.6~0.7V电压除以R6电阻阻值（固定不变），因此流过R1负载的电流即为恒定不变，即使R1负载的电源端VCC电压是可变的，也能达到恒流的电路效果。
运放恒流电路 运放的恒流电路，主要是利用运放的“电压跟随特性”，即运放的两个输入引脚Pin3与Pin2电压相等电路特性；当在电阻R4输入Vin稳定电源电压时，电阻R7两端的电压也为Vin不变，因此无论外界电路如何变化，流过R7电阻的电流是不变的；同三极管恒流电路原理分析一样，R2负载的电流等于R7电阻的电流，所以即使R2负载的电源为可变电压电源，R2负载的电流也是保持固定不变，达到恒流的效果。
除去运用三极管与运放设计的恒流电路，芯片哥介绍另外一种恒流电路设计方案，主要是利用稳压二极管的稳压特性。
稳压二极管恒流电路 稳压二极管的恒流电路中，三极管Q4的基级电压被限定在稳压二极管工作的稳定电压Uzd下，因此R10电阻的电压等于Uzd减去三极管基级与发射级的导通压降0.7V，即U=Uzd-0.7保持恒定不变，所以流过R10电阻的电流在VCC电源即使可变的条件下也是固定不变，也就是R8负载的电流保持不变，达到恒流的效果。
原文引用： 恒流电路的三种设计方案  
]]></content>
  </entry>
  
  <entry>
    <title>一图说清8 种流行的网络协议</title>
    <url>/post/datacenter/8-popular-network-protocols.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Network Protocol</tag>
      <tag>HTTP</tag>
      <tag>HTTPS</tag>
      <tag>WebSocket</tag>
      <tag>TCP</tag>
      <tag>UDP</tag>
      <tag>SMTP</tag>
      <tag>FTP</tag>
    </tags>
    <content type="html"><![CDATA[网络协议是在网络中的两台计算机之间传输数据的标准方法。
 HTTP（超文本传输协议：HyperText Transfer Protocol)  HTTP 是一种用于获取 HTML 文档等资源的协议。它是 Web 上任何数据交换的基础，是一种客户端-服务器协议。
HTTP/3 HTTP/3  HTTP/3 是 HTTP 的下一个主要修订版。它运行在 QUIC 上，这是一种专为移动互联网使用而设计的新传输协议。它依赖于 UDP 而不是 TCP，从而实现更快的网页响应。VR 应用程序需要更多带宽来渲染虚拟场景的复杂细节，并且可能会从迁移到由 QUIC 提供支持的 HTTP/3 中受益。
HTTPS（安全超文本传输协议：HyperText Transfer Protocol Secure)  HTTPS 扩展了 HTTP 并使用加密技术实现安全通信。
WebSocket  WebSocket 是一种通过 TCP 提供全双工通信的协议。客户端建立WebSocket来接收来自后端服务的实时更新。与总是“拉取”数据的 REST 不同，WebSocket 允许“推送”数据。在线游戏、股票交易和消息传递应用程序等应用程序利用 WebSocket 进行实时通信。
TCP（传输控制协议：Transmission Control Protocol)  TCP 旨在通过互联网发送数据包并确保通过网络成功传送数据和消息。许多应用层协议构建在 TCP 之上。
UDP（用户数据报协议：User Datagram Protocol)  UDP 直接将数据包发送到目标计算机，无需先建立连接。UDP 通常用于时间敏感的通信，其中偶尔丢弃数据包比等待更好。语音和视频流量通常使用此协议发送。
SMTP（简单邮件传输协议：Simple Mail Transfer Protocol)  SMTP 是一种用于将电子邮件从一个用户传输到另一个用户的标准协议。
FTP（文件传输协议：File Transfer Protocol)  FTP 用于在客户端和服务器之间传输计算机文件。它具有单独的控制通道和数据通道连接。
]]></content>
  </entry>
  
  <entry>
    <title>大型语言模型入门必知术语汇总</title>
    <url>/post/datacenter/terminology-of-large-scale-language-models.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Large Scale</tag>
      <tag>Terminology</tag>
      <tag>AI</tag>
      <tag>LLM</tag>
    </tags>
    <content type="html"><![CDATA[在本文中，我将以一种非数据科学家易于理解的方式分解与LLM和AI相关的一些基本术语和概念。我将涵盖从神经网络到数据增强的所有内容，并为每个术语提供简单的解释和示例。
   人工智能（AI）  ：它就像一个智能机器人，可以像人类一样思考和做事。人工智能帮助计算机解决问题、做出决策并理解我们的语言。示例：iPhone 上的 Siri。
  深度学习：这是计算机从许多例子中学习的一种方式，比如你如何从经验中学习。深度学习使用称为神经网络的特殊计算机程序来查找数据中的模式。示例：计算机学习识别图片中的猫。
  神经网络：一种像人脑一样工作的计算机程序，使用连接的节点（如脑细胞）分层。示例：可以玩视频游戏的计算机“大脑”。
  Transformer：谷歌创建的一种特殊类型的神经网络，用于更好地理解和生成语言。示例：可以像朋友一样与您聊天的计算机。
  大型语言模型（LLM）：一种计算机程序，通过研究大量文本来学习理解和创建人类语言。示例：可以编写故事或回答问题的计算机。
  参数：神经网络中在训练期间进行调整以帮助其学习的部分。示例：就像调整吉他以使其听起来更好。
  位置编码：Transformer 记住句子中单词顺序的一种方式。示例：记住“狗追猫”与“猫追狗”不同。
  自我注意（Self-Attention）：Transformer专注于句子中最重要的部分的一种方式。示例：知道“蛋糕”是“我想吃蛋糕”中的关键词。
  编码器：Transformer 的一部分，帮助它理解并记住你告诉它的内容。示例：计算机记住问题“今天的天气怎么样？
  解码器：Transformer 的一部分，可帮助它创建响应或答案。示例：计算机回答：今天的天气晴朗而温暖。
  BERT：一种 Transformer 模型，可帮助计算机理解语言，以执行诸如猜测人们对电影的看法之类的任务。示例：知道评论是正面还是负面的计算机。
  GPT-3 和 GPT-4：一种 Transformer 模型，可帮助计算机像人类一样生成文本，例如完成句子或撰写摘要。示例：一台计算机为您编写读书报告。
  T5：擅长理解和生成文本的 Transformer 模型，例如将一种语言翻译成另一种语言。示例：可以将英语翻译成西班牙语的计算机。
  无监督学习：当计算机在没有被告知什么是对或错的情况下学习模式时。示例：一台计算机学习将相似的图片组合在一起。
  基础模型：大型AI模型，如LLM，可用于许多不同的任务。示例：可以帮助完成家庭作业、写电子邮件和讲笑话的计算机。
  零样本学习(Zero-Shot Learning)：当计算机无需接受训练即可完成任务时。示例：无需先练习即可玩新游戏的计算机。
  少样本学习(Few-Shot Learning)：当计算机只需几个例子就可以学习新任务时。示例：一台计算机，可以在听一两次后学习您喜欢的歌曲。
  微调：调整经过训练的模型以更好地完成特定任务。示例：教计算机理解和回答有关恐龙的问题。
  提示调优：改变您向计算机提问以获得更好答案的方式。示例：问“法国的首都是什么？”而不是“巴黎在哪里？”
  适配器(Adapters)：您可以添加到训练模型的微小部件中，以帮助它完成特定任务，而无需对其进行太多更改。示例：在不更改整个游戏的情况下向电脑游戏角色添加新技能。
  自然语言处理（NLP）：教计算机理解、解释和创建人类语言。示例：可以与您聊天或阅读您的论文的计算机。
  自然语言理解（NLU）：教计算机理解和寻找人类语言的含义。示例：一台知道“我喜欢猫”和“我不喜欢猫”之间区别的计算机。
  自然语言生成（NLG）：教计算机创建类似人类的文本。示例：可以写故事或诗歌的计算机。
  token化（Tokenization）：将文本分解为单词或单词的一部分，称为标记，以帮助计算机理解语言。示例：将句子“我有一只狗”拆分为标记：“我”、“有”、“一只”和“狗”。
  词汇：计算机程序可以理解的独特单词或标记集。示例：计算机知道单词“苹果”、“香蕉”和“橙子”，但不知道“猕猴桃”。
  预训练：训练LLM的第一步，它从大量文本中学习语言。示例：一台计算机阅读大量书籍和文章以学习如何写作。
  迁移学习：使用计算机从一个任务中学到的知识来帮助它完成另一个相关的任务。示例：一台计算机，它学会了识别猫，使用该知识来识别狗。
  序列到序列 （Seq2Seq） 模型：一种将一个序列（如文本）更改为另一个序列（如翻译）的模型。示例：计算机将英语文本转换为法语文本。
  注意力机制：计算机在创建输出时专注于输入重要部分的一种方式。示例：计算机知道“披萨”是“我想吃披萨”中最重要的词。
  波束搜索(Beam Search)：一种在计算机生成文本时查找最佳单词序列的方法。示例：计算机选择句子中最有可能的下一个单词。
  困惑（Perplexity）：一种衡量计算机预测文本能力的方法。示例：较低的困惑度意味着计算机更善于猜测句子中下一个单词。
  上下文学习：当计算机可以根据输入更改其行为时，无需额外的培训。示例：一台计算机在谈论体育后知道如何回答有关体育的问题。
  数据增强：通过创建新样本（如改写句子）使数据集更大、更多样化。示例：将“猫在垫子上”更改为“猫坐在垫子上”。
  偏差：当计算机因其训练数据不平衡或不具有代表性而犯错误时。示例：计算机认为所有医生都是男性，因为它主要阅读有关男性医生的信息。
  可解释的AI（XAI）：使计算机的决策过程更容易被人类理解。示例：一台计算机解释为什么它认为某部电影是喜剧。
 ]]></content>
  </entry>
  
  <entry>
    <title>智能网卡（SmartNIC）才是未来</title>
    <url>/post/datacenter/smart-nic-is-the-trend-for-future.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>SmartNIC</tag>
    </tags>
    <content type="html"><![CDATA[网络技术和硬件设备的不断发展已经改变了数据中心和云计算的格局。随着虚拟化、微服务架构的不断激增，以及移动设备和云服务的高度利用率，网络工作负载的增加速度已经远远超过了传统数据中心CPU的处理速度。这使得寻找更高性能、更灵活性以及更强大功能的网络解决方案变得迫切。
当涉及到网络硬件的选择时，智能网卡（SmartNIC）和标准网卡（NIC）是两个备受关注的选项。本文将深入探讨这两种网卡的特性、差异以及如何在特定用例中选择合适的选项。
标准网卡（NIC）：传统的网络连接 几十年来，标准网卡一直是网络连接的主要方式。它们是将计算机连接到网络的硬件组件，实现设备和互联网之间的通信。标准网卡主要用于数据传输和接收，通常因其可靠性和鲁棒性而在数据中心环境中得到广泛应用。
标准网卡的功能 标准网卡具有以下基本功能：
 数据包传输和接收。 数据包校验和校正。 数据包分段和重组。 MAC地址管理。  这些功能足以支持常见的网络连接需求，但在面对数据密集型应用、虚拟化环境、云计算和高性能计算等要求更高性能和功能的情况下，标准网卡可能显得力不从心。
传统硬件的限制 传统的数据中心硬件包括基于NIC的网络卡，这些硬件已经无法满足现代数据中心的需求。
CPU负担过重 传统基于NIC的架构将网络任务交给服务器的CPU来处理，这导致CPU负担过重，无法应对高强度网络工作负载。
难以满足高性能需求 计算密集型应用程序需要更高的性能和低延迟。传统硬件无法提供足够的性能，导致性能瓶颈。
难以管理和维护 随着数据中心规模的扩大，管理和维护传统硬件变得更加困难。虚拟化和微服务的增长也增加了管理的复杂性。
智能网卡（SmartNIC）：超越传统网络 智能网卡是一种新型网卡，它超越了传统的数据传输功能。它们配备了强大的处理器、内存和专用硬件，可以执行各种高级功能。这些功能包括网络增强、存储加速、安全功能等等。智能网卡的主要目标是从主机CPU上卸载和加速各种网络相关任务，提供专用处理能力和硬件功能，以增强网络性能、安全性和效率。
智能网卡的功能 智能网卡提供了一系列高级功能，包括：
 数据包过滤和负载平衡。 服务质量（QoS）实施。 存储加速，包括远程直接内存访问（RDMA）、iSCSI和NVMe over Fabrics。 安全功能，如防火墙处理和入侵检测系统（IDS）检查。  这些功能使得智能网卡成为适用于各种高级用例的强大工具，从提高网络性能到加强安全性，再到存储加速。
早期SmartNIC实现 早期的SmartNIC实现使用寄存器驱动的ASIC逻辑，这些设计在性能方面表现出色，具有极低的延迟、高数据包吞吐量和低功耗（通常在15瓦到35瓦之间）。然而，尽管在性能上具有优势，但它们通常缺乏必要的可编程性和灵活性。它们通常需要深奥的命令行工具来设置寄存器，缺乏以编程方式管理数据包和流的能力。
SmartNIC的多种部署方式 SmartNIC在多种不同的部署场景中发挥着关键作用，其中包括存储、安全和网络处理。SmartNIC可能承担的特定任务包括隧道协议（例如VxLAN）的处理以及复杂虚拟交换，如图1所示。它的最终目标是消耗更少的主机CPU处理器内核，同时以更低的成本提供更高的性能解决方案。
标准网卡与智能网卡的区别 联网功能 标准网卡主要设计用于处理基本网络任务，例如数据包传输、分段和校验和校正。虽然智能网卡也执行这些任务，但它们在卡本身上实现了更复杂的网络功能，如数据包过滤、负载平衡和服务质量（QoS）实施，从而从主机CPU上卸载了这些任务。这降低了延迟、降低了CPU利用率，并提高了整体网络性能。
存储功能 智能网卡在存储加速方面表现出色，特别适用于现代数据密集型应用。它们可以卸载远程直接内存访问（RDMA）、iSCSI和NVMe over Fabrics等存储功能，从而提高数据传输速度并减少CPU开销。相比之下，标准网卡缺乏这些存储密集型任务所需的硬件和处理能力。
任务卸载到SmartNIC 智能网卡最显著的优势之一是能够将各种任务从主机CPU卸载到网卡上。这不仅包括网络和存储功能，还包括防火墙处理和入侵检测系统（IDS）检查等安全任务。通过将这些任务转移到智能网卡，主机CPU可以腾出时间来专注于应用程序特定的处理，从而提高应用程序性能并减少服务器蔓延。
如何选择标准网卡或智能网卡 在选择标准网卡或智能网卡时，需要评估特定网络需求和用例。以下是需要考虑的一些因素：
性能和速度 如果您的应用程序需要卓越的网络性能、更低的延迟以及负载平衡、存储加速等高级功能，那么智能网卡可能是更好的选择。它们可以卸载并加速各种任务，从而提高整体性能。
工作负载和用例 考虑工作负载和用例的性质。智能网卡在数据密集型场景、虚拟化环境、机器学习和云服务中尤其具有优势，因为在这些场景中，高级网络和存储功能至关重要。如果您的环境需要更多的功能和性能，那么智能网卡可能是更合适的选择。
预算和成本 在选择网卡时，还需要考虑您的预算限制。智能网卡通常比标准网卡更昂贵，因为它们提供了更多功能和性能。如果成本是一个关键因素，那么标准网卡可能是更经济的选择。
结论 在网络连接领域，我们常常发现自己在标准网卡（NIC）和智能网卡（SmartNIC）之间做出选择。不同的用例和需求将决定您的最佳选择。一些基于Intel的以太网适配器，例如E810CAM2-2CP，具备许多高级功能，包括以太网、TCP/IP、UDP/IP等多种网络协议的支持，片上QoS、流量管理、iWARP/RDMA、RoCEv2/RDMA、智能卸载以及iSCSI和NFS的存储功能。这些适配器经过严格测试和认证，确保与各种操作系统和管理程序全面兼容，为那些寻求最佳网络性能和功能的用户提供了平衡且可靠的解决方案。
除了这些特色的NIC解决方案外，还有Mellanox SmartNIC可供选择。与Mellanox Technologies合作，这些SmartNIC开启了网络增强、卸载和安全性的无限可能。无论您选择高级NIC还是SmartNIC，都可以根据您的网络需求，确保网络以最佳、安全和高效的方式运行。
在不断演进的网络领域，选择合适的网卡是至关重要的，它将直接影响到您的网络性能、安全性和应用程序的运行。因此，深入了解标准网卡和智能网卡的优劣势以及如何根据特定需求进行选择，将有助于确保您的网络能够满足不断变化的需求。
]]></content>
  </entry>
  
  <entry>
    <title>详细讲解MMU—为什么嵌入式linux没他不行</title>
    <url>/post/linux/why-linux-need-mmu.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>mmu</tag>
    </tags>
    <content type="html"><![CDATA[MMU（Memory Management Unit，内存管理单元）是一种硬件模块，用于在CPU和内存之间实现虚拟内存管理。
MMU内存管理 其主要功能是将虚拟地址转换为物理地址，同时提供访问权限的控制和缓存管理等功能。MMU是现代计算机操作系统中重要的组成部分，可以提高系统的稳定性和安全性。
在内存管理方面，MMU可以通过页面表（Page Table）实现虚拟内存管理。页面表是一种数据结构，记录了每个虚拟页面和其对应的物理页面之间的映射关系。
当CPU发出一个虚拟地址时，MMU会通过页面表查找并将其转换为对应的物理地址。
此外，MMU还可以通过页面表实现内存保护和共享等功能，从而提高系统的安全性和效率。
总之，MMU是内存管理中一个重要的硬件组件，可以实现虚拟内存管理、内存保护、共享和缓存等功能，为现代计算机操作系统的稳定性和安全性提供支持。
举个例子 假设我们有一个程序，它需要访问两个内存区域：一个是只读的代码区域，一个是可读写的数据区域。
我们现在想要在一个没有 MMU 的系统上运行这个程序。如果没有 MMU，代码区域和数据区域就只能被映射到两个固定的物理地址上。这就意味着，如果程序尝试访问一个不正确的地址，可能会导致系统崩溃。
现在，如果我们在一个具有 MMU 的系统上运行这个程序，情况会有所不同。MMU 可以将程序尝试访问的地址映射到不同的物理地址，这样可以使得代码区域和数据区域在物理内存中不再是固定的位置。
这意味着，如果程序尝试访问不正确的地址，MMU 可以通过重新映射来保护系统不崩溃。
MMU 还可以将多个虚拟地址映射到同一个物理地址上，这就是所谓的页共享（page sharing），可以减少物理内存的使用。
如果没有MMU，程序访问内存时只能使用物理地址，而物理地址是直接映射到内存芯片上的地址，程序可以随意访问任何一个物理地址。
这种情况下，程序如果访问了错误的地址或试图访问未被授权的地址，就会产生访问错误或非法访问，可能导致系统崩溃、数据丢失等问题。
而有了MMU，程序访问的是虚拟地址，由MMU负责将虚拟地址映射到物理地址上，这样程序就无法直接访问物理地址。
同时，MMU可以根据内存访问权限来限制程序对内存的访问，确保系统的安全性和稳定性。
因此，没有MMU时，程序可能会访问到其他地址，而有了MMU，程序只能访问被允许访问的地址，可以有效地避免非法访问的问题。
为什么相同的虚拟地址空间在物理地址不会发生冲突呢？ 相同的虚拟地址空间在不同的进程中可能会映射到不同的物理地址，这个映射的过程是由MMU完成的。在操作系统中，每个进程都有独立的虚拟地址空间，且这些虚拟地址空间互不干扰。
MMU会将每个进程的虚拟地址映射到对应的物理地址上，使得不同进程间的内存访问不会相互干扰。同时，MMU也会提供一些安全机制，如页面保护等，来防止进程越界访问内存或访问其他进程的内存。
因此，MMU起到了保护进程间内存互不干扰的作用，也是现代操作系统的重要组成部分。
页表是什么？ 页表是一种用于存储虚拟内存地址与物理内存地址映射关系的数据结构。在使用虚拟内存的系统中，每个进程都有自己的虚拟地址空间，而这些虚拟地址空间被分割成许多页（通常大小为4KB或更大），而不是一整块连续的内存。
因此，当进程需要访问某个虚拟地址时，需要将其翻译成对应的物理地址。这个翻译过程就是通过页表来完成的。
页表的基本原理是将虚拟地址划分成一个页号和一个偏移量。
页号用于在页表中查找对应的物理页帧号，而偏移量则用于计算该虚拟地址在物理页帧中的偏移量。通过这种方式，就可以将虚拟地址映射到物理地址，使得进程可以访问对应的内存区域。
页表一般由操作系统来维护，因为操作系统需要掌握虚拟地址和物理地址之间的映射关系。
在使用MMU（Memory Management Unit）的硬件支持的系统中，当进程访问虚拟地址时，MMU会通过页表将虚拟地址转换为物理地址，并将访问指向正确的物理地址。这样，进程就可以在不知道自己真实物理地址的情况下访问内存。
为什么没有MMU就无法运行Linux系统？ 这是因为 Linux   内核将虚拟地址空间分为多个页面，并将这些页面映射到物理地址空间上，以实现内存隔离、保护和虚拟内存等功能。
没有 MMU，就无法实现这种映射，从而无法运行 Linux 系统。
为什么有些较为简单的SOC可能没有MMU，但仍然可以运行一些嵌入式操作系统或者裸机程序？
RTOS可以运行在没有MMU的系统上，因为RTOS通常不需要进行内存保护和虚拟地址映射等高级特性。
相反，RTOS的设计侧重于实时性和低延迟，因此通常只需要简单的内存管理和任务调度即可。
这使得RTOS可以运行在许多嵌入式系统上，包括一些没有MMU的系统。
]]></content>
  </entry>
  
  <entry>
    <title>CXL开启高性能计算的新纪元</title>
    <url>/post/datacenter/CXL-opens-a-new-era-of-high-performance-computing.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>CXL</tag>
      <tag>PCIe</tag>
    </tags>
    <content type="html"><![CDATA[随着科学研究和工业生产对数据处理能力的需求不断攀升，高性能计算（HPC）已成为推动这些领域进步的重要力量。在这样的背景下，计算机技术尤其是互连技术的进步显得尤为关键。Compute Express Link（ CXL  ）作为新一代的高速互连技术，因其在带宽、延迟和扩展性方面的优势，正展现出在HPC领域的广泛应用潜力，有可能彻底改变数据中心和加速器之间的通信方式，从而推动HPC领域的革命性进步。
然而，为了充分发挥CXL的价值，软件必须得及时跟得上技术的发展，所以，其中的关键是开发针对SoC（System on Chip）设计师所需的硬件生态系统的软件框架。这意味着我们需要构建支持CXL的全面软件解决方案，以简化应用开发，确保硬件的充分利用，并最终实现系统级的优化。
三种CXL设备解锁HPC全新潜能 CXL主要由三种协议构成：CXL.io、CXL.cache 和 CXL.mem。具体来说，CXL.io被视为是PCIe通信的低延迟备选，它们在很多场合上可以互相替代。CXL.io为I/O设备带来了增强且非一致的加载/存储接口。CXL.cache为相关设备提供了在系统内存中建立一致性缓存的能力，并通过请求与响应方式为低延迟I/O事务创建缓存。而CXL.mem则与CXL.cache的功能相反，它允许主机处理器直接访问通过CXL连接的设备，尤其是那些具备低延迟加载/存储指令集的存储设备。
这三种协议能够以不同方式组合，支持三种不同类型的设备，为方便起见，分别简称为类型1、类型2和类型3。
  类型1设备融合了CXL.io和CXL.cache协议，允许无内部存储器的智能 NIC 或加速器等设备直接控制和访问系统存储器的区域。
  类型2设备在类型1的基础上，拥有额外的或内置的存储，并利用所有协议来允许系统或设备以两者间硬件支持的一致性在另一个存储器中分配区域。这两个不同的方向（由系统启动或由主机启动）分别称为主机偏置或设备偏置模式。
  类型3设备是由CXL.io和CXL.mem协议支持的存储设备，它们能够在DRAM、NVRAM和其他各类持久性及易失性存储设备上实现字节级内存寻址。通过这种架构，主机能够像访问本地内存一样访问其他系统的额外内存和专用内存扩展器。
  下图显示了这三种设备类型。
图 1：CXL设备类型，摘自 Compute Express Link Specification r.3.0，v1.0
相较于传统的PCIe事务，CXL.mem和CXL.cache拥有更低的延迟。例如，PCIe 5.0常见的延迟大约是100纳秒，而CXL 2.0只有20-40纳秒。CXL.mem所带来的延迟降低，使得通过利用类型3设备可以进行内存扩展。这意味着应用程序线程可以访问系统外的存储器，同时避免了因内存不足而导致作业失败的风险，这一功能被称为“内存池”。通过CXL 2.0得到加强，并可通过支持交换机附加内存来拓展此范式。这不仅让系统能外加内存，还可以向其他系统提供未使用的本地内存，从而提高利用率和降低初始系统成本。
CXL 3.0 规范中引入的内存共享允许多个主机访问给定的 CXL 附加内存分配。它还定义了结构连接内存扩展器的概念——这些设备可以包含各种类型的内存，以便进行池化和共享，并且可以实现本地内存分层，以代表主机优化池的性能特性。这为Cray Research定义的 SHMEM协议创造了一个有趣的替代方案，使多个主机对共享内存池进行极低延迟访问。由于本机总线互连介质，这不仅提供了比 SHMEM 库例程更好的性能，而且还为该共享内存池上的并行计算提供了可能简单得多的编程模型。
CXL延迟降低的另一个内在价值是，它还具有促进设备到设备内存事务的潜力，例如在一个或多个系统中使用多个 GPU，不需要花费专有的辅助总线或软件层来互连这些设备。这一点在小型AI训练场景中尤为明显，因为可以轻松展示即时性能影响。这种通过结构性连接将远程硬件直接整合进共享内存系统的方法，可能会开启AI训练的新篇章，特别是在数据中心，因为 CXL 引入了对称的对等设备通信功能，从而减少了对CPU的持续依赖。
总的来说，CXL结构为服务器分解提供了机会，有助于克服由于资源不在本地系统架构中而限制特定应用程序工作流的问题。例如，当存储可以被集中到通过结构连接的扩展器中，每个系统对独立（且孤立）存储的需求就会减少。专用内存总线和CXL提供的附加带宽有助于解决核心内存带宽瓶颈，允许设计和配置单个服务器，更注重性能而不是容量。要实现这一愿景，关键在于开发具有低延迟的CXL交换功能和灵活的内存分层系统，这些功能需要在支持软件和扩展器硬件中实现。
新思科技新方案，保障CXL的完整性和数据加密 鉴于 CXL 2.0中的外部交换和 CXL 3.0 中的增强结构的引入，随着数据在服务器外部电缆上传输，增强的总线安全性变得至关重要。因此，为了保护数据免受未经授权的访问或篡改，PCIe和CXL控制器可以使用完整性和数据加密 (IDE) 安全IP模块，即使是在数据被外部人员接触的情况下，也能确保数据的安全和隐私。
面对日益增长的安全需求，新思科技推出了一种创新方案，将高安全性的CXL控制器与符合标准的、可定制的IDE安全模块相结合。这一技术的目的是确保数据在SoC内部的传输过程中免遭篡改和物理攻击的威胁。更具体地说，该方案可以在 CXL.cache/.mem 协议情况下为 FLIT 提供保密性、完整性和重放保护，并在 CXL.io 的情况下为事务层数据包 (TLP) 提供保密性、完整性和重放保护。值得一提的是，这个系统不仅与控制器的数据接口总线宽度和通道配置相匹配，且在面积、性能和延迟方面都经过了精心优化，甚至在CXL.cache/.mem滑动模式情况下，可以实现几乎零延迟的数据传输。
CXL的未来展望 在经历了多年对OpenCAPI、GenZ等一致性协议的标准化努力后，整个行业开始聚焦于CXL。CXL控制器将采用一种名为“信用可扩展流”(CXS)的流接口协议，这种协议通过封装更新的CCIX版本，为多处理器架构带来了对称一致性。这一做法最初因为一致性成本的上升而在原生形态下遭遇了较高的延迟，尤其是在处理小型写入操作时。CXS.B（CXS的CXL托管版本）通过提供专门用于CPU间对称通信的流媒体通道对，巧妙地解决了这一挑战。
CXL技术的发展和应用正在深刻改变高性能计算领域，对于降低延迟、实现内存资源共享和服务器的功能解耦，都展现出了明显的进步和潜力。随着软件框架的不断完善，这种技术有望开启计算性能和效率的新纪元，满足日益增长的硬件生态系统的需求。作为PCIe和CXL物理层、控制器以及IDE和验证IP的领先提供商，新思科技凭借在超过1800个设计项目中的集成和验证经验，可以显著降低风险，协助SoC工程师加快产品推向市场的进程。
]]></content>
  </entry>
  
  <entry>
    <title>常见的电路保护元器件</title>
    <url>/post/hardware/common-circuit-protection-components.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>电路保护</tag>
    </tags>
    <content type="html"><![CDATA[电子电路很容易在过压、过流、浪涌等情况发生的时候损坏，随着技术的发展，电子电路的产品日益多样化和复杂化，而电路保护则变得尤为重要。电路保护元件也从简单的玻璃管保险丝，变得种类更多，防护性能更优越。
电路保护的意义 在各类电子产品中，设置过压保护和过流保护变得越来越重要，那么电路保护的意义到底是什么，今天就来跟大家聊一聊：
  由于如今电路板的集成度越来越高，板子的价格也跟着水涨船高，因此我们要加强保护。
  半导体器件，IC的工作电压有越来越低的趋势，而电路保护的目的则是降低能耗损失，减少发热现象，延长使用寿命。
  车载设备，由于使用环境的条件比一般电子产品更加恶劣，汽车行驶状况万变，汽车启动时产生很大的瞬间峰值电压等。因此，在为这些电子设备配套产品的电源适配器中，一般要使用过压保护元件。
  通信设备，通信场所对防雷浪涌有一定的要求，在这些设备中使用过压保护、过流保护元件就变得重要起来，它们是保证用户人身安全和通信正常的关键。
  大部分电子产品出现的故障，都是电子设备电路中出现的过压或者电路现象造成的，随着我们对电子设备质量的要求越来越高，电子电路保护也变得更加不容忽视。
  那么电路保护如此重要，常用的电路保护元件有哪些？今天就给大家介绍几种。
防雷器件 陶瓷气体放电管： 防雷器件中应用最广泛的是陶瓷气体放电管，之所以说陶瓷气体放电管是应用最广泛的防雷器件，是因为无论是直流电源的防雷还是各种信号的防雷，陶瓷气体放电管都能起到很好的防护作用。
其最大的特点是通流量大，级间电容小，绝缘电阻高，击穿电压可选范围大。
半导体放电管： 半导体放电管是一种过压保护器件，是利用晶闸管原理制成的，依靠PN结的击穿电流触发器件导通放电，可以流过很大的浪涌电流或脉冲电流。其击穿电压的范围，构成了过压保护的范围。
固体放电管使用时可直接跨接在被保护电路两端。具有精确导通、快速响应（响应时间ns级）、浪涌吸收能力较强、双向对称、可靠性高等特点。
玻璃放电管： 玻璃放电管（强效放电管、防雷管）是20世纪末新推出的防雷器件，它兼有陶瓷气体放电管和半导体过压保护器的优点：绝缘电阻高（≥10^8Ω）、极间电容小（≤0.8pF）、放电电流较大（最大达3 kA）、双向对称性、反应速度快（不存在冲击击穿的滞后现象）、性能稳定可靠、导通后电压较低。
此外还有直流击穿电压高（最高达5000V）、体积小、寿命长等优点。其缺点是直流击穿电压分散性较大（±20%）。
过压器件 压敏电阻： 压敏电阻也是一种用得最多的限压器件。利用压敏电阻的非线性特性，当过电压出现在压敏电阻的两极间，压敏电阻可以将电压钳位到一个相对固定的电压值，从而实现对后级电路的保护。
压敏电阻的响应时间为ns级，比空气放电管快，比TVS管稍慢一些，一般情况下用于电子电路的过电压保护其响应速度可以满足要求。压敏电阻的结电容一般在几百到几千pF的数量级范围，很多情况下不宜直接应用在高频信号线路的保护中，应用在交流电路的保护中时，因为其结电容较大会增加漏电流，在设计防护电路时需要充分考虑。压敏电阻的通流容量较大，但比气体放电管小。
贴片压敏电阻的作用：
贴片压敏电阻主要用于保护元件和电路，防止在电源供应、控制和信号线产生的ESD。
瞬态抑制二极管： 瞬态抑制器TVS二极管广泛应用于半导体及敏感器件的保护，通常用于二级保护。基本都会是用于在陶瓷气体放电管之后的二级保护，也有用户直接将其用于产品的一级保护。
其特点为反应速度快(为 ps 级) ，体积小 ，脉冲功率较大 ，箝位电压低等。其 10/1000μs波脉冲功率从400W ～30KW，脉冲峰值电流从 0.52A～544A ；击穿电压有从6.8V～550V的系列值，便于各种不同电压的电路使用。
过流器件 自恢复保险丝： 自恢复保险丝PPTC就是一种过流电子保护元件，采用高分子有机聚合物在高压、高温，硫化反应的条件下，搀加导电粒子材料后，经过特殊的工艺加工而成。自恢复保险丝（PPTC：高分子自恢复保险丝）是一种正温度系数聚合物热敏电阻，作过流保护用，可代替电流保险丝。
电路正常工作时它的阻值很小（压降很小），当电路出现过流使它温 度升高时，阻值急剧增大几个数量级，使电路中的电流减小到安全值以下，从而使后面的电路得到保护，过流消失后自动恢复为低阻值。
静电元件 ESD静电放电二极管： ESD静电放电二极管是一种过压、防静电保护元件，是为高速数据传输应用的I/O端口保护设计的器件。ESD静电二极管是用来避免电子设备中的敏感电路受到ESD(静电放电)的影响。
可提供非常低的电容，具有优异的传输线脉冲（TLP）测试，以及IEC6100-4-2测试能力，尤其是在多采样数高达1000之后，进而改善对敏感电子元件的保护。
电感的作用： 电磁的关系相信大家都清楚，电感的作用就是在电路刚开始的时候，一切还不稳定的时候，如果电感中有电流通过，就一定会产生一个与电流方向相反的感应电流(法拉第电磁感应定律)，等到电路运行了一段时间后，一切都稳定了，电流没有什么变化了，电磁感应也就不会产生电流，这时候就稳定了，不会出现突发性的变故，从而保证了电路的安全，就像水车，一开始由于阻力转动的比较慢，后来慢慢趋于平和。
磁珠的作用： 磁珠有很高的电阻率和磁导率，他等效于电阻和电感串联，但电阻值和电感值都随频率变化。他比普通的电感有更好的高频滤波特性，在高频时呈现阻性，所以能在相当宽的频率范围内保持较高的阻抗，从而提高调频滤波效果，在以太网芯片上用到过。
]]></content>
  </entry>
  
  <entry>
    <title>揭穿四个常见的AI/ML数据存储迷思</title>
    <url>/post/datacenter/busting-four-common-AI-ML-and-Data-Storage-Myths.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>AI</tag>
      <tag>ML</tag>
    </tags>
    <content type="html"><![CDATA[如今，消费者和商业用户在不知不觉中与人工智能和机器学习互动。从消费者的角度来看，这涉及从流媒体上观看喜爱的节目到随时叫车的车辆服务。从商业的角度来看，组织利用人工智能和机器学习来获得更好的洞察，支持其业务目标。
 AI  /ML是关于模式识别的。具体来说，它是指识别和理解实时模式以改进业务流程、经营业绩和人们的生活。根据2022年市场研究洞察报告，人工智能市场预计从2022年的3874.5亿美元增长到2029年的13943亿美元。
随着越来越多的组织采用这些深度学习技术，IT团队正在探索如何最好地理解如何以具有成本效益的方式构建和管理基础设施，以支持人工智能和机器学习为组织带来的机会，并支持其能够为未来的业务增长进行扩展。一个不容小觑的要素，而应该成为重点关注的焦点，是支持这些新兴应用所需的数据存储基础设施。
以下是需要揭穿的四个常见的AI/ML存储迷思。
AI/ML应用必须由高IOPS全闪存存储支持
为了“满足需求”，加速器需要在需要时随时提供数据。因此，这强调了AI/ML存储不仅仅关乎纯速度。全闪存存储系统具有令人印象深刻的高IOPS，但也可能耗尽您的IT预算。
与大多数AI/ML应用一样，加速器也有不同级别的性能。例如，目标识别应用中每个图像的计算时间足够长，以至于混合（HDD和SSD）系统可以作为全NVMe解决方案的可比解决方案，而价格要低得多。IT团队必须注意并平衡计算加速器、AI/ML工作负载和存储方案，以找到最佳解决方案。独立的基准报告，如MLPerf，可以在这方面提供帮助。
AI/ML完全依赖于GPU 在具有极高计算能力的现代GPU出现之前，今天使用的AI/ML应用和神经网络仅仅是一个有趣的概念而已。毫无疑问，加速器芯片对于AI/ML应用至关重要，但如果没有足够的存储和网络支持，它就毫无价值。
存储和网络被认为是“满足需求”的手段，它们确保在加速器完成当前数据集之前，下一组数据始终可用。因此，组织必须像仔细考虑GPU一样慎重选择存储和网络基础设施。每个元素必须平衡，以实现最佳结果：过多的存储性能或容量将被浪费，而过少则会导致昂贵的计算芯片闲置。
AI/ML可以有效利用专用的单一用途存储系统 当将AI/ML应用于其核心数据源时，组织能够从中获得最大价值。已经有银行采用这些技术进行欺诈检测，药品制造商可以更好地分析实验或生产的数据以加快药物开发。多家领先的零售商也正在将人工智能技术应用于其技术和业务基础设施的核心，以最好地满足客户的需求。许多企业不再将AI/ML视为试验性的边缘项目，而是作为业务的一部分和未来增长的催化剂。因此，这些应用最适合在公司核心的IT基础设施中使用专用存储系统。
分层降低AI/ML存储成本 分层存储是一种常见的策略，用于最大化存储资源并降低成本。将“热门”的关键任务和频繁访问的数据存储在昂贵且快速的存储介质上（例如SSD），而将很少访问或更新的“冷藏”归档数据存储在最便宜的存储设备上（例如磁带）。
由于不存在所谓的“冷藏”AI/ML数据，这种模型无法应用于这些类型的应用程序。
由于在每次训练运行中都会使用所有的AI/ML训练数据，将部分数据分层存储到不同的存储层将导致严重的性能问题。AI/ML存储解决方案必须将所有数据视为“热数据”，确保所有数据始终可用。
值得注意的是，AI/ML工作负载的准确性与可用的训练数据量成正比。这意味着存储基础设施必须能够在训练数据量扩大时无缝扩展。与存储分层相比，规模化的线性增长是这些环境的关键存储要求。
AI/ML的创新正在推动企业进行大规模的数字转型，从而实现更好的业务成果。如果使用和管理不当，它将影响组织的几乎所有方面，并不是以积极的方式。根据2022 Gartner Hype Cycle，许多技术预计在未来两到五年内实现主流应用，例如边缘人工智能、决策智能和深度学习。在组织踏上自己的数字化之旅时，不要将基础存储基础设施作为一个次要考虑因素，因为它在实现组织最大化AI/ML应用潜力的成功中扮演着关键角色。
]]></content>
  </entry>
  
  <entry>
    <title>yum安装PHP5.4的亲身体验系统管理员</title>
    <url>/post/linux/how-to-install-PHP5.4-through-yum.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>CentOS</tag>
      <tag>yum</tag>
      <tag>PHP5.4</tag>
    </tags>
    <content type="html"><![CDATA[本文将以一位系统管理员的角度，分享在CentOS系统上使用yum安装PHP 5.4的亲身体验。通过8个方面的详细介绍嵌入式linux驱动程序设计从入门到精通，帮助读者顺利完成安装，并提供一些实用的技巧和注意事项。
安装yum 首先，我们需要确保系统已经安装了yum软件包管理器。通过命令行输入“yum”，如果出现相关信息，则说明已经安装成功。若未安装，可以通过搜索引擎找到相关教程进行安装。
配置yum源 为了能够顺利安装PHP 5.4，我们需要配置正确的yum源。可以选择官方源或其他可信赖的源。根据系统版本和架构选择对应的源，并将其配置到yum的配置文件中。
更新yum缓存 在安装之前，务必执行“yum makecache”命令来更新yum缓存。这样可以确保我们获取到最新的软件包列表，避免因为旧版本软件包而导致的问题。
安装PHP 5.4 通过执行“yum install php54”命令来安装PHP 5.4及其相关依赖包。在此过程中，可以根据需要选择其他附加模块进行安装。
配置PHP 安装完成后，我们需要进行一些基本的配置。可以编辑php.ini文件来修改一些常用的配置项，如时区、内存限制等。同时centos yum php 54，也可以根据实际需求加载或禁用一些扩展模块。
启动PHP 安装完成后linux命令ls，默认情况下，PHP会自动启动。可以通过命令“service php-fpm start”来手动启动PHP服务。如果需要设置开机自启动，可以执行“chkconfig php-fpm on”命令。
测试PHP 为了验证安装是否成功，我们可以创建一个简单的php文件，并在浏览器中访问该文件。如果能够正常显示phpinfo信息，则说明安装成功。
常见问题及解决方法 最后，我们还将介绍一些常见的安装问题及解决方法。例如，遇到依赖问题时应如何处理；如何切换不同版本的PHP等等。这些问题和解决方法将帮助读者更好地应对实际情况。
通过以上8个方面的介绍，相信读者已经对在CentOS系统上使用yum安装PHP 5.4有了初步的了解。希望本文能够为读者提供实用的指导和帮助centos yum php 54，让大家能够顺利完成相关操作，并在使用中发挥出PHP 5.4的优势。
]]></content>
  </entry>
  
  <entry>
    <title>数据中心的机架密度：何以见高峰</title>
    <url>/post/datacenter/data-center-rack-density-how-high-can-it-go.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Rack Density</tag>
      <tag>HPC</tag>
      <tag>AI</tag>
    </tags>
    <content type="html"><![CDATA[人工智能（ AI  ）和高性能计算（HPC）已将计算、存储容量和网络资源的需求推向了极限。更强大的芯片是推动机架密度不断提升的主要驱动力之一。仅仅十年前，平均处理器的功耗还不到100瓦，但如今它们的功耗已经达到了约500瓦。
随着电力在有限的空间内变得更加充裕，应用开发人员不断寻找利用这一潜力的方法，并迫切需要更多的算力。Hyperion Research的首席执行官Earl Joseph指出，HPC的增长主要源于更加复杂和苛刻的应用程序的涌现。
“人工智能、机器学习和深度学习每年以接近30%的速度增长，” Joseph表示。
现在让我们一起来探讨机架密度的演变以及它可能达到的高度。
饥不择食的需求 数据中心正以前所未有的数量进行规划、建设和投入运营。根据Uptime Institute的统计数据，预计到2025年，数据中心的电力占地面积将增加50%，而2019年至2025年的全球数据产量将增长500%。数据中心的兴建数量创下历史新高，计算密度远远超越以往。为了满足这种需求，数据中心必须在每个机架或每平方英尺的空间内提供更多的算力。
十多年前，根据Uptime Institute的数据，每个机架的平均功率密度仅在4-5千瓦左右。然而，到了2020年，这个数字已经飙升至每个机架8-10千瓦。值得注意的是，在美国进行的数据中心调查中，有三分之二的数据中心表示，它们已经在每个机架16-20千瓦的功率密度范围内迎来了峰值需求。而截止到2022年的最新数据则显示，有10%的数据中心报告每个机架的功率密度已达到20-29千瓦，7%的数据中心每个机架的功率密度达到30-39千瓦，3%的数据中心每个机架的功率密度达到40-49千瓦，还有5%的数据中心每个机架的功率密度高达50千瓦或更高。显然，现代应用程序和数据量正在推动机架密度达到前所未有的高水平。
巨头主导的推力 在过去的十年里，像亚马逊、Facebook、谷歌和微软等超大规模数据中心一直在密度增长方面发挥着引领作用。他们开创了更佳冷却和供电方法，同时在有限的空间内提供尽可能多的算力。一些数据中心的机架已经演化到每个机架50千瓦甚至更高的功率密度。
然而，有趣的是，这些超大规模数据中心现在更愿意将最高密度的机架留给其他玩家。虽然他们曾将整个行业推向新的密度水平，但如今更倾向于在每个机架约30千瓦左右的密度范围内稳健运营。由于他们拥有规模庞大和高效率的优势，他们能够满足大多数用户的需求。超大规模数据中心所追求的是优化。他们需要的是高度（但不过高）的机架密度，这种密度可扩展，并且要以有吸引力的价格提供。
专业高密度数据中心 针对高性能计算（HPC）和人工智能（AI）市场，出现了专业的高密度数据中心供应商，它们正在从意想不到的领域赢得业务。
“尽管过去，HPC主要是亿美元营业额的大企业和研究机构的专属领域，但如今，越来越多规模较小的企业正在将其用于获取竞争优势，” Oper8 Global的首席执行官Mike Andrea指出。
他指出，HPC应用程序的民主化已经从大型、经济充裕的组织扩展到规模较小的研发单位，以及航天、地震工程、三维建模、自动驾驶汽车仿真、各种AI应用案例、能源、石油和天然气生产、天气预测、数据分析、医疗保健和三维电影渲染等领域。对HPC的需求不断增加。
“延迟问题仍然是HPC的主要驱动因素，与此同时，数据中心必须支持每个机架超过100千瓦的高机架密度，” Andrea表示。他的公司正在与多个客户合作，这些客户要求每个机架的功率从80千瓦到200千瓦不等。
为了在这一市场竞争中脱颖而出，高密度数据中心必须位于其客户附近，以减小延迟。因此，这些专门提供高密度服务的数据中心很可能只会在一些特定地区找到市场，这些地区有一批对高密度要求非常苛刻的客户。
逐步拓展的HPC机架 另一个趋势是，数据中心开始通过部署仅有一个或两个高密度机架来服务不断增长的HPC市场。一些边缘和合作数据中心开始实施一个高密度HPC机架单元，包括两到十二个机架，或者将高密度HPC机架与更常规密度的机架组成的集群并置。这种策略有助于数据中心满足一两个客户的需求，同时无需大规模投资对整个数据中心进行彻底重新设计。
然而，即使只引入一个或两个机架，也需要大量的工作。除了新的服务器和支持设备外，还需要进行布线和其他改进。这还要求有足够的额外电力供应，并确保HPC机架可以获得足够的冷却。因此，高密度数据中心可能需要投资计算流体动力学（CFD）技术，以增强空气和冷却流动，以避免出现热点问题。
此外，它们需要采用先进的冷却技术，甚至可能需要某种形式的液体冷却来保持新机架的温度在可接受范围内。极高温度的机架可能会导致电力配电单元（PDU）发生故障，因为机架后部积聚了大量的热空气。特别是在超过35千瓦的机架上，这一问题尤为明显。
“当高功耗元件安装在有限的空间内时，采用液冷技术可能变得不可或缺，”戴尔科技高性能计算（HPC）和新兴工作负载高级工程总监Onur Celebioglu指出。
基于水冷的方案，如主动冷却机架门、采用冷板直接液冷和液体浸没冷却，正在HPC机架中变得越来越常见。但需要注意的是，采用液冷的HPC应用可能需要更宽更深的机柜，以容纳额外的电力供应和液体管道。这些成本，再加上需要对现有数据中心进行大规模重新配置的情况，可能会使一些人望而却步，不进入HPC市场。
未来的高密度 不久前，高密度机架被认为是10千瓦或更多。与现代的密度数字相比，这看起来相当微不足道。没有人知道密度能够达到多高。但在未来几年，让我们为一些惊人的数字做好准备吧。
“如今，高密度机架大约在40千瓦到125千瓦之间，而极高密度机架甚至可达到200千瓦甚至更高，” Andrea表示。
]]></content>
  </entry>
  
  <entry>
    <title>什么是Page Cache</title>
    <url>/post/linux/what-is-page-cache.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>page cache</tag>
    </tags>
    <content type="html"><![CDATA[什么是Page Cache Page Cache，翻译为页高速缓冲存储器。它是动态变化的，因为操作系统会将所有未直接分配给应用程序的物理内存都用于页面缓存。Page Cache是文件系统层级的缓存，用于缓存文件的页数据，属于内核管理的内存。从磁盘中读取到的内容是存储在page cache里的。
为什么需要Page Cache Page Cache机制的目的是为了减少IO，提升IO磁盘读写的效率。由于程序的时间局部性和空间局部性，读写过的文件在下次还可能再次读取，如果每次读写文件都去磁盘中获取，显然读写性能太差，因为磁盘的读写速率相对于内存来说，慢了不止一点点。
因为有Page Cache机制，所以我们可以发现读写一个文件第一次非常慢，但是第二次就会变得很快，这是因为第一次读写这个文件的时候，Linux内核已经把文件内容缓存到了内存中的Page Cache里面，第二次读写的时候，由于发现文件内容已经在内存中了，就直接从内存中读取了，这显然比从硬盘读取快很多。
Page Cache的机制是很复杂的，那我们可不可以不用Page Cache呢？
答案当然是可以的，我们可以在应用层实现自己的类似这种的Cache机制，比如MySQL的Buffer Pool，我们也可以在使用open打开文件时指定为Direct I/O来绕开Page Cache，所以说是否使用Page Cache还是由应用程序自己决定，Linux内核只是提供了这种机制，并非要求我们强制使用。
Linux中Page Cache含义的变化 在 Linux 的实现中，文件 Cache 分为两个层面，一是 Page Cache，另一个是 Buffer Cache（块缓存）。page cache用于缓存文件的页数据，大小通常为4K；Buffer cache用于缓存块设备（如磁盘）的块数据，大小通常为1K。
在Linux2.4版本的内核之前，page cache和buffer cache是完全分离的。但是块设备大多数是磁盘，磁盘上的数据又大多通过文件系统来组织，这种设计导致很多数据被缓存了两次，浪费内存空间。
所以在2.4版本内核之后，两块内存近似融合在了一起，如果一个文件的页加载到了page cache，那么buffer cache只需要维护块指向页的指针。
在2.6版本内核中，page cache和buffer cache进一步结合。每一个 Page Cache 包含若干 Buffer Cache。将文件一页一页缓存到page cache中，buffer cache里面的指针指向磁盘block。
2.6内核中的buffer cache和page cache在处理上是保持一致的，但是存在概念上的差别，page cache是针对文件的cache，buffer是针对磁盘块数据的cache，仅此而已。
Page Cache大小的计算 通过命令cat /proc/meminfo可以看到Linux内存管理统计相关的各项数据：
Page Cache的大小有如下计算公式：
Page Cache = Buffers + Cached + SwapCached = Active(file) + Inactive(file) + Shmem + SwapCached 先对等号左边的字段做一个说明：
Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据，一般不会特别大（20MB 左右），Buffers 既可以用作“将要写入磁盘数据的缓存”，也可以用作“从磁盘读取数据的缓存”。
Cached 是从磁盘读取文件的内存页缓存，但是不包括SwapCached，也就是用来缓存从文件读取的数据。Cached 既可以用作“从文件读取数据的页缓存”，也可以用作“写文件的页缓存”。
SwapCached 是在打开了 Swap 分区后，把 Inactive(anon)+Active(anon) 这两项里的匿 名页给交换到磁盘（swap out），然后再读入到内存（swap in）后分配的内存。由于读入到 内存后原来的 Swap File 还在，所以 SwapCached 也可以认为是 File-backed page，即属 于 Page Cache。这样做的目的也是为了减少 I/O。注意，SwapCached 只在 Swap 分区打开的情况下才会有，我这个环境是关闭掉swap的，所以SwapCached为0。
再来看右边的字段：
在 Page Cache 中，Active(file)+Inactive(file) 是 File-backed page（与文件对应的内存 页），是最需要关注的部分。因为我们平时用的 mmap() 内存映射方式和 buffered I/O 来消 耗的内存就属于这部分。Active和Inactive的区别在于内存空间中是否包含最近被使用过的数据。当物理内存不足，不得不释放正在使用的内存空间时，会优先释放Inactive的内存空间。Linux内核中使用LRU表来分别记录对应的这两类文件内存页。
Page Cache 中的 Shmem 是指匿名共享映射这种方式分配的内存 （free 命令中 shared 这一项），比如 tmpfs（临时文件系统）。
free命令看到的buff/cache又是什么 Page Cache的概念很容易跟free命令看到的buff/cache混淆，所以这里我们有必要区分一下。
free 命令中的 buff/cache 是由 Buffers、Cached 和 SReclaimable 这三项组成的，它强调的是内存的可回收性，也就是说，可以被回收的内存会 统计在这一项。它只是free命令为了统计内存可回收性人为将这三个值进行统计到一起的。
buff/cache的大小还是来源于/proc/meminfo中看到的内存统计信息，计算公式如下：
buff/cache = Buffers + Cached + SReclaimable Buffers和Cached的含义前面已经讲过了，我们来看看SReclaimable：
SReclaimable 是 Slab 的一部分，是可以被回收的，例如缓存，linux内核使用 Slab 机制，管理文件系统的目录项和索引节点的缓存。Slab 包括两部分，其中的可回收部分，是指可以被回收的内核内存，包括目录项（dentry） 和索引节点（ inode ）的缓存等，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录。
Linux 文件系统为每个文件都分配两个数据结构，索引节点（index node）和目录项（directory entry）。它们主要用来记录文件的元信息和目录结构。
索引节点，简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以记住，索引节点同样占用磁盘空间。
目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存。
换句话说，索引节点是每个文件的唯一标志，而目录项维护的正是文件系统的树状结构。目录项和索引节点的关系是多对一，你可以简单理解为，一个文件可以有多个别名。举个例子，通过硬链接为文件创建的别名，就会对应不同的目录项，不过这些目录项本质上还是链接同一个文件，所以，它们的索引节点相同。
目录项本身就是一个内存缓存，而索引节点则是存储在磁盘中的数据。在前面的 Buffers 和 Cached 原理中，我们提到过，为了协调慢速磁盘与快速内存和 CPU 的性能差异，文件内容会缓存到页缓存 Cache 中。其实，这些索引节点也会缓存到内存中，加速文件的访问。
可以通过下面这张图，理解一下目录项、索引节点以及文件数据的关系：
总结 内存页包括文件页和匿名页，内核缓存的磁盘数据（Buffer）和内核缓存的文件数据（Cache）都叫作文件页，包括page cache、slab中的dcache、icache、用户进程的可执行程序的代码段。
匿名页包括进程使用各种api（malloc,mmap,brk/sbrk）申请到的物理内存(这些api通常只是申请虚拟地址，真实的页分配发生在page fault中)，包括堆、栈，进程间通信中的共享内存，bss段，数据段，tmpfs的页。
文件页和匿名页两个内存的区别在于，物理内存的内容是否与物理磁盘上的文件相关联，文件页与物理磁盘的文件是有关联的，而匿名页没有。
可以看出来，Page Cache的大小等于内核磁盘数据和文件数据的缓存与匿名页通过Swap机制交换出去的内存大小之和，也等于活跃文件页、未活跃文件页、匿名共享映射内存与匿名页通过Swap机制交换出去的内存大小之和。
而free命令看到的buff/cache等于内核磁盘数据和文件数据的缓存与 Slab 机制中文件系统的目录项和索引节点的缓存的可回收部分之和，指的是可直接回收的内存大小。
所以也可以看出来，文件页的缓存，在内存不足时是可以直接回收的（或者是脏页先会写到磁盘再回收），而匿名页是程序动态申请的内存，是不能直接回收的，即使是匿名页通过Swap机制换出的内存，以后也是得再从磁盘换入的。
另外，对于上面Page Cache和buff/cache的计算公式，我们需要注意一下，在做比较的过程中，一定要考虑到这些数据是动态变化的，而且执行 命令本身也会带来内存开销，所以这个等式未必会严格相等。
]]></content>
  </entry>
  
  <entry>
    <title>Intel下代至强缓存暴增至448MB</title>
    <url>/post/datacenter/intel-next-xeon-Emerald-Rapids.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>EPYC</tag>
      <tag>Bergamo</tag>
      <tag>9754</tag>
    </tags>
    <content type="html"><![CDATA[Intel已经宣布，将在12月14日正式发布第五代可扩展至强Emerald Rapids，和酷睿Ultra同一天。
它虽然只是Sapphire Rapids四代至强的升级版，不如明年Intel 3工艺的Granite Rapids、Sierra Forest变化那么大(后者288个小核)，但升级亮点依然不少。
YuuKi_AnS放出了高端型号至强铂金8580的软件识别截图，证实为双芯片整合，60核心120线程，二级缓存每核心2MB、总计120MB，三级缓存多达300MB，合计420MB，比现在多了2.6倍。
更关键的是，向上还有64核心旗舰，三级缓存增加到320MB，再加上128MB二级缓存，合计就是448MB，比现在增加足足3倍。
当然，AMD EPYC那边更猛，96核心旗舰EPYC 9654 96MB二级缓存、384MB三级缓存，合计480MB，3D缓存加持的EPYC 9684X也又堆叠了768MB，合计1248MB。
AMD Zen5如果能把二级缓存翻倍到每核心2MB，总缓存量又是一次突飞猛进。
泄露消息显示，五代至强还会提升DDR5内存支持的频率，优化电源模式，集成Intel加速器引擎以提升能效，总体能效提升可达17％。
回到桌面上，Intel最初计划在Meteor Lake这一代产品上更换新的接口LGA1851，但因为新的Intel 4工艺不够成熟，性能上不去，所以只能用于主流和轻薄笔记本，而高端游戏本、桌面都由13代酷睿的升级版14代酷睿来撑场面。
事实上，Meteor Lake-S桌面版一度曾经做出样品，但最终被砍掉。
Meteor Lake-S桌面版样品
就在酷睿Ultra架构技术已经官宣、将于12月14日正式发布的时刻，Intel执行副总裁、客户端计算事业部总经理Michelle Johnston Holthaus却给出了惊人的说法。
他确认，Meteor Lake确实会有桌面版，将在2024年发布，架构和移动版是完全一致的！
从目前的情况看，Meteor Lake不可能突然变得足够高性能，达到i9-13900K这样的性能，毕竟已经可以基本确认，i9-14900K等首批六款K/KF型号将在10月17日发布，主流和低功耗版本的型号也已经流出。
所谓的Meteor Lake桌面版，极有可能类似当年Boardwell五代酷睿唯二的桌面版i7-5775C、i5-5675C，本质上仍然是移动版本，只不过改成了桌面封装接口，当然不可能是下一代LGA1851，而肯定是现在的LGA1700。
理论上，这样的产品型号不会多，也算是Meteor Lake最后的倔强了，但是也不排除完全可以满足65W主流市场需求，毕竟6+8核心用于i5、i3系列是足够了。
]]></content>
  </entry>
  
  <entry>
    <title>CPU架构 C-states</title>
    <url>/post/datacenter/cpu-architecture-c-state.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>CPU</tag>
      <tag>C State</tag>
    </tags>
    <content type="html"><![CDATA[Power and Performance是使用CPU当中一个重要的课题，不管是芯片厂商，系统厂商，还是互联网大厂都会有专门的人员来做这方面的调优。
我们希望CPU可以做到，静若处子，动若狡兔。
当我需要你的时候，你就疯狂的运行，追求极致的性能，当我不需要你的时候，请你安静地休息，睡得越悄无声息越好。在之前的文章中，我们介绍了CPU的几种电源状态和性能状态，今天我们就来讲一讲其中的C-states。
C-states介绍 在CPU中，C-states（C状态）为软件提供了请求CPU进入低功耗状态的能力，C-states就是通过关闭core或其他逻辑单元来实现的。
如果CPU支持同时多线程（SMT：simultaneous multithreading ），那么单个CPU core可以支持多个软件线程。每个硬件线程都有自己的状态，并有机会请求不同的C-states。这些被称为线程C-states，以TCx表示（其中x是一个整数）。为了让一个core进入core C-state（以CCx表示），该core上的每个线程必须请求该状态或更深层次的状态。例如，对于支持两个线程的core，如果任何一个线程处于TC0状态，那么该core必须处于CC0状态。如果一个线程处于TC3状态，另一个线程处于TC6状态，那么该core将被允许进入CC3状态。线程C-states本身仅能节省少量的功耗，而core C-states可以显著地节省功耗。
同样的道理，还有package C-states，当一个CPU的所有core进入深度C-state时，那么整个CPU的package就可以进入这些状态。这些状态通常表示为PCx或PkgCx（其中x是一个整数）。通常来说，package C-states的编号与该package中的core状态相关联，不过也不一定是这样。例如，某些服务器处理器上的PC2状态是在该package中的所有core都处于CC3或CC6状态，但还会有一些条件限制使得CPU只能停留在PC2，而不是更深的休眠状态。
在实际应用中，C-states的运用对于节能和功耗管理至关重要。通过充分利用C-states，我们可以实现对CPU的动态功耗管理，根据实际使用情况灵活调整CPU的性能与功耗平衡。对于移动设备和笔记本电脑来说，C-states可以显著延长电池续航时间，提供更好的用户体验；对于服务器和数据中心来说，C-states可以降低能源消耗，提高整体系统效率。
然而，C-states的管理也存在一些挑战。合理的C-states策略需要综合考虑性能需求、功耗控制和响应速度。过于激进的C-states配置可能导致性能下降，而过于保守的配置则无法发挥节能潜力。
Thread C-states 软件可以在线程级别请求进入C-states。当一个线程进入线程C-state时，如果没有引发core C-state，那么通常不会产生可见的功耗节省效果，或者节省的功耗非常有限。在支持SMT的CPU上，线程C-states实际上是进入core C-states的一个中间步骤。而在不支持SMT的CPU上，线程C-states和core C-states实际上是相同的。
通过将线程级别的C-states和core级别的C-states结合起来管理，我们可以更精确地控制CPU的功耗和性能。在一些高性能应用场景中，我们可以让某些线程进入较浅的C-states，以保持高性能，而其他较空闲的线程则可以进入更深的C-states，以实现节能效果。这种精细的管理方式可以使得CPU在兼顾性能的同时，最大限度地降低功耗。
Core C-States 在C-states中，Core C-states的作用是确定core是处于开启还是关闭状态。在正常执行中，core通常处于C0状态，即活跃状态。当软件（通常是操作系统）指示逻辑处理器进入空闲状态时，它将进入一个C-state。
各种唤醒事件可能触发core重新开始执行代码（常见的例子是中断和定时器）。软件向CPU提供关于应该进入的状态的提示。MWAIT指令用于告诉CPU进入C-state，并包含有关所需状态的参数。然而，CPU的电源管理子系统有权执行其认为最佳的状态（这称为C-state降级 C-state demotion），也就是说将在外，君命有所不受。操作系统可以建议CPU休息，但是CPU表示我还可以继续肝！
Intel的不同代的CPU的C-state定义并没有硬性规定，但在跨多个产品系列中，这些定义基本保持一致。
]]></content>
  </entry>
  
  <entry>
    <title>SR-IOV技术简介</title>
    <url>/post/datacenter/introduction-of-SR-IOV-technology.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>SR-IO</tag>
    </tags>
    <content type="html"><![CDATA[虚拟化场景都会听到SR-IOV，究竟什么是SR-IOV呢，下面资料可参考，具体实现步骤请再硬件满足的情况下自行尝试。
基础概念 SR-IOV全称为Single Root I/O Virtualization（单根输入/输出虚拟化），是一种硬件加速的虚拟化技术，它允许多个虚拟机同时访问物理设备，从而提高虚拟机的性能和可靠性。SR-IOV技术是通过将单个物理设备划分为多个虚拟设备或虚拟端口（即一张物理网卡虚拟化成多个虚拟网卡给虚拟机(VM)用），为每个虚拟机提供独立的物理通道。这样，每个虚拟机可以直接访问独立的虚拟设备或虚拟端口，而无需在主机操作系统和虚拟化层之间进行上下文切换。
SR-IOV中有两个PCIe的function types：
  物理功能 (Physical Function, PF)：用于支持 SR-IOV 功能的 PCI 功能，每个PF都可以被物理主机发现和管理。PF 包含 SR-IOV 功能结构，用于管理 SR-IOV 功能。PF 拥有完全配置资源，可以用于配置或控制 PCIe 设备。进一步讲，借助物理主机上的PF驱动可以直接访问PF所有资源，并对所有VF并进行配置，比如：设置VF数量，并对其进行全局启动或停止。
  虚拟功能 (Virtual Function, VF)：PF虚拟出来的功能，VF 是一种轻量级 PCIe 功能，仅允许拥有用于其自身行为的配置资源。一个或者多个VF共享一个PF，其驱动装在虚拟机上，当VF分配给虚拟机以后，虚拟机就能像使用普通PCIe设备一样初始化和配置VF。如果PF代表的是一张物理网卡，那么VF则是一个虚拟机可以看见和使用的虚拟网卡。
  每个 SR-IOV 设备都可有一个物理功能 (Physical Function, PF)，并且每个 PF 最多可有 64,000 个与其关联的虚拟功能 (Virtual Function, VF)。PF 可以通过寄存器创建 VF，这些寄存器设计有专用于此目的的属性。只要在 PF 中启用了 SR-IOV，就可以通过 PF 的总线、设备和功能编号（路由 ID）访问各个 VF 的 PCI 配置空间。每个 VF 都具有一个 PCI 内存空间，用于映射其寄存器集。VF 设备驱动程序对寄存器集进行操作以启用其功能，并且显示为实际存在的 PCI 设备。创建 VF 后，可以直接将其指定给 IO 或各个应用程序（如裸机平台上的 Oracle Solaris Zones）。此功能使得虚拟功能可以共享物理设备，并在没有 CPU 和虚拟机管理程序软件开销的情况下执行 I/O，即可跳过中间的虚拟化堆栈（即VMM层），以达到近乎于纯物理环境的性能。
SR-IOV技术的优缺点及应用场景 SR-IOV的缺点：   高性能：SR-IOV技术可以降低虚拟机与物理设备之间的通信延迟，提高虚拟机的性能和响应速度。
  简化管理：SR-IOV技术可以使虚拟机直接访问物理设备，从而简化了虚拟化环境的管理。
  提高可靠性：SR-IOV技术可以将物理设备的错误隔离到虚拟机级别，从而提高了系统的可靠性。
  提高安全性：SR-IOV技术将物理设备划分成多个虚拟设备，可以使不同虚拟机之间的数据传输更加安全。
  SR-IOV的缺点： 使用了VFs的虚拟机不能在线迁移。
应用场景：SR-IOV技术在云计算、虚拟化、服务器应用等领域得到了广泛应用。在云计算数据中心中，SR-IOV技术可以提高虚拟机的网络和存储性能，允许灵活的分配和管理资源，从而降低了资源消耗和成本。在服务器应用中，SR-IOV技术可以支持高密度虚拟化、高性能计算和大规模分布式存储。
如何实现SR-IOV功能？ 前提首先SR-IOV需要软硬都支持才行：
  一台支持SR-IOV的主机或服务器（主板），SR-IOV功能在BIOS中已开启；
  一块支持SR-IOV且能安装于上述设备的网卡。
  步骤如下：确认测试的设备在BIOS打开VT-D及SRIOV支持选项；进入ESXi硬件配置页面，找到我们需要进行配置的SR-IOV网卡；两个网口，配置某一个网口为SR-IOV口。并设置虚拟VF的数量；保存配置之后，重新启动ESXi，就可以看到硬件上出现对应的虚拟网卡设备。这样子我们就完成了SR-IOV配置。
]]></content>
  </entry>
  
  <entry>
    <title>2023Q2全球WLAN市场：思科、HPE、华为位列前三</title>
    <url>/post/datacenter/worldwide-top-5-enterprise-wlan-companies.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>WLAN</tag>
    </tags>
    <content type="html"><![CDATA[根据IDC全球季度无线局域网跟踪报告发布的结果，WLAN企业细分市场在2023年第二季度的收入同比增长43.3%，达到30亿美元；WLAN消费细分市场同比下降14.0%。
Wi-Fi 6 和 Wi-Fi 6E 作为 WLAN 行业的最新标准，将继续推动企业细分市场的增长。与此同时，Wi-Fi 6E 将 Wi-Fi 的使用扩展到 6 GHz 频谱频段，增长势头持续强劲，23 年第二季度，收入环比增长 51.5%。2023 年第二季度，Wi-Fi 6 占消费市场收入的 51.6%。
IDC 企业网络研究经理Brandon Butler表示：“推动 WLAN 市场增长的一个重要因素仍然是组件短缺和供应链中断的缓解，这使得供应商从完成积压的产品订单中获得收入。同时，新 Wi-Fi 标准的采用也增加了市场动力。”
企业 WLAN 市场在全球范围内普遍取得了强劲的业绩。2023 年第二季度，美国市场同比增长 79.5%，加拿大市场同比增长 79.1%，拉丁美洲市场同比增长 49.6%。西欧市场同比增长 29.7%，中欧和东欧市场同比增长 42.0%；在中东和非洲地区，市场收入增长了51.0%；亚太地区（不包括日本和中国），市场上涨36.8%；中国市场下降了 12.5%，日本市场上涨了 14.6%。
企业市场份额 2023 年第二季度，思科企业 WLAN 收入同比增长 65.6%，达到 13 亿美元。截至本季度末，该公司的市场份额为43.5%。
HPE Aruba Networking 收入同比增长 55.3%，该季度市场份额为 16.2%。
华为企业WLAN收入同比增长26.6%，2023年第二季度的市场份额达到7.5%。
Ubiquiti 企业 WLAN 收入同比增长了 4.4%，该季度市场份额为 5.8%。
CommScope 企业 WLAN 收入同比增长 72.2%，市场份额为 4.8%。
Juniper Networks 企业 WLAN 收入同比增长 39.3%，市场份额为3.7%。
]]></content>
  </entry>
  
  <entry>
    <title>866.5亿！全球以太网交换机市场Top 3 出炉</title>
    <url>/post/datacenter/worldwide-top-3-ethernet-switch-companies.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Network</tag>
      <tag>Ethernet</tag>
      <tag>Top 3</tag>
      <tag>Revenue</tag>
    </tags>
    <content type="html"><![CDATA[据IDC报告显示，2023年第二季度全球以太网交换机市场收入为118亿美元（约866.5亿人民币），同比增长 38.4%。2023 年第二季度，全球企业和服务提供商 (SP) 路由器市场总收入达到 46 亿美元，同比增长 9.4%。
以太网交换机市场 2023 年上半年以太网交换机市场与 2022 年上半年相比增长了 35.2%。市场增长的主要推动力仍然是供应链问题的缓解。随着组件可用性的提高，供应商能够通过履行积压的产品订单来增长收入。
这一趋势在以太网交换市场的非数据中心领域尤其重要，2023 年第二季度非数据中心的收入同比增长 52.5%，端口出货量增长 16.6%。第二季度数据中心部分市场收入同比增长 21.7%，端口出货量下降 2.4%。
在超大规模企业和云提供商构建数据中心网络容量的推动下，高速以太网交换机市场在数据中心部分继续保持强劲增长：
 2023 年第二季度200/400 GbE 交换机的市场收入同比增长 61.9%。 100GbE 交换机收入增长 18.5%。 25/50 GbE 收入同比增长 54.2%。 ODM（原始设备制造商）直销仍然是数据中心细分市场的重要组成部分，占数据中心细分市场收入的12.6%，与2022年第二季度相比增长12.2%。  通常部署在企业园区和分支机构的低速交换机也表现出了优势：
 1GbE 交换机的收入同比增长 53.1%。 10GbE 交换机同比增长 18.1%。 2.5/5GbE 交换机（也称为多千兆以太网交换机）收入同比增长 157.5%。  从地域角度来看，以太网交换机市场在全球大部分地区都出现了增长：
 美洲第二季度市场收入同比增长 54.3%，其中美国增长 51.8%，拉丁美洲增长 88.3%。 欧洲市场同比增长49.1%，其中中东欧增长60.8%，西欧增长44.0%。 中东和非洲地区收入同比增长62.4%。 亚太地区市场增长7.7%，其中中国市场同比下降7.8%，日本市场增长18.9%。  路由器市场 2023 年第二季度，包括通信服务提供商和云服务提供商在内的服务提供商部分占市场总收入的 77.5%，同比增长 14.8%。企业部分的收入下降 6.1%%。
从地域角度来看：
 美洲的综合服务提供商和企业路由器市场合计增长了10.3%。 亚太地区的市场同比增长了3.0%。 欧洲、中东和非洲 (EMEA) 地区市场同比增长18.3%。  厂商排名 思科2023年第二季度以太网交换机收入同比增长55.3%，市场份额达到47.2%。思科的服务提供商和企业路由器合并收入在本季度增长了 18.1%，市场份额达到 35.9%。
Arista Networks 的以太网交换机收入在 2023 年第二季度同比增长 42.6%，市场份额为 10.4%。
华为第二季度以太网交换机收入增长17.7%，市场份额为9.0%。服务提供商和企业路由器收入合计增长了 10.8%，市场份额为 33.3%。
HPE 的以太网交换机收入在第二季度增长了78.8%，市场份额为7.1%。
H3C 的以太网交换机收入第二季度同比下降 10.9%，市场份额为 4.1%。在服务提供商和企业路由综合市场中，H3C 的收入下降了 10.0%，市场份额为 2.0%。
瞻博网络的以太网交换机收入在第二季度同比增长 35.2%，市场份额为 2.9%。瞻博网络第二季度的路由收入同比增长 2.5%，市场份额为 10.3%。
]]></content>
  </entry>
  
  <entry>
    <title>降低29%能耗，AMD EPYC吊打同行</title>
    <url>/post/datacenter/amd-epyc-helps-reducing-power-consumption.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>EPYC</tag>
      <tag>Bergamo</tag>
      <tag>9754</tag>
    </tags>
    <content type="html"><![CDATA[在碳减排的今天，数据中心因其高能耗密度而受到严格审查。据 Energy Information, Policy &amp; Technology LLC 估计，数据中心消耗了全球约 1% 电力。美国能源部计算得出，数据中心耗电量占美国用电总量 2%，数据中心成为碳减排的主要对象。
开放数据中心委员会 ODCC 测算数据显示， 2020年中国数据中心能耗总量为 939 亿千瓦时， 碳排放量为 6464 万吨。 预计2030 年中国数据中心能耗总量将达到 3800 亿千瓦时左右， 碳排放增长率将超过 300%。
数据中心的能耗大户 服务器作为 IT 基础设施中最基本的算力设备， 在数据中心硬件设备耗电量中所占比重最高，随着多样化业务场景在数据中心出现，服务器需承担的计算量越来越大，其能耗和碳排放也在成倍增长，同时给数据中心的维护难度和支出成本带来压力。
据中国信通院统计， 2021 年全国数据中心服务器数量 1900 万台， 耗电量达 1100 亿千瓦时， 每台服务器一年的碳排放量约 2600KG。
Energy Innovation认为，对大多数公司来说，减少数据中心能耗的更实用方法，是降低数据中心两大耗电主力(服务器和散热)的用电量，其中每一项都占美国数据中心总用电量的 43%。
为此，整机柜技术、液冷技术、高密度技术等相关技术层出不穷，其终极目的，是如何在计算密集型工作负载和低能耗运行之间找到平衡， 如何在满足技术需求的基础上尽可能降低碳排放。
解决核心的CPU能耗问题 如果我们把视线回收到服务器层面来看，作为核心部件，CPU在控制服务器整体能耗中起到了至关重要的作用。一方面服务器能耗70%来自于CPU，另外服务器CPU的能耗将极大影响其他散热、制冷等辅助设备。研究发现，服务器CPU能耗每降低1W，由此带来自身及其他相应辅助设备的总能耗将降低2.84W！
这意味着对服务器CPU的持续优化，将更有效减少服务器以及整体数据中心的能耗及相关碳排放。
我们看到，在日益强调碳排放的今天，数据中心将更加“精打细算”，会更在意CPU的能效表现，即单位算力性能下的能耗/碳排放表现，或者说尽可能降低单位工作负载下的CPU功耗。
通常情况下，降低单位工作负载功耗的最好办法，是增加CPU的核心密度。CPU的核心密度越高，性能越强，其处理的单位芯片工作负载量也会大幅增加。
Moor Insights &amp; Strategy 数据中心副总裁兼首席分析师 Matt Kimball 表示，提高数据中心能源效率的首要方法是减少服务器占用空间。实现这一目标的最有效方法是提高处理器的效率，使用更多的内核并缩小组件之间的空间。“如果能够提高服务器中 CPU 的利用率，这将对数据中心的整体功耗产生巨大影响，”他说。
AMD EPYC CPU出类拔萃的能效表现 Moor Insights &amp; Strategy 数据中心副总裁兼首席分析师 Matt Kimball特别举例强调，与其他类似x86处理器相比，数据中心运行2000个由第四代AMD EPYC 9654处理器驱动的虚拟机，服务器数量同比减少35%；而每年因此降低的能源消耗则达到29%。[1]
ODCC的研究表明，CPU 性能越好，从服务器算力碳效、即单位算力性能的碳排放量来看会越低。 在服务器使用周期为 5 年的情况下， 单位算力性能得分的碳排放量在 20-60KG 之间， 其中 AMD 服务器单位算力性能得分的碳排放量相对更低， 几乎都低于 30KG。
以 SPEC 得分 8,000 为例， 替换 21 台基于 Intel 至强 Gold 6342 服务器，可使用 16 台 Intel 至强® Platinum 8380 服务器或 11 台 AMD EPYC 7763服务器，服务器台数最多可减少 10 台，其使用周期内的碳排放量最多减少 43%， 相当于 8100 多棵树一年吸收的碳排放量（数据来自：ODCC数据中心算力碳效白皮书）。
在体现能效的SPECpower_ssj®2008测试中，AMD EPYC 9654 的表现是Intel® Xeon® Platinum 8490H的1.8倍，能效基准测试数据表明，AMD EPYC处理器拥有比其他处理器更好的能效；AMD内部数据显示，在提供2000个虚拟机的场景下，仅需11台基于AMD EPYC 9654 处理器的系统，就可替换17台基于Intel Xeon Platinum 8490H处理器系统——服务器数量节省35%、能耗降低29%、投资节省46%！[2]
当AMD EPYC 97x4 （产品代号Bergamo）推出后，这款具有128个Zen 4c内核的处理器将提供更好的能效表现，并进一步降低占地面积和能源消耗等TCO费用——AMD内部测试表明，客户只用15台基于Bergamo处理器的服务器，即可替换38台基于其他品牌顶级CPU的服务器，数量减少了61%，每年功耗降低66%，年均碳排放量接近120吨。 [3]
]]></content>
  </entry>
  
  <entry>
    <title>哪个Linux发行版可以替代Ubuntu</title>
    <url>/post/linux/which-linux-distribution-could-be-alternative-of-ubuntu.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Ubuntu</tag>
      <tag>Fedora</tag>
      <tag>Mint</tag>
      <tag>Debian</tag>
    </tags>
    <content type="html"><![CDATA[在Linux的世界里，Ubuntu是广受欢迎的一个发行版，那么究竟还有哪些Linux的发行版可以和Ubuntu媲美的呢，今天就让我们一起来看看。
Linux Mint 自带音视频解码器，开箱即用，Ubuntu还要下载编解码器才能播放
主要利用免费和开源的编程，对一些限制性的编程，如 MP3、DVD 和 Adobe Flash 进行了豁免。 Linux Mint 对独占编程的考虑很奇怪； 许多 Linux 流通自然排除了限制性编程，因为一些 Linux 拨款的共同目标是坚持自由和开源编程的模型。 它可以通过各种桌面环境进行浏览，包括默认的 Cinnamon 桌面、APT、MATE 和 KDE。
Linux Mint 伴随着大量引入的编程，其中包括 VLC 播放器、Firefox、LibreOffice 等。它允许组织端口利用其防火墙关闭，并且可以访问重做端口选择。 默认的 Linux Mint 桌面环境 CinnamonandMATE 支持多种语言。 通过利用适用于 Linux 的 Wine Windows 相似层编程或虚拟化编程（包括 VMware 或基于内核的虚拟机），它同样可以运行许多用于 Microsoft Windows 的项目。
Debian GNU / Linux 于1993年首次公布。它的创始人Ian Murdock的初始想法是在空闲时间创建一个由数百名志愿者开发的完全非商业项目。当时怀疑论者远远超过乐观主义者，似乎注定要夭折收尾，但实际情况却恰恰相反。 Debian不仅幸存下来，而且还在不到十年的时间里成为了最大的Linux发行版，也是有史以来创建的最大的协作软件项目！
Debian GNU / Linux的成功可以用下面的数字来说明。它由1000多名志愿者开发，它的软件库包含近50,000个二进制包（编译为8个处理器架构），有120个基于Debian的发行版和live CD。这些数字是任何其他基于Linux的操作系统无法比拟的。 Debian主要有三个主要分支（或四个，如果包括增加稳定性的“实验”分支）：“unstable”（也称为“sid”），“testing”和“stable ”。软件包和功能的逐步整合和稳定性，以及项目完善的质量控制机制，使得Debian获得了今天可用的最佳测试和无缺陷发行版之一的声誉。
Fedora Linux 是由 Red Hat 赞助的社区构建发行版，在被 IBM 收购之前，它是世界上最赚钱的开源公司。Red Hat 仍然是开源世界的巨头，为维护整个 Linux 生态系统所依赖的大部分软件和基础设施的开发人员付费。
Red Hat 并不直接开发 Fedora Linux，尽管该公司的一些员工是 Fedora 社区的成员。相反，Red Hat 使用 Fedora Linux 开发自己的独立产品 CentOS 和 Red Hat Enterprise Linux。这两个版本的 Linux 广泛用于企业界、学术机构或任何需要维护自己的服务器的人。
Fedora 提供易于学习的桌面，集成了大多数其他 Linux 发行版之前的最新功能，并内置了 SELinux 等安全工具。
Arch Linux 是在2002年由加拿大计算机科学专业毕业生Judd Vinet在2002年推出的，几年来，它一直是一个为中级和高级Linux用户设计的边缘项目。但是它“滚动更新”，只需要安装一次，然后保持一直更新，不要从头安装新的系统。这都要感谢其强大的包管理器和一个总是最新的软件库。因此，Arch Linux的“发行版”很少，而且现在只限于一个基本的安装光盘，只有在基本系统发生相当大的变化时，才会发行新的安装介质。
Arch Linux除了拥有备受推崇的“滚动发布”更新机制之外，还以其快速和强大的软件包管理器“Pacman”而闻名，能够从源代码安装软件包，并且由于其AUR基础架构，以及经过充分测试的软件包不断增加的软件库。其高度重视的文档，以及卓越的Arch Linux手册，使得一些高级Linux用户可以自行安装和定制分发。用户可以使用的强大工具意味着发行版可以无限定制到最细微的细节，并且没有两个安装可能是相同的。
引用于 哪个Linux发行版可以替代Ubuntu  
]]></content>
  </entry>
  
  <entry>
    <title>HPC需求高速增长推动224G以太网SerDes技术发展加速</title>
    <url>/post/datacenter/hpc-demand-growth-promotes-the-development-of-224G-Ethernet-SerDes-technology.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>HPC</tag>
      <tag>Network</tag>
      <tag>SerDes</tag>
    </tags>
    <content type="html"><![CDATA[数据，是现在网联社会的核心，数据量级的提升远超我们的想象。根据IDC的预测，到2025年全球的数据量将达到175 ZB。数据量的增长带动了对高带宽和网络速度这些新基础设施的需求。
算力网络的发展对骨干网和大型数据中心提出了更高的要求，如今200G/400G的以太网链路已经在加速部署，其超高带宽可完全满足各种带宽密集型应用的需求，并大大降低端口成本。800G和1.6T以太网链路也在加速来袭。
224G SerDes技术实现更高速率以太网 去年200G/400G 产品大规模放量，800G则进入导入阶段。以800G以太网为例，利用了两组现有400G以太网逻辑，并进行了一些修改，将数据分布在八个112Gbps物理通道上。那继续提升每个通道速率，达到224Gbps，则能够支持高达1.6T的链路。网络提速的下一个前沿趋势无疑是1.6T以太网。
人工智能、自动驾驶、高性能计算HPC和云计算这些快速增长的应用对数据网络提速的需求肉眼可见，网络速度必须足够快，才能在计算、网络和存储组件之间快速移动数据。但相对来说，算力增长的步伐是快于传输速度增长步伐的。
以太网高速接口的出现正是为了满足连接方面的需求，高性能SerDes也为每一代的标准实现了速度的翻倍，以太网速度的发展已经在尽力跟上脚步了。最新一代以太网标准将提供224G的数据速率，为1.6T以太网的发展奠定了基础。
224G以太网 SerDes技术驱动力 驱动224G以太网SerDes发展的因素，不妨以数据中心的角度来看，因为不论是在数据中心内部、数据中心之间还是数据中心与终端之间都有着大量的数据交换需求。
其中最主要的带宽需求在数据中心内部，向更高速度以太网连接的转变不仅可以节省电力，还可以节省面积，从而增加互连密度。
空间的节省也是很明显的，从早前的640G交换机到现在51.2T甚至102.4T交换机，每一代的端口数都在不断变化，现在已经有多达512个端口，如果不向更高速率的以太网SerDes发展，端口数量继续增多的交换芯片越来越难做，而且良率很低。
SerDes接口必须不断提高运转速度来顺应发展。在高密度的数据中心，224G以太网SerDes的应用将大大减少所需的线缆和交换机数量，既节省宝贵的空间又增加互连密度。
当然，224G真正部署起来却并没有那么简单。半导体封装技术、链路连接技术、通道技术都还在努力地跟上224G以太网SerDes发展的节奏，每一项技术欠缺都会增加链路的损耗，增加串扰的风险，根据相关厂商的评估，想要112G增加到224G，实现难度总体上升了5倍。
Light Counting调研认为，今年224G以太网SerDes会有3到5个Design开始，到2026年，224G将会迎来第一波部署热潮。IP Nest在对SerDes IP的调研中，同样认为224G以太网SerDes PHY将在今年开始加快发展速度。
目前，已经有一些厂商能够提供针对HPC和数据中心应用支持更高速度的MAC、PCS和PHY，同时适用于多种先进FinFET工艺，并在性能最大化的情况下提供卓越的BER。
224G以太网SerDes早期的一些应用主要会覆盖重定时器、交换机、拓展AI、光学模块、I/O芯片和FPGA上，成熟应用后，将延伸至更多数据需求领域，在各行各业充分出更高数据速率的优势。
小结 现代数据中心对更高数据速率的需求，对网络扁平化减少延迟的需求，大大推动了对更高带宽连接的需求。交换器SoC芯片尺寸正在达到最大限制也意味着需要更高的连接速率来支持更高的带宽要求。
这些切实需要解决的问题大大推动了对224G以太网SerDes连接的需求。随着224G以太网 SerDes技术在今后的成熟，高密度数据应用将会有完全不同的解决方案。
]]></content>
  </entry>
  
  <entry>
    <title>Intel 4工艺太难了！酷睿Ultra终于突破5GHz</title>
    <url>/post/news/intel-core-ultra-finally-breaks-through-5G-HZ.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>Core</tag>
      <tag>Ultra</tag>
    </tags>
    <content type="html"><![CDATA[无论是14nm还是10nm，Intel这些年的新工艺都有一个通性：刚诞生的时候性能平平，高频率都上不去，只能用于笔记本移动端(分别对应5代酷睿、10代酷睿)，后期才不断成熟，比如到了13代酷睿就达到史无前例的6GHz。
接下来的Intel 4，也就是最初的7nm，也要重复这条路了。
代号Meteor Lake的酷睿Ultra将首发Intel 4工艺，第一代也是仅限笔记本移动端，而且前期样品只能达到4.8GHz的单核频率。
目前，酷睿Ultra已经进入冲刺阶段，也就是质量验证(QS)，和零售正式版相差无几。
据了解，酷睿Ultra QS样品运行在20-65W的热设计功耗范围内，最高配置6个P性能核、8个E能效核、2个LPE超低功耗能效核，组成16核心22线程。
根据快科技8月6日了解到的消息，酷睿Ultra 7系列现在已经可以做到单核5GHz，酷睿Ultra 9系列则超过了5GHz，后续虽然不可能再有天大突破，但仍有望继续提升一些。
作为对比，现有13代酷睿移动版的i9-13900H已经可以跑到5.4GHz，i7-1370P也能加速到5.2GHz。
酷睿Ultra系列要超越它们，频率难以企及，只能依靠新的架构和更高的IPC，难度不小。
核显方面，之前消息说样品频率已高达2.2GHz，比以往大幅提升，但暂无进一步消息。
酷睿Ultra的核显将会非常抢眼，升级到全新的Xe LPG架构，也就是Arc锐炫系列独立显卡Xe HPG高性能架构的低功耗版本，集成最多8个Xe核心，也就是128个EU执行单元/1024个流处理器，比增加1/3。
值得一提的是，酷睿Ultra系列会加大对AI技术的支持，有望增加一个VPU视觉计算单元，将AI芯片成为处理器的功能基础，性能据说在3T到7.1TFLOPS之间，是上代的10倍，能效很高。
Intel CEO基辛格就明确表态，AI将会融入到Intel构建的每一款产品中。
在此之前，Intel去年底推出的第四代至强可扩展处理器上就已经加入了AI单元，性能10倍提升，现在也是时候轮到消费级处理器升级AI单元了。
酷睿Ultra加强AI也跟微软的战略不谋而合，9月份的Win11 23H2中微软就会加入AI助手Copilot，而明年的Win12系统中更是把AI作为核心技术植入，要重塑所有体验。
此前泄露的消息显示，Intel的酷睿Ultra第一代产品Meteor Lake就会首发支持Win12系统，还有Wi-Fi 7新技术，亮点不少。
]]></content>
  </entry>
  
  <entry>
    <title>多核同构SMP--调度算法分析</title>
    <url>/post/linux/smp-scheduler-algorithm.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>SMP</tag>
      <tag>Scheduler Algorithm</tag>
    </tags>
    <content type="html"><![CDATA[随着智能化产品的需求不断提高，慢慢的单芯片单核处理器已经不能满足我们的需求，于是就在一个芯片上集成两个或多个核心，进而转向了多核处理器的发展，多核处理器具有更高的计算密度和更强的并行处理能力，所以它也是大趋势。多核处理器从硬件的角度来区分，又分为同构和异构：
 多核同构处理器：一个处理器的多个核心的体系架构是一样的，如：T113 多核异构处理器：一个处理器中包含不同体系架构的核心，如：STM32MP157  多核处理器从软件的角度来区分，又分为SMP和AMP：
 SMP：又称对称多处理（Symmetric multiprocessing），只有一个操作系统（OS）实例上运行多个核心，一个OS同等的管理各个内核，为各个内核分配工作负载，系统中所有的内核平等地访问内存资源和外设资源。 AMP：又称非对称多处理（Asymmetric Multi-Processing），每个核心运行自己的OS或同一OS的独立实例，或者说不运行OS，如运行裸机，每个内核有自己独立的内存空间，也可以和其它内核共享部分内存空间，每个核心相对独立地运行不同的任务，但是有一个核心为主要核心，它负责控制其它核心以及整个系统的运 行，而其它核心负责“配合”主核心来完成特定的任务。  本篇文章围绕SMP展开讲解。
什么是SMP 对称多处理器结构 , 英文名称为 &ldquo;Symmetrical Multi-Processing&rdquo; , 简称SMP。SMP又称为UMA, 全称&quot;Uniform Memory Access&quot;, 中文名称&quot;统一内存访问架构&quot;。
在 &quot; 对称多处理器结构 &quot; 的 系统中 , 所有的处理器单元的地位都是平等的 , 一般指的是服务器设备上 , 运行的 多个 CPU , 没有 主次/从属 关系，都是平等的。
这些处理器共享所有的设备资源, 所有的资源对处理器单元具有相同的可访问性, 如: 内存, 总线等，多个CPU处理器共享相同的物理内存, 每个CPU访问相同的物理地址, 所消耗的时间是相同的;
SMP的优缺点 优点：避免了结构障碍, 其最大的特点是所有的资源共享。
缺点：SMP架构的系统, 扩展能力有限, 有瓶颈限制。如: 内存瓶颈限制, 每个CPU处理器必须通过相同的总线访问相同的内存资源, 如果CPU数量不断增加, 使用同一条总线, 就会导致内存访问冲突; 这样就降低了CPU的性能;
操作系统如何满足SMP  公平共享: CPU的负载, 需要公平地共享, 不能出现某个CPU空闲, 造成资源浪费。 可设置线程(进程)与CPU亲和性: 可以为某些类型的线程（进程）与指定的处理器设置亲和性, 可以针对性地匹配线程（进程）与处理器。 线程（进程）迁移: 可以将线程（进程）在不同的CPU处理器之间进行迁移 。  总结：操作系统的SMP对称多处理器结构调度，核心就是将线程（进程）迁移到合适的处理器上, 并且可以保持各个处理器的负载均衡。
SMP调度方式 作者总结SMP的调度算法可以分为三种：
线程（进程）默认核心0运行，可以指定亲和性：  当用户创建线程（进程）时，可以指定挂在到指定核心运行。当任务挂在到指定核心，那么该任务只能在该核心上运行。 当用户创建线程（进程）时，没有指定挂在到指定核心运行，线程（进程）默认挂在到核心0。该任务核心0上运行。  问题：  优点：可以规定某个核心专注的做某一件事或某一类事。 缺点：核心0的负载会很大，它需要调度其他核心不调度的任务。  适用场景：  项目需要指定核心专一处理某一件事情的时候，可以使用这种调度算法  线程（进程）默认均分到不同核心，可以指定亲和性。  当用户创建线程（进程）时，可以指定挂在到指定核心运行。当任务挂在到指定核心，那么该任务只能在该核心上运行。 当用户创建线程（进程）时，没有指定挂在到指定核心运行，系统会判断每个核心的任务数，将该任务放在任务数最少的核心中。  问题：  优点：将任务平分给每个核心，每个核心的负载会相对均衡。 缺点：可能存在某个核心分配的任务都是比较轻的，某个核心分配的任务比较重。导致核心中的任务比较轻的，会更加容易进入空闲状态，核心中的任务比较重的，可能会一直处于忙碌状态，这样也会导致每个核心的负载不均衡。  适用场景：  项目中，所有的任务的复杂程度都差不多，可以均分到每个核心上，这样可以提高系统性能。  线程（进程）根据核心负载获取任务调度，可以指定亲和性。  当用户创建线程（进程）时，可以指定挂在到指定核心运行。当任务挂在到指定核心，那么该任务只能在该核心上运行。 当用户创建线程（进程）时，没有指定挂在到指定核心运行，将该任务挂载一个总任务队列中，当某个核心调度空闲时，就从总任务队列中获取一个任务运行。运行完毕之后归还给总任务队列。  问题：  优点：根据每个核心的负载，均分整个系统的任务调度，提供了每个核心的利用率。 缺点：调度算法比较复杂  适用场景：  项目中不需要关心任务的具体运行到那个核心。  总结  上述的调度算法，只有第三种满足：①公平共享；②可设置线程(进程)与CPU亲和性；③线程（进程）迁移。 调度算法，第一种和第二种，只满足三个条件的某一部分。 不用的调度适用于不同的场景，需要根据实际的需求选择相应的调度算法。 ]]></content>
  </entry>
  
  <entry>
    <title>Napatech如何将服务器转变为网络安全设备</title>
    <url>/post/datacenter/napatech-turned-servers-into-cybersecurity-appliances.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Intel</tag>
      <tag>Smart NIC</tag>
    </tags>
    <content type="html"><![CDATA[目前，几乎所有企业、政府机构、非营利实体及其它各类型组织都面临着网络安全挑战，随着网络罪犯不断提高作案能力，这类挑战将日益加剧。
网络安全现状 病毒、木马、蠕虫和其它恶意软件数量的增加，分布式拒绝服务 (DDoS) 攻击的蔓延，物联网设备与互联网的融合，以及数据中心线路速度的不断增长（从 1 Gbps 增长至 10 Gbps、25 Gbps 乃至 40 Gbps），各种趋势进一步加剧了网络安全挑战。
开源网络安全软件的缺点 在汹涌的数据海啸中，我们需要使用专业的软硬件确保网络安全性。网络安全呈现双轨并进的发展态势，其中既包括根据网络安全需求开发的自定义软硬件（较为昂贵），也包括开源网络安全软件，这些开源安全软件包括：
 Zeek（前身为 Bro）：Zeek 入侵检测系统 (IDS) 框架是一款非常强大的网络监控工具，可捕捉有关网络连接的数百个元数据字段。这些元数据可为网络流量提供无与伦比的可视性，帮助我们识别异常行为，如可疑或威胁活动。 Suricata：Suricata 是一款成熟的开源网络威胁检测引擎，具备实时网络入侵检测、内联入侵防御 (IPS)、网络安全监控 (NSM) 和捕捉数据包离线处理等功能。 Snort：Snort 是一款开源网络 IPS 工具，可在 IP 网络上执行实时流量分析和数据包记录。该工具可实施协议分析、内容搜索/匹配，并可用于检测各种攻击与探头，包括缓冲区溢出、秘密端口扫描、CGI 攻击、服务器信息块 (SMB) 探头及操作系统指纹识别尝试等。 ntop n2disk 和 nProbe Cento：ntop n2disk 和 ntop nProbe Cento 分别是网络流量记录器和高速流量分析探头，支持 1/10/100 Gbps 以太网连接。  相比定制化的网络安全系统，开源网络安全软件具有更低的成本，但基于 CPU 的服务器无力应对流量增长。使用开源网络安全软件检测实时数据流时，每台基于 CPU 的服务器最高可达到约 15 Gbps 的速率。但是在数据中心，网络安全系统所产生的负载往往会远高于这个数值。
这时候用户往往使用多台基于 CPU 的网络安全服务器处理较大负载，他们通常利用负载均衡器将入站流量分为大小适当的数据流，并将这些数据流分配至各个网络安全服务器。分别通过开源网络安全软件进行处理。在这种方式中，由于服务器节点增加、采购负载均衡器等原因，会推高网络安全系统的整体成本。
英特尔® 可编程加速卡加速网络应用 借助一款基于 FPGA 的加速器卡，即采用英特尔® Arria® 10 GX FPGA 的英特尔® 可编程加速卡 (PAC)，Napatech 获得了一种中间方案，从而提升了开源网络安全应用的性能，实现了加速。
此外，采用英特尔® Arria® 10 GX FPGA 的英特尔® PAC 基于 FPGA 的通用型加速技术还可帮助 Napatech 加速其他网络应用，包括：
 TRex：TRex 是一款开源状态和无状态流量生成器，基于数据平面开发套件 (DPDK)。通过对真实流量模板的预处理和使用，TRex 可实现智能重播，生成第 4 层到第 7 层流量。 Wireshark：Wireshark 是一款广泛使用的网络协议分析器，可提供网络活动的微观视图。Wireshark 是实际上（通常也是法定的）协议分析标准，被众多企业、非营利机构、政府机构和教育机构广泛使用。  英特尔® PAC 中的英特尔® Arria® 10 FPGA 可加速关键网络安全与其它网络应用，支持具备适当配置的服务器全速处理 40-Gbps 流量，且不会遗漏任何数据包。最新的加速统计数据如下：
 Suricata – 加速 4 倍 n2disk – 加速 3 倍 TRex – 加速 4 倍 Wireshark – 加速 7 倍  采用英特尔® Arria® 10 GX FPGA 的英特尔® 可编程加速卡 (PAC)
面向采用英特尔® Arria® 10 GX FPGA 的英特尔® 可编程加速卡的 Napatech Link™ Capture Software 将英特尔® 加速器卡转变为 SmartNIC，对上述开源应用实施多种加速网络安全和其他网络功能，数据中心运营商可根据特定要求选择所需的网络安全应用。
]]></content>
  </entry>
  
  <entry>
    <title>如何做个PCB电动机</title>
    <url>/post/hardware/how-to-make-a-pcb-motor.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCB</tag>
      <tag>Motor</tag>
    </tags>
    <content type="html"><![CDATA[电动机印制电路板的每一层都有一组线圈，它们堆叠在一起并互相连接以形成连续的迹线。
我一开始只是想做一架非常小的无人机。但我很快意识到，在我的设计中，有一个制约因素，那就是马达的体积和重量。即使是小电动机也仍然是分立的装置，需要连接到所有其他电子元件和结构元件上去。所以我开始想知道是否有办法合并这些元件并减轻一些质量。
我的灵感来自于一些无线电系统如何使用由印制电路板（PCB）上的铜迹线制成的天线。我可以使用类似的东西来制造足够强大的磁场来驱动电动机吗？我决定看看是否可以使用由PCB迹线制成的电磁线圈来制造轴向磁通电动机。在轴向磁通电动机中，形成电动机定子的电磁线圈平行于圆盘形转子安装。永磁体嵌入转子的圆盘中。用交流电驱动定子线圈使转子旋转。
第一个挑战是确保我能够创造出足够的磁通量来转动转子。设计一个扁平的螺旋线圈迹线并让电流流过它是很简单的，但是我将我的电动机的直径限制在16毫米，以使整个电动机的直径可与最小的成品无刷电动机的相媲美。16毫米意味着我只能在转子圆盘的下面总共安装6个线圈，每个螺旋上安装大约10匝。十匝不足以产生足够大的磁场，但是如今很容易制作出多层的PCB。通过打印成堆叠的线圈（四层的每一层上都有线圈），我可以让每一线圈获得40匝，足以转动一个转子。
随着设计的向前推进，一个更大的问题出现了。为了保持电动机的旋转，必须使转子和定子之间动态变化的磁场同步。在由交流电驱动的典型电动机中，由于桥接起定子和转子的电刷的排列，这种同步自然就产生了。在无刷电动机中，需要的是实现反馈系统的控制电路。
左图：完成的四层印制电路板。
中图：对这些线圈施加脉冲，驱动3D打印出来的带有嵌入式永磁体的转子。
右图：虽然没有传统的无刷电动机那么强大，但PCB更便宜、更轻。
在我以前制造的一个无刷电动机驱动器中，我测量了作为反馈来控制速度的反电动势。反电动势产生的原因是旋转的电动机就像一个小发电机，在定子线圈中产生与用于驱动电动机的电压相反的电压。对反电动势进行感应，可以提供有关转子旋转方式的反馈信息，并让控制电路使线圈同步。但在我的PCB电动机中，反电动势太弱而无法使用。为此，我安装了霍尔效应传感器，它可以直接测量磁场的变化以测量转子及其永磁体在传感器上方旋转的速度。随后这些信息被输入到电动机控制电路中。
为了制造转子本身，我转向了3D打印。起初，我制作了一个转子，我安装在一个单独的金属轴上，但后来我开始将卡扣轴作为转子的一个组成部分进行打印。这将物理组件简化为了只有转子、四个永磁体、一个轴承以及提供线圈和结构支撑的PCB。
我很快就得到了我的第一台电动机。测试表明它能产生0.9克厘米的静态扭矩。这不足以满足我最初的制造一个集成进无人机的电动机的目标，但我意识到这个电动机仍然可以用来推动小型廉价的机器人轮子上用轮子沿着地面前进，所以我坚持进行研究（电动机通常是机器人身上最昂贵的部件之一）。这一印制电动机可以在3.5到7伏的电压下工作，尽管它在较高的电压下会明显升温。在5 V时，其工作温度为70°C，这仍然是可控的。它吸收大约250毫安电流。
目前，我一直在努力增加电动机的扭矩。通过在定子线圈的背面添加铁氧体片来包含线圈的磁场线，我几乎可以使扭矩倍增。我还在研究设计具有不同绕组配置和更多定子线圈的其他原型。此外，我一直在努力使用相同的技术来构建一个PCB电动推杆，它可以驱动一个3d打印出来的滑块在一排12个线圈上滑动。而且，我正在测试一个柔性PCB原型，它使用相同的印制线圈来执行电磁驱动。我的目标是——即使我还不能制造出能飞上天空的小无人机——开始制造具有比现有机器人更小更简单的机械构造的机器人。
]]></content>
  </entry>
  
  <entry>
    <title>ARM服务器在信创市场能突围吗</title>
    <url>/post/datacenter/arm-server-in-information-technology-industry.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>ARM</tag>
      <tag>Server</tag>
    </tags>
    <content type="html"><![CDATA[近几年随着中美脱钩加速，在信息技术领域频频遇到美国卡脖子情况。为保障技术自主可控，信创产业必要性愈发凸显。信创作为一项国家战略，发展信创是为了解决本质安全的问题。本质安全也就是说，把它变成中国自己可掌控、可研究、可发展、可生产的。
而信创包含IT基础设施，基础软件、应用软件、信息安全等领域，其中CPU芯片作为服务器、PC整机等IT基础设施底座支撑，CPU设计技术的自主可控格外重要。当前国内六家主流CPU厂商鲲鹏、飞腾、兆芯、龙芯、海光、申威纷纷发力，从国外引进各种不同技术架构如 ARM  、 X86  、MIPS等，共同推进信国产CPU发展。
引进各种不同的架构路线的初衷，是希望充分发挥试错作用，借市场和用户的选择，找到国产CPU最正确的技术发展路线。而在几年的技术发展后，或是龙芯LoongArch，或是国产X86，技术积累和产品迭代都做得不错；只是发现ARM架构，貌似无法扛起自主可控的重任。
ARM服务器难以成为主流 ARM作为精简指令集，其优势在功耗比，主要应用在移动和嵌入式市场。早在2008年，ARM公司在研究ARM架构是否可以用于服务器，为数据中心提供算力支持，AMD、惠普、博通、高通等美国企业都曾发布过ARM架构的服务器芯片，但全都折戟。
其主要原因在于ARM芯片依靠多核心堆砌，单核性能孱弱，总体比不上x86架构芯片性能。后来在2018年，高通推出ARM的高效服务器芯片Centriq 2400，性能比英特尔 Purley铂金8160高出7%，但高通的ARM CPU还是没有厂商愿意使用，最后无疾而终。
业界终于认识到，ARM架构生态不健全才是最大的问题。那为什么鲲鹏和飞腾还要引进ARM架构？原因在于ARM授权模式。ARM公司不涉及芯片生产，只对外销售IP核授权，国内厂商拿到授权的公版架构后，开发设计门槛低，容易造出量产芯片。
目前国产Arm架构CPU服务器大体分为华为的鲲鹏系和中电子的飞腾系。鲲鹏系主要是以华为自家TaiShan系列服务器为核心，只向服务器合作厂商（如神州数码、宝德等）出售鲲鹏主板而非鲲鹏芯片。而飞腾系聚焦于国产芯片研发，自身并不设计生产服务器，直接向服务器合作厂商（如长城、浪潮、联想、宝德、同方、五舟等）出售服务器芯片。
当前国内ARM服务器受制于ARM芯片性能、生态原因发展较为有限。鲲鹏系服务器，因为只供货搭载鲲鹏CPU的主板，导致神州、宝德等服务器产品性能同质化严重，另外华为还有自家TaiShan系列，导致鲲鹏下游厂商出货情况基本受华为控制。
而飞腾系服务器，据众多金融行业IT从业者反映，在资源使用率达到 70%-80% 左右就出现问题，可能跟相关产品的适配不兼容有关联。因为服务器领域应用最广泛的还是X86架构，生最为健全，很多情况下ARM服务器需要重新编译适配，才能使用X86软件应用。
虽然近两年在鲲鹏和飞腾的大力推动下，国内ARM生态发展迅速，但目前国内市占仅有7%，长远看也难以成为主流选择。
桌面应用生态有限 相对于ARM芯片在服务器领域还能做到小部分应用，国产ARM芯片在PC电脑的应用更为惨淡，目前主要为飞腾PKS体系内，长城有一些ARM芯片电脑。主要原因还是性能与生态孱弱，低性价比导致在商业市场竞争力较弱。
PC电脑文字处理、图片编辑等日常办公用途，CPU单核性能更为重要。以长城（Great Wall）TN140A2笔记本电脑为例，搭载8核的飞腾D-2000芯片，最高主频仅有2.3GHz，单核性能不足Intel同期产品的1/4，8个核心性能仅相当于Intel CPU的两个核心。而21年的龙芯3A5000的主频为2.5GHz，单核性能测试较飞腾D-2000也高出不少。
除了性能较差很难支持桌面办公的需要，ARM电脑的生态也是个大问题。主流的Windows生态都是基于X86开发，此前因为Arm在PC领域的表现不佳，让很多应用开发者的积极性不够高。原生应用数量严重不足，这些都会影响到用户的基础体验。如果靠编译器移植到ARM电脑，则会损失大量性能，让本就不充足的CPU性能雪上加霜。
在性能和生态孱弱的情况下，国产ARM PC售价却不低，基本价格都在9000元以上，售价比肩苹果MacBook，且售后质保仅有一年。
综上原因，ARM PC所占份额很小，仅在某些特定领域有些应用。
迁移成本高昂，后续维护困难 从用户角度出发，对于国产CPU及终端产品，生态覆盖面广、处理性能好，并结合自身应用系统对国产芯片的适配情况，迁移改造工作量小的产品才是最佳选择。
但无论是ARM服务器还是电脑，使用成本不低。
一方面是ARM服务器，因跟最主流的x86生态相互不兼容，如果用户需要从x86平台切换到ARM平台，迁移工作量大，改造时间长。并且因为各行各业部署环境不相同，若遇到适配错误的情况，也很难从服务器厂商获得帮助。整个过程费心费力不说，迁移工作还会导致业务停摆，其中造成的损失难以避免。另一方面，办公使用ARM电脑，其操作逻辑与主流电脑并不相同，用户需要花费很多精力从新学习使用，况且因生态不健全，很多软件也用不了。
就这些用户痛点，ARM厂商会在前期承诺免费开发移植调优，附加条件为全套系统全部采购ARM架构产品。这对用户意味着，这套强封闭和强捆绑手段，后期若是出现问题，将面临着高昂的维修维护费用。
用户与其重新适应ARM架构，不如从一开始选择自主程度更高、性能更好的龙芯产品。
技术迭代之路已被掐断 除了ARM产品性能、生态的本身问题外，ARM架构不适合信创产业的最根本原因在于，国外已经禁止出口ARM架构后续架构，国产ARM芯片停止迭代。ARM公司多次制裁中国企业，已经证明ARM芯片自主可控是伪命题。
并且ARM公司今年开始赴美IPO，获得美国资本之后，对国内制裁将更顺理成章，后续架构授权必然遥遥无期。在此情况下，国产ARM芯片性能逐渐落后，生态脱钩加速，对于用户业务的影响将一步步凸显出来。
因此对于国内信创产业， ARM架构在延续性和自主性上都有亟待解决的问题，很难担负自主可控的重任。
在国外制裁之下，国产ARM芯片未来何去何从？已投入的资源如何止损？这些问题都需要ARM厂商们再做考量。
]]></content>
  </entry>
  
  <entry>
    <title>无惧SSD压力 8月起机械硬盘要涨价</title>
    <url>/post/datacenter/prices-of-hard-drive-will-raise-in-auguest.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Storage</tag>
      <tag>SSD</tag>
      <tag>HDD</tag>
      <tag>Solidigm</tag>
      <tag>P5336</tag>
    </tags>
    <content type="html"><![CDATA[在装机硬盘选择上，现在大家几乎默认选择SSD硬盘，系统盘没人选HDD机械硬盘了，然而对市场来说，HDD硬盘厂商似乎并不担心被SSD侵蚀份额，8月份还会上调价格。
来自硬盘渠道商的消息显示，8月份起硬盘厂商计划提价，最近零售版的行货硬盘价格已经上涨了。
目前的低价机械硬盘主要是来自OEM厂商之前的库存，所以市面上还是能买到一些低价仓库盘。
目前机械硬盘市场上主要是西数、希捷及东芝三大厂商，前两年占据主要份额，具体涨价是哪家开始的不得而知，但双方应该有默契，在总销量一直下滑的情况下，涨价是改善业绩的主要手段了。
考虑到消费级市场上，SSD硬盘已经成为必选，因此这次的涨价对大部分个人用户来说影响不大，主要影响企业级客户，毕竟大容量存储上，机械硬盘还是不可少的。
前几天Solidigm推出了61.44TB的SSD硬盘，这个容量远超当前的机械硬盘，后者最大容量还在26TB级别，30TB的尚未正式出货。
不过超大容量的SSD价格也很贵，单位价格下依然是机械硬盘占优，因此企业级也不可能完全淘汰机械硬盘，至少8TB到20TB以上的市场中，机械硬盘还会长期存在。
存储行业确实在底部了，一些公司业绩已经在证实。
全球第二大存储芯片制造商韩国SK海力士给出的报告称，第二季度净亏损2.98万亿韩元（约合23亿美元），上年同期为净利润2.88万亿韩元。
该公司表示，第二季度营业亏损2.88万亿韩元，而上年同期为盈利4.19万亿韩元。营收下降47.1%，至7.3万亿韩元。
这是SK海力士连续第三个季度出现亏损，原因是需求疲软导致整体存储芯片价格持续低迷。
但第二季度亏损比第一季度3.4万亿韩元的创纪录亏损有所收窄。该公司表示，内存芯片市场正开始从深度低迷中复苏。
该公司在一份声明中表示，尽管消费者需求持续疲软，但“生成式人工智能市场的扩张”迅速推高了“对人工智能服务器内存的需求”。
“因此，高端产品(如HBM3和DDR5)的销售增加，导致第二季度营收环比增长44%，而营业亏损环比缩小了15%。”
行业人士表示，随着存储行业需求的增加，一些存储产品也即将迎来大涨价时代，这会缓解存储厂商的业绩。
]]></content>
  </entry>
  
  <entry>
    <title>Intel全线涨价！12/13/14代、酷睿Ultra无一例外</title>
    <url>/post/datacenter/intel-raises-prices-across-the-board.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Core</tag>
      <tag>Alder Lake - Raptor Lake</tag>
    </tags>
    <content type="html"><![CDATA[据报道，Intel即将对旗下处理器全线涨价，现有的和即将发布的，无一例外。
权威德国硬件媒体PCGH从两个不同的德国供应商处获悉，他们都收到了Intel的通知信，包括在库、打折的处理器，都会涨价，这让他们感到非常意外。
已经在售的Alder Lake 12代酷睿、Raptor Lake 13代酷睿，即将推出的Raptor Lake Refresh 14代处理器、Meteor Lake一代酷睿Ultra处理器，都在涨价范围之内。
不过，Intel并没有明确涨价的市场区域。
德国供应商和德国媒体都怀疑，这次涨价有可能是区域性的，甚至仅限德国，原因就是Intel在德国建厂需要钱，需要大量的钱。
根据Intel IDM 2.0战略，他们将投资上千亿美元在美国、欧洲兴建多座晶圆厂，包括德国马格德堡，最初计划投资170亿欧元，后来一路飙升到300亿欧元。
Intel要求德国政府补贴100亿欧元，自己投资200亿欧元。
就在近日，德国政府基本确定拨款200亿欧元支持国内的半导体制造业，其中Intel就拿走一半。
如果Intel真的是因为在德国投资建厂就全线涨价，德国消费者就真的有点欲哭无泪了。
涨价就算了，关键是新品进步也不大。
Raptor Lake Refresh 14代酷睿只会是13代的升级版，不同型号增加核心、提升频率、扩大缓存，大概率还会拉高内存频率支持。
不过，最初的曝料看起来很美好，后来却被发现并非如此，i5系列让人失望之后i3系列也翻车了。
早先说法称，14代酷睿i3系列包括i3-14300、i3-143100/F，从4核心升级为6核心，而且是全系列唯一仅有大核心而没有小核心的。
但是根据最新消息，14代酷睿i3系列依然是4核心，而且目前只能确认有i3-14100/F，三级缓存12MB，主频最高4.7GHz，相比13代、12代分别提高200MHz、600MHz。
i3-14300是否还有暂时无法完全确认，13代就缺失了这个档位，12代则是4.4GHz频率，这一代如果有的话怎么也得做到4.9GHz。
对于14代酷睿i5系列，最初的说法是i5-14600系列都升级为8+8 16核心、i5-14500/14400系列都升级为6+8 14核心。
但后来也变了，现在也得到确认：i5-14600/14500系列停留在6+8 14核心，三级缓存都是24MB，最高频率5.2GHz、5.0GHz。
i5-14400系列停留在6+4 10核心，三级缓存20MB，最高频率4.7GHz。
再往上就好说了，酷睿i7系列8+8核心升级为8+12核心、三级缓存33MB，其中i7-14700/F最高频率5.4GHz。
酷睿i9系列自然就是8+16核心，i9-14900K/KF可以加速到6.0GHz，i9-14900/F可以加速到5.8GHz。
传说还有i9-14900KS，频率达到史无前例的6.2GHz，但暂无进一步消息，可能要看能不能挑到足够多超好体质的芯片了。
]]></content>
  </entry>
  
  <entry>
    <title>如何同时运行多个 Linux 命令</title>
    <url>/post/linux/how-to-run-multiple-linux-commands-simultaneously.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Multiple Commands</tag>
    </tags>
    <content type="html"><![CDATA[了解如何在 Linux 中同时执行多个命令可以显著提高您的效率和生产力。本文将指导您通过各种方式在单行中运行多个 Linux 命令，甚至如何自动化重复的任务。
理解基础知识 深入了解高级技巧之前，您应该熟悉命令行或终端，这是 Linux 的强大工具。在这里，您可以通过输入一系列命令来执行任务。虽然一开始可能会觉得令人生畏，但学会使用它可以打开一个提高效率和生产力的新世界。
连续运行命令 如果您想连续运行多个命令，即在前一个命令完成后运行下一个命令，请使用分号（;）。例如，command1 ; command2 ; command3 将执行 command1，等待它完成，然后执行 command2，以此类推。
并行执行命令 要同时运行或并行运行命令，请使用和号（&amp;）。但请记住，使用和号会将进程发送到后台，允许下一个命令立即启动。例如，command1 &amp; command2 同时执行 command1 和 command2。
使用逻辑运算符 您还可以使用逻辑运算符（&amp;&amp; 和 ||）根据前一个命令的成功或失败来运行命令。’&amp;&amp;’ 运算符将在前一个命令成功时执行下一个命令。例如，command1 &amp;&amp; command2 仅在 command1 成功时才执行 command2。相反，’||’ 运算符仅在前一个命令失败时才执行下一个命令。
分组命令 如果您有一组要按特定顺序执行的命令，可以使用括号。例如，(command1 ; command2) &amp; command3 将同时运行 command1 和 command2，但仅在两个命令都完成后才启动 command3。
利用命令行管道 管道是一种非常有用的工具，可以将一个命令的输出传递为另一个命令的输入。您可以使用垂直线（|）来实现这一点。例如，command1 | command2 将 command1 的输出作为 command2 的输入传递。
自动化重复任务 如果您经常执行特定的一组命令，可以编写一个简单的 bash 脚本来自动化该过程。您只需要在文本文件中编写命令并将其保存为 .sh 扩展名即可。例如，您可以创建一个名为 ‘myscript.sh’ 的文件并编写：
#!/bin/bash command1 command2 command3 然后，运行 chmod +x myscript.sh 使脚本可执行，并使用 ./myscript.sh 执行它。
总结 掌握同时执行多个 Linux 命令的艺术可以节省大量时间，极大地增强您的工作流程。通过理解分号、和号、逻辑运算符、括号、管道和 bash 脚本，您将能够以更高效、更强大的方式让终端为您工作。
原文地址： 如何同时运行多个 Linux 命令  
]]></content>
  </entry>
  
  <entry>
    <title>关于超级以太网，大佬有话要说</title>
    <url>/post/datacenter/the-boss-has-something-to-say-about-super-ethernet.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Ethernet</tag>
    </tags>
    <content type="html"><![CDATA[关于超级以太网，大佬有话要说
来自行业分析师引述 &ldquo;由于系统互连功能的弱点，许多HPC和AI用户发现很难从他们的系统中获得全部性能。用户也很难集成和学习多个新的或不同的解决方案。看到这群令人印象深刻的领先公司的共同努力，创建一个新的通用的更高性能互连解决方案，这令人兴奋。HPC和AI领域的买家具有非常苛刻的工作负载，Ultra Ethernet Consortium（UEC）可以极大地帮助提高互操作性、性能和功能。我们期待在不久的将来看到一套新产品进入市场。&quot;——Dr. Earl Joseph, CEO of Hyperion Research.
“AI/ML和HPC的业务用例正在继续扩大，越来越多的公司希望利用可扩展的计算来获得竞争优势，无论是在自己的计算设施中还是在云中。如今，没有标准的、供应商中立的数据中心网络解决方案专注于并行应用程序的大规模性能。由于大多数数据中心都是基于以太网的，因此拥有由UEC驱动的可扩展解决方案将使可扩展性更加直接和可访问。参与UEC的公司能够开发一致的以太网解决方案，从单一连接到最大的超级计算机和超大规模数据中心。”——Addison Snell, CEO of Intersect360 Research.
“我敢说，关于用于基础设施的最佳网络的讨论一直在进行，以支持生成式AI的大型语言模型的训练和推理。一些公司已经转向基于以太网的网络，更喜欢其易于安装和使用。UEC计划将成为AI社区的一个受欢迎的补充。”——Karl Freund, Founder and Principal Analyst at Cambrian-AI Research.
UEC创始成员语录 “高度计算密集型工作负载（例如 AI 培训、机器学习以及 HPC 模拟和建模）需要可扩展且经济高效的行业解决方案，并将互操作性作为重中之重。为了创建基于以太网的开放式架构，以满足现代数据中心工作负载不断变化的需求，我们将作为创始成员加入超级以太网联盟。AMD 在支持开放行业标准方面有着悠久的历史，我们很自豪今天能继续与 UEC 一起走这条路。&quot;——Robert Hormuth, corporate vice president, Architecture and Strategy, Data Center Solutions Group, AMD.
“Arista Networks 很高兴参与 UEC，支持以太网向更多用例的演进，作为 HPC 和 AI/ML 工作负载的无处不在的传输。”——Hugh Holbrook, Group Vice President, Software Engineering for Arista Networks.
“凭借其无与伦比的生态系统、极高的灵活性和高性能，以太网已成为几乎所有类型数据网络的首选结构。Broadcom长期以来一直是以太网技术的支持者，推动网络堆栈各个方面的创新。我们很高兴能与许多云和网络行业巨头合作，推动以太网满足下一代人工智能和HPC网络的需求。“——Ram Velaga, senior vice president and general manager, Core Switching Group, Broadcom.
“我们正处于几乎每个行业大规模转型的开始。AI / ML将从根本上改变我们做所有事情的内容，时间和方式。为了实现这种转变，行业需要改进未来网络的构建方式。思科支持UEC的目标，即识别和标准化优化，这将使部署AI/ML基础设施的客户受益。”——Rakesh Chopra, Cisco Fellow, Common Hardware Group, Cisco.
“HPC 市场一直是开发高速互连的关键驱动力。随着 AI/ML/DL 密集型和大规模工作负载的出现，市场正在向创建包含互操作性、成本效益和真正高性能的新标准的方向趋同。我们很自豪也很热心成为超级以太网联盟（UEC）的创始成员之一，该联盟旨在通过基于以太网的通信协议和软件堆栈来应对这些挑战。我们相信UEC将提供强劲的结果，以满足市场需求和要求。”——Eric Eppe, Group VP, HPC/AI/Quantum Portfolio &amp; Strategy for Eviden at Atos Group.
“生成式 AI 工作负载将要求我们构建网络以实现超级计算规模和性能。超级以太网联盟的重要性在于开发一个开放、可扩展且经济高效的基于以太网的通信堆栈，以支持这些高性能工作负载高效运行。以太网的普遍性和互操作性将为客户提供选择，以及处理各种数据密集型工作负载的性能，包括模拟以及AI模型的训练和调整。随着人工智能模型的数据和规模的持续增长，高度并行化的计算成为性能、可靠性和可持续性的重要组成部分。”—— Justin Hotard, executive vice president and general manager, HPC &amp; AI, at Hewlett Packard Enterprise.
”人工智能、机器学习和大规模高性能工作负载的计算和网络性能需求是永不满足的。该行业需要开放式解决方案来满足这些需求，以实现专有解决方案的选择和自由。英特尔很荣幸成为超级以太网联盟 （UEC） 的创始成员，该联盟将通过更新和优化的基于以太网的高性能、可扩展和开放的网络解决方案和通信堆栈迎来未来的计算基础设施。“——Jeff McVeigh, corporate vice president &amp; general manager of the Super Compute Group at Intel.
“下一代人工智能系统需要前所未有的规模和性能。Meta 致力于构建高性能以太网结构和技术的开放生态系统，以实现下一个计算时代。”——Alexis Björlin, Vice President of Infrastructure, AI Systems and Accelerated Platforms at Meta.
“下一个计算时代的特点将是人工智能和人工智能优化的基础设施取得突破性进展，Microsoft致力于使组织能够利用Azure的强大功能推动一切可能。联手开发一套通用标准，以增强超大规模人工智能和高性能计算工作负载的以太网，将有助于实现现在和未来的持续创新。”——Steve Scott, Corporate Vice President of Azure Hardware Architecture at Microsoft.
]]></content>
  </entry>
  
  <entry>
    <title>英伟达的芯片版图</title>
    <url>/post/datacenter/nvidia-chip-layout.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Nvidia</tag>
      <tag>GPU</tag>
      <tag>CUDA</tag>
    </tags>
    <content type="html"><![CDATA[当英伟达市值站上万亿美元之巅时，全球最神秘资本大鳄罗斯柴尔德家族却传出减持英伟达股票的消息，股神巴菲特也作出了相同选择。
这一最新投资动向似乎传递出一个信号：支撑英伟达万亿美元市值大厦的支柱，或许没有大家想象得那般牢固。站在芯片市值之颠的英伟达，其实少了一块芯片“拼图”。
“三剑客”的芯片版图 在AI时代，仅凭借GPU或CPU，恐怕很难胜任高算力需求的大模型训练与推理。作为英伟达强有力的竞争对手，英特尔和AMD明显意识到了这点，不仅加快开发对标英伟达GPU的直接竞品，更是纷纷出手布局整个芯片版图。
各企业加紧研发AI芯片 在GPU领域，英特尔其实是一位“老玩家”。1998年2月12日，英特尔发布了旗下首款独立GPU产品——英特尔i740 AGP显卡，但是i740的各项性能表现远不及预期，无法与同期英伟达的RIVA TNT同台竞技，故而在一年后含恨退市。而于2009年发布Larrabee也并未收获良好的市场表现，最终英特尔不得不终止该计划。虽在独立GPU上屡屡受挫，但自i740开始的技术积累和CPU的强大底蕴使得英特尔摸索出集成显卡的发展路径。即便如此，英特尔也仍未放弃对独立GPU的探索。2017年，英特尔推出Xe架构，这成为几年后英特尔开发GPU的基石架构。
英特尔首款GPU i740
英特尔对芯片的追逐不仅仅只有CPU和GPU。2015年，英特尔以167亿美元高调收购元老级FPGA厂商Altera，以求在芯片设计上弥补自身芯片灵活性不足的短板。2018年，英特尔首次推出了负责神经网络处理的处理器IPU。在同年的英特尔架构日上，英特尔向业界提出了XPU异构愿景：一个由标量、矢量、矩阵、空间组成的SVMS架构——分别对应了CPU、GPU、IPU和FPGA，可进行多种异构组合。
AMD公布第四代EPYC家族处理器一系列更新
AMD在GPU上也马不停蹄。AMD的GPU发展历程以2006年7月收购图形处理公司ATI为分水岭。被收购前的ATI凭借Radeon系列的自身性能与英伟达直接竞争；而收购ATI后的AMD，则利用高性价比战略抢占中端市场，并在此后不断同英伟达的GPU进行对抗和拉扯。
2022年2月，AMD斥资498亿美元完成对FPGA厂商赛灵思的收购，以加强在数据中心业务的布局；同年4月，AMD宣布以19亿美元收购DPU芯片厂商Pensando，继续扩大数据中心业务。今年6月13日在美国旧金山的发布会上，CEO苏姿丰发布了多款面向数据中心领域的产品，AMD的“CPU+GPU+FPGA+DPU”的芯片版图也搭建完成，并逐渐成熟。
面对挑战者的层层加码，英伟达不敢有丝毫怠慢。
英伟达Grace Hopper使用Grace CPU
英伟达深知自己能成为当今AI芯片的领头羊，依靠的绝不仅是CUDA平台这一条护城河。英伟达在手握GPU和软件生态上的先发优势的同时，也积极向CPU探索。虽然英伟达对Arm的收购计划因反垄断原因以失败告终，但其在CPU上布局的脚步仍未停止。早在2021年，英伟达便已经发布了名为Grace的处理器，而Grace Hopper于今年在ISC上亮相。这款包含了Hopper架构和Grace CPU的超级处理器，彰显了英伟达对CPU的野心。
另外，2019年，英伟达以69亿美元的价格收购以色列网络芯片公司Mellanox，并于同年推出BlueField-2 DPU，自此拉开DPU高速发展的序幕，并实现了“CPU+GPU+DPU”三芯布局。
对比三家的芯片版图，不难发现无论英特尔的XPU封装，还是AMD的异构集成，均把FPGA作为重要砝码。相反，英伟达在FPGA的部署上则稍显犹豫。业界不禁发出疑问：FPGA有多重要？它会是目前正坐在AI王座上的英伟达所忽略遗漏的那一块拼图吗？
英伟达没有FPGA FPGA这块拼图，看似市场体量不大，但其实非常重要。与GPU相比，FPGA作为可编程逻辑列阵，具备更低能耗、更强的灵活度和可编辑性，因而具备较短的设计周期，更适合算法快速迭代、应用场景不断拓展的AI时代。
各类芯片功能对比 进入AI时代，FPGA广泛应用于云计算、网络，如5G通信和边缘、端的垂直市场。整体而言，高性能计算与云服务提供商是FPGA扮演的两大重要角色。
英特尔FPGA产品图
“目前，AI在市场上仍处于早期萌芽状态，很多客户都在探索AI在边缘及嵌入式市场中究竟能够为大家带来什么。而FPGA的内在价值恰恰是可编程性和灵活性，可以赋予AI更高的灵活性。当客户在边缘领域运行AI相关工作负载时，FPGA能够为其特定工作负载进行优化，这一点至关重要。”英特尔数据中心与人工智能集团副总裁兼可编程解决方案事业部产品营销总经理Deepali Trehan对《中国电子报》记者表示。
正是意识到了FPGA未来可期，英特尔和AMD纷纷展开收购FPGA厂商的行动。而被两大巨头收入囊中的Altera和赛灵思，一度占据80%以上的市场份额。完成收购后，英特尔、AMD在业务、营收规模、对下游的议价权等方面，会获得更大主动权。
AMD收购FPGA企业赛灵思
AMD AECG 有线与无线事业部高级总监 Gilles Garcia对《中国电子报》记者表示：“此前赛灵思在通信领域一直处于领先地位。AMD对赛灵思的收购使得AMD处理器与赛灵思原有产品在5G领域实现强强联合，在7大5G无线设备制造商中，AMD现已部署6家。”
当前市场上已难有堪比Altera和赛灵思的优质标的。由于错失了通过收购抢占市场的先手棋，英伟达目前几乎不可能再通过收购来补充FPGA这块拼图。
总体而言，相对于较早布局FPGA的英特尔和AMD，没有FPGA产品线的英伟达在技术解决方案的多样性和灵活性上略逊一筹。
毕竟，高性能计算芯片不止GPU这一处理架构，“CPU+FPGA”的方案同样能满足AI的算力需求。FPGA芯片作为可编程芯片，可以针对特定功能进行扩展，在AI模型构建第二阶段具有一定的发挥空间。通过与CPU结合，FPGA能够实现深度学习功能，两者共同应用于深度学习模型。
既然已经错过了收购的最佳时期，那么自研FPGA或许是另一条路。记者注意到，英伟达近日在求职网站上发布了招聘FPGA工程师的招聘信息。对于FPGA，英伟达或许已经暗中行动。
但是，英伟达研发FPGA的道路并非坦途，短期来看性价比仍然较低。
“FPGA研发的难点在于工具链和工艺，需要大量人力、物力、财力。”澎峰科技联合创始人兼首席运营官王军辉对《中国电子报》记者表示，行业通常将CPU、GPU、DSP（已经提的越来越少了）和FPGA这四类计算硬件，从计算能效、计算密度、计算性能、易用性等几个维度去进行对比，看它们适合于什么市场。在大算力、大模型时代，英伟达的“GPU+CUDA”方向可作为一个选择。
]]></content>
  </entry>
  
  <entry>
    <title>聊聊闪存存储的延迟可预测性</title>
    <url>/post/datacenter/predictability-on-unpredictable-flash-storage-with-a-light-neural-network.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Storage</tag>
      <tag>SSD</tag>
      <tag>LinnOS</tag>
    </tags>
    <content type="html"><![CDATA[如何利用神经网络预测闪存尾端延迟的发生
如何利用神经网络预测闪存尾端延迟的发生 由于用户对低且稳定的延迟（微秒级）的需求越来越大，人们对SSD的百分比延迟越来越关心，即SSD有99%的概率可以提供低且稳定的延迟，但有1%的概率产生几倍于正常情况的延迟，而这1%的高延迟被称为尾端延迟。尾端延迟有什么影响？如何降低尾端延迟的影响？如何在存储环境下利用神经网络？这些疑问，本文将一一解答。
尾端延迟与Hedged Request 百分比延迟 也许你已经查了维基百科中”百分比延迟“的定义，但我想对大多数人而言有点晦涩难懂，下面我将举一个简单的例子以帮助你理解。
首先，我们先列举出一系列收集到的延迟：
23，20，21，20，23，20，45，21，25，25
对它们排序：
20，20，20，21，21，23，23，25，25，45
接下来可以选择前x%的延迟，例如假设我们想要得到50th百分比延迟，则选择前5个延迟：
20，20，20，21，21
然后选择这一组延迟中最大的那个——即21——就是这一组延迟中的50th百分比延迟（也可以写作p50），同理，p90是25。
尾端延迟 尾端延迟就是百分比延迟中末尾的（通常p99之后）那些延迟。看起来尾端延迟占比并不多，但当系统处理的请求达到10^6个数量级，可能足足有104个请求处理延迟远高于正常情况——你不会想成为那不幸的1%，对吗？
分析SSD的内部行为后，本文作者认为尾端延迟的产生源自SSD内部日益复杂的内部活动，如垃圾回收、负载均衡等，和用户请求的冲突。为了降低尾端延迟或者降低尾端延迟的影响，业界提出的方案分为两大类：
 白盒子方案  此时SSD内部的行为可知，通过改进SSD内部架构来降低尾端延迟。这种方式无疑是直接而强有效的，但是不利于推广到市场。
 灰盒子方案  此时不需要修改SSD的内部架构，但是需要修改上层的软件栈。
 黑盒子方案  以各种预测为代表，既不需要修改上层软件栈，也不需要修改SSD内部架构，是目前最流行的解决方案。其中一个经典的方案是Hedged Request，它的原理和应用环境将在下文中介绍。
Hedged Request 为了保证数据安全、实现负载均衡，现代的存储系统通常存在一定冗余，而多个不同的SSD的内部行为同时和用户请求产生冲突的概率非常低。基于这样的思考，Hedged Request将一个请求发给一个SSD后，若等待请求完成的时间超过了阈值，则重发请求到另一个可用的SSD。如下图所示：
然而，传统Hedged Request中，快SSD需要等待一段时间（等待慢SSD处理的时间超过阈值）后才能处理请求，对于微秒级SSD而言，这个等待时间是致命的。如果可以学习SSD的特征，预测将要变慢的SSD而及时将请求重发到快SSD中，则可以节约出等待的时间，从而降低闪存组的尾端延迟——这就是LinnOS完成的工作，如下图所示，用户发送请求后，若经过LinnOS网络预测得知该SSD将变慢，则提前告知用户重发请求，随后请求将被送到下一个SSD，减少了Hedged Request中的等待时间。
LinnOS的三大挑战 设计LinnOS存在三大挑战，接下来将一一阐述。
 对用户输出什么结果？  需要输出具体的延迟（如120μs）吗？虽然这样更灵活，但是一方面，对用户而言，120μs或者125μs其实区别不大，另一方面，如此精确的输出意味着准确率低，并不划算。那么如果输出一个延迟范围，如80~100μs、100~120μs呢？此时准确率稍高了些，但不够（仅60%-70%），处于区间交界处的延迟往往预测不准确。回顾Hedged Request的原理，其实对用户而言，知道SSD是”快“或者”慢“就足够了！所以LinnOS使用简单的二分类模型。
 使用什么信息进行预测？  看起来一系列信息都和SSD快或慢有关：读写请求？请求的块内偏移？长期的写入历史？然而，作者发现这些请求都对提高精确度没有明显帮助。首先，由于当前SSD常有内置写缓存，写之后的读延迟常常没有明显提高，更为常见的其实是数据从缓存”冲“（flush）入SSD后，读延迟会更高。其次，一组I/O请求会通过条带均匀地写入各个通道或者芯片，它们写入同一个芯片的概率很低，所以块内偏移这个特征其实并不重要。最后，GC或者flush通常发生时间短，短期写入历史足矣预测。
因此，可以使用SSD当前I/O队列长度来预测SSD快或者慢：一个直观的感受是，当I/O队列较长时，SSD处理通常比较慢。但是这样并不能体现SSD的内部活动的发生，因此额外增加了历史四条请求进入SSD时的队列长度和完成请求的时间。若某个请求进入SSD时队列短而完成请求的时间长，意味着SSD内部行为可能和用户请求冲突了。
 如何最小化预测错误的影响？  作者分析发现，若将一个快的SSD预测为慢的从而错误地重发了，将带来微秒级延迟，而若将一个慢的SSD预测为快，将带来毫秒级延迟，比第一种情况严重许多，所以作者在训练时对第二种情况施加了更加严重的惩罚以减少它们的发生。此外，还补充了hedged request以减少预测失败的损失。
 实验结果与总结  作者上层使用了不同的软件产生负载，底层使用同构的消费级SSD阵列或者异构企业级SSD阵列测试它们的表现，以读延迟为指标展示结果。总共比较了7种不同的方案：
 Base：无优化 Clone：同时发送两份请求，选择先返回的SSD的结果返回给用户 Hedge95：等待p95之后重发请求 Hedge IP（inflection point）：和上一个相比，使用针对负载优化后的等待时间 HeurSim：队列较长时重发请求 HeurAdv：队列较长、且考虑历史信息（和LinnOS一样）后决定重发请求 LinnOS-Raw：没有hedged补偿的LinnOS LinnOS+HL：最终的LinnOS方案  实验结果如下图：
]]></content>
  </entry>
  
  <entry>
    <title>DDRC中的RAS：Parity & ECC</title>
    <url>/post/datacenter/DDRC-RAS-Parity-and-ECC.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>DDRC</tag>
      <tag>RAS</tag>
      <tag>Parity & ECC</tag>
    </tags>
    <content type="html"><![CDATA[DDRC 中常见的RAS功能有两种：一种为Parity（奇偶）校验，一种为ECC（纠错码）校验。
什么是RAS？ DDR subsystem会因为设计bug或传输过程中的干扰导致bit翻转出现数据错误，其中设计缺陷导致的通常认为是硬错误，而系统噪声或者其他原因导致DRAM阵列翻转的则认为是软错误。硬错误是永久性的错误，而软错误则是短暂性的。为了能够在运行中纠正软错误，保障Memory subsystem的稳定运行，DDRC（DDR Controller）需要具有先进的RAS功能（可靠性，可用性，可维护性功能）。如果没有 RAS 功能，系统很可能会因为内存错误而崩溃。RAS 功能允许系统在出现可纠正的错误时继续运行，同时记录不可纠正错误的详细信息，以便将来进行调试。DDRC 中常见的RAS功能有两种：一种为Parity（奇偶）校验，一种为ECC（纠错码）校验。
Parity Check 奇偶校验是最简单的一种校验码，用于检测数据传输过程中是否发生错误，但是parity check只能用于检测1bit error，如果碰上2bit error，则无法检测，而且也无法区分出错是在data上出错，还是校验位出错。其次parity check也无法修复出错的数据。Parity check分为Odd parity check（奇校验）和Even parity check（偶校验）。
 奇校验：原始数据+校验位 总共有奇数个1 偶校验：原始数据+校验位 总共有偶数个1  Image DDRC中通常会有一根单独的信号用于传输Parity bit, 并且Pairty Check方式一般可以通过寄存器配置。
ECC Check DDRC会针对数据产生ECC bits，并将ECC bits存储到DRAM存储器中, 可对DRAM发送的数据进行1bit ECC error纠错或者2bit ECC error检错。
ECC生成校验数据步骤一般为：
  DDRC接收到写数据，在DDRC内部产生ECC bits，然后将写数据和产生的ECC bits一起写入颗粒
  DDRC收到读请求，DDRC从DRAM读取数据和对应ECC bits，然后DDRC通过读回数据重新产生ECC bits，然后和从DRAM读回的ECC数据进行比较。如果两者匹配，则认为该传输没有Error，如果不match，则开始检测是否为1bit ECC error或者2bit ECC error，如果为1bit ECC Error，则纠正单bit错误，然后继续传输。如果为2bit ECC error，直接通过中断上报。一般针对1bit ECC error，也可以设计一个阈值，例如一定时间内出现多少1bit ECC error, 也会上报中断。
  ECC bit根据存储在DRAM中的位置可以分为两种类型：side-band ECC 或者 inline ECC。在 side-band ECC 中，ECC 数据存储在单独的 DRAM 上；在inline ECC 中，ECC 数据与实际数据一起存储在同一个 DRAM 上。
Side-band ECC Side-band ECC一般运用在标准DDR内存（DDR4，DDR5）或者HBM中。ECC bits会作为side-band数据同实际数据一起发送到内存。例如64位数据，会增加8个数据位宽用于传输ECC bits。因此，一般企业级DDR4 ECC DIMM会有72bit位宽，并且DIMM会有两个额外x4 DRAM或者一个x8DRAM用于ECC bits存储。而对于HBM2E，则是通过复用DM（data mask）信号作为ECC bits传输，同样HBM中也有额外的区域用于存储ECC bits。因为Side-band ECC中，DDRC会同时写入或者读取ECC bits和实际数据，这种场景下不需要额外的WR或者RD command，因此和inline ECC方案相比，会有更高的Efficiency。
Side-band ECC的WR 和 RD 操作流程：
Inline ECC inline ECC常见于LPDDR或者GDDR中。通常LPDDR或者GDDR为固定信道宽度(GDDR6为x16), 因此side-band因为需要额外的side-band信号，这需要额外的昂贵花销。例如，对于 16 位数据宽度，需要为 7 位或 8 位 ECC 位宽的 inline ECC 额外分配 16 位 LPDDR 信道。此外，7 或 8 位 ECC 数据字段仅部分填充了 16 位额外的通路，导致存储效率低下，还给地址命令信道带来额外负载，可能会对性能有所影响。
Inline ECC 中的控制器不需要额外的信道来存储 ECC，而是将 ECC 数据存储在存储实际数据的同一 DRAM 信道中。因此，内存信道的总体数据宽度与实际数据宽度相同。同side-band一样，假设64bit Data则对应8bit数据，由于ECC bits和Data存储在同一个DRAM，因此需要1/9的DRAM容量作为ECC bits区域，其余8/9则用于存放数据。
当 ECC 数据未与读写数据一起发送时，控制器为 ECC 数据生成单独的开销 WR 和 RD 命令。因此，实际数据的每条 WR 和 RD 命令都伴有一条 ECC 数据的开销 WR 和 RD 命令。高性能控制器通过在一条 ECC WR 命令中封装几个连续地址的 ECC 数据，以此来降低此类 ECC 命令的损失。同样，控制器在一条 ECC RD 命令中读取内存发出的若干连续地址的 ECC 数据，并且可以将读出的 ECC 数据，应用于该连续地址产生的实际数据。因此traffic越有序，inline ECC导致的Efficiency Loss越小。假设GDDR中我们通过AXI发送AwLen=7，AwSize=5的一笔AXI transaction，那么一次发送2^5 * 8 = 256Byte的数据，并且产生32Byte的ECC bits。根据GDDR6的Spec，一个WR CMD能够写入32Byte的数据，那么正好9个cmd 能够完成256B data+32B ECC bits的写入，此时inline ECC导致的速率损失最小。因此可以计算得到inline ECC enable之后最大Efficiency为disable inline ECC的8/9 = 88.89%。如果不够64bit，即narrow transfer小于8Byte的场景，则还需要通过先读回DRAM内的数据，和narrow transfer data拼成64bit 计算 ECC bits，然后再写回DRAM，这种场景下inline ECC带来的效率损失会进一步扩大。
Inline ECC 的 WR 和 RD 操作流程：
]]></content>
  </entry>
  
  <entry>
    <title>61.44TB！全球第一SSD诞生：QLC闪存70年写不死</title>
    <url>/post/datacenter/the-birth-of-the-world-first-SSD-of-61.44TB.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>Storage</tag>
      <tag>SSD</tag>
      <tag>Solidigm</tag>
      <tag>P5336</tag>
    </tags>
    <content type="html"><![CDATA[Solidigm公司近日发布了新一代旗舰企业级SSD D5-P5336，容量起步就有7.68TB，最高更是做到了史无前例的61.44TB，是此前纪录的整整两倍，1U机箱就可以达成2PB的存储空间！
Solidigm D5-P5336提供了多种形态、容量规格，首发出货的是E1.L，最高容量30.72TB。
今年晚些时候增加U.2并升级E1.L，最高容量升至61.44TB，明年还会扩大到E3.S，最高容量30.72TB。
它采用了QLC闪存，192层堆叠，企业级品质。
按照Solidigm此前给出的数据，最大写入寿命可达65PBW，五年质保期内平均每天可以写入最多35.6TB，甚至能够坚持使用长达70年都不会坏。
性能方面，目前官方只给出了首发的E1.L 15.36TB，支持PCIe 4.0 x4，顺序读写最高7GB/s、3.1GB/s，4K随机读取最高1005K IOPS，16KB随机写入最高35K IOPS，写入寿命14.11PBW，平均故障间隔时间200万小时。
Solidigm表示，相比于全TLC闪存阵列，D5-P5336可节省17％的TCO成本，机架占地面积减少1.9倍，供电和散热能耗减少1.25倍，对比全SAS硬盘阵列、混合阵列的优势更大。
如果61.44TB还是不够用，也有办法，一块不行就多块！
Apex Storage曾经发布过一款SSD扩展卡“X21”，双卡并排，可以挂载21块M.2 SSD，单个最大容量8TB，合计最多168TB，而凭借100条PCIe 4.0系统通道，顺序读写速度可达31GB/s。
现在，Apex Storage又发布了一款“X16”，支持16块SSD，看起来弱了一些，但其实更强悍了。
它只用了一块PCB，正反面都可以挂载8块M.2 SSD，每块还是最大8TB，合计容量最多128TB。
PCIe 4.0通道依然有84条，读写速度仍旧能达到31GB/s，也支持完整的UEFI、安全启动。
正面中间布置了一个主动风扇和一些散热片，为PLX桥接控制器散热，同时让整体体积只有单插槽厚度，非常迷你。
X21扩展卡的价格为2800美元(约合人民币2.0万元)，X16则是1800美元(约合人民币1.3万元)。
当然，SSD你得另外单独购买。
今年PCIe 5.0硬盘终于迎来了一波爆发，多家厂商的旗舰SSD都换成PCIe 5.0的了，主控也多是群联E26，速度冲上14GB/s，然而享受超高速度不是没代价，最近有多款硬盘热到崩溃了。
出现问题的SSD此前主要是Corsair MP700，现在有用户报告Seagate FireCuda 540、Gigabyte Aorus Gen5 10000和Adata Legend 970等PCIe 5.0硬盘也会有问题，就是使用过程中会突然崩溃、关机。
问题的根源并不负责，这些SSD设计之初是搭配散热器使用的，但是如果是没有散热器的情况下，PCIe 5.0的高性能会导致大量热量产生，SSD的保护机制甚至做不到过热降频，就直接停止工作了。
解决方法也不复杂，已有厂商承诺发布固件升级，群联最新的22.1固件引入了链路状态热调节功能，降低了PCIe接口速度，也就是从5.0降至4.0甚至3.0，这样就能降低PCIe物理层温度，SSD不用热到崩溃、关机。
当然，这样做就是变相降低了性能，PCIe 5.0变成PCIe 4.0、3.0而已。
彻底解决办法就是上散热器，不要在没有散热器的情况下使用PCIe 5.0硬盘，甚至传统的被动散热都不够用了，最好是上那种带有小风扇的主动散热。
如果风扇还不够用，还有厂商推出了SSD水冷散热器，之前大家还嘲笑厂商，现在应该理解厂商的一片苦心了吧。
]]></content>
  </entry>
  
  <entry>
    <title>Linux性能优化</title>
    <url>/post/linux/linux-performance-optimization.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>performance</tag>
    </tags>
    <content type="html"><![CDATA[本文详细讲解了Linux性能优化的全景
Linux性能优化 性能优化 性能指标 高并发和响应快对应着性能优化的两个核心指标：吞吐和延时
 应用负载角度：直接影响了产品终端的用户体验 系统资源角度：资源使用率、饱和度等  性能问题的本质就是系统资源已经到达瓶颈，但请求的处理还不够快，无法支撑更多的请求。 性能分析实际上就是找出应用或系统的瓶颈，设法去避免或缓解它们。
 选择指标评估应用程序和系统性能 为应用程序和系统设置性能目标 进行性能基准测试 性能分析定位瓶颈 性能监控和告警  对于不同的性能问题要选取不同的性能分析工具。 下面是常用的Linux Performance Tools以及对应分析的性能问题类型。
到底应该怎么理解“平均负载” **平均负载：**单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数。它和我们传统意义上理解的CPU使用率并没有直接关系。
其中不可中断进程是正处于内核态关键流程中的进程（如常见的等待设备的I/O响应）。不可中断状态实际上是系统对进程和硬件设备的一种保护机制。
平均负载多少时合理 实际生产环境中将系统的平均负载监控起来，根据历史数据判断负载的变化趋势。当负载存在明显升高趋势时，及时进行分析和调查。 当然也可以当设置阈值（如当平均负载高于CPU数量的70%时）
现实工作中我们会经常混淆平均负载和CPU使用率的概念，其实两者并不完全对等：
 CPU密集型进程，大量CPU使用会导致平均负载升高，此时两者一致 I/O密集型进程，等待I/O也会导致平均负载升高，此时CPU使用率并不一定高 大量等待CPU的进程调度会导致平均负载升高，此时CPU使用率也会比较高  平均负载高时可能是CPU密集型进程导致，也可能是I/O繁忙导致。具体分析时可以结合mpstat/pidstat工具辅助分析负载来源
CPU CPU上下文切换(上) CPU上下文切换，就是把前一个任务的CPU上下文（CPU寄存器和PC）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的位置，运行新任务。其中，保存下来的上下文会存储在系统内核中，待任务重新调度执行时再加载，保证原来的任务状态不受影响。
按照任务类型，CPU上下文切换分为：
 进程上下文切换 线程上下文切换 中断上下文切换  进程上下文切换 Linux进程按照等级权限将进程的运行空间分为内核空间和用户空间。从用户态向内核态转变时需要通过系统调用来完成。
一次系统调用过程其实进行了两次CPU上下文切换：
 CPU寄存器中用户态的指令位置先保存起来，CPU寄存器更新为内核态指令的位置，跳转到内核态运行内核任务； 系统调用结束后，CPU寄存器恢复原来保存的用户态数据，再切换到用户空间继续运行。  系统调用过程中并不会涉及虚拟内存等进程用户态资源，也不会切换进程。和传统意义上的进程上下文切换不同。因此系统调用通常称为特权模式切换。
进程是由内核管理和调度的，进程上下文切换只能发生在内核态。 因此相比系统调用来说，在保存当前进程的内核状态和CPU寄存器之前，需要先把该进程的虚拟内存，栈保存下来。再加载新进程的内核态后，还要刷新进程的虚拟内存和用户栈。
进程只有在调度到CPU上运行时才需要切换上下文，有以下几种场景： CPU时间片轮流分配，系统资源不足导致进程挂起，进程通过sleep函数主动挂起，高优先级进程抢占时间片，硬件中断时CPU上的进程被挂起转而执行内核中的中断服务。
线程上下文切换 线程上下文切换分为两种：
 前后线程同属于一个进程，切换时虚拟内存资源不变，只需要切换线程的私有数据，寄存器等； 前后线程属于不同进程，与进程上下文切换相同。  同进程的线程切换消耗资源较少，这也是多线程的优势。
中断上下文切换 中断上下文切换并不涉及到进程的用户态，因此中断上下文只包括内核态中断服务程序执行所必须的状态（CPU寄存器，内核堆栈，硬件中断参数等）。
中断处理优先级比进程高，所以中断上下文切换和进程上下文切换不会同时发生
CPU上下文切换(下) 通过vmstat可以查看系统总体的上下文切换情况
vmstat 5 #每隔5s输出一组数据 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 103388 145412 511056 0 0 18 60 1 1 2 1 96 0 0 0 0 0 103388 145412 511076 0 0 0 2 450 1176 1 1 99 0 0 0 0 0 103388 145412 511076 0 0 0 8 429 1135 1 1 98 0 0 0 0 0 103388 145412 511076 0 0 0 0 431 1132 1 1 98 0 0 0 0 0 103388 145412 511076 0 0 0 10 467 1195 1 1 98 0 0 1 0 0 103388 145412 511076 0 0 0 2 426 1139 1 0 99 0 0 4 0 0 95184 145412 511108 0 0 0 74 500 1228 4 1 94 0 0 0 0 0 103512 145416 511076 0 0 0 455 723 1573 12 3 83 2 0  cs （context switch） 每秒上下文切换次数 in （interrupt） 每秒中断次数 r （runnning or runnable）就绪队列的长度，正在运行和等待CPU的进程数 b （Blocked） 处于不可中断睡眠状态的进程数  要查看每个进程的详细情况，需要使用pidstat来查看每个进程上下文切换情况
pidstat -w 5 14时51分16秒 UID PID cswch/s nvcswch/s Command 14时51分21秒 0 1 0.80 0.00 systemd 14时51分21秒 0 6 1.40 0.00 ksoftirqd/0 14时51分21秒 0 9 32.67 0.00 rcu_sched 14时51分21秒 0 11 0.40 0.00 watchdog/0 14时51分21秒 0 32 0.20 0.00 khugepaged 14时51分21秒 0 271 0.20 0.00 jbd2/vda1-8 14时51分21秒 0 1332 0.20 0.00 argusagent 14时51分21秒 0 5265 10.02 0.00 AliSecGuard 14时51分21秒 0 7439 7.82 0.00 kworker/0:2 14时51分21秒 0 7906 0.20 0.00 pidstat 14时51分21秒 0 8346 0.20 0.00 sshd 14时51分21秒 0 20654 9.82 0.00 AliYunDun 14时51分21秒 0 25766 0.20 0.00 kworker/u2:1 14时51分21秒 0 28603 1.00 0.00 python3  cswch 每秒自愿上下文切换次数 （进程无法获取所需资源导致的上下文切换） nvcswch 每秒非自愿上下文切换次数 （时间片轮流等系统强制调度）  vmstat 1 1 #首先获取空闲系统的上下文切换次数 sysbench --threads=10 --max-time=300 threads run #模拟多线程切换问题 vmstat 1 1 #新终端观察上下文切换情况 此时发现cs数据明显升高，同时观察其他指标： r列： 远超系统CPU个数，说明存在大量CPU竞争 us和sy列： sy列占比80%，说明CPU主要被内核占用 in列： 中断次数明显上升，说明中断处理也是潜在问题 说明运行/等待CPU的进程过多，导致大量的上下文切换，上下文切换导致系统的CPU占用率高
pidstat -w -u 1 #查看到底哪个进程导致的问题 从结果中看出是sysbench导致CPU使用率过高，但是pidstat输出的上下文次数加起来也并不多。分析sysbench模拟的是线程的切换，因此需要在pidstat后加-t参数查看线程指标。
另外对于中断次数过多，我们可以通过/proc/interrupts文件读取
watch -d cat /proc/interrupts 发现次数变化速度最快的是重调度中断（RES），该中断用来唤醒空闲状态的CPU来调度新的任务运行。分析还是因为过多任务的调度问题，和上下文切换分析一致。
某个应用的CPU使用率达到100%，怎么办？ Linux作为多任务操作系统，将CPU时间划分为很短的时间片，通过调度器轮流分配给各个任务使用。为了维护CPU时间，Linux通过事先定义的节拍率，触发时间中断，并使用全局变了jiffies记录开机以来的节拍数。时间中断发生一次该值+1.
CPU使用率，除了空闲时间以外的其他时间占总CPU时间的百分比。可以通过/proc/stat中的数据来计算出CPU使用率。因为/proc/stat时开机以来的节拍数累加值，计算出来的是开机以来的平均CPU使用率，一般意义不大。可以间隔取一段时间的两次值作差来计算该段时间内的平均CPU使用率。 性能分析工具给出的都是间隔一段时间的平均CPU使用率，要注意间隔时间的设置。
CPU使用率可以通过top 或 ps来查看。分析进程的CPU问题可以通过perf，它以性能事件采样为基础，不仅可以分析系统的各种事件和内核性能，还可以用来分析指定应用程序的性能问题。
perf top / perf record / perf report （-g 开启调用关系的采样）
sudo docker run --name nginx -p 10000:80 -itd feisky/nginx sudo docker run --name phpfpm -itd --network container:nginx feisky/php-fpm ab -c 10 -n 100 http://XXX.XXX.XXX.XXX:10000/ #测试Nginx服务性能 发现此时每秒可承受请求给长少，此时将测试的请求数从100增加到10000。 在另外一个终端运行top查看每个CPU的使用率。发现系统中几个php-fpm进程导致CPU使用率骤升。
接着用perf来分析具体是php-fpm中哪个函数导致该问题。
perf top -g -p XXXX #对某一个php-fpm进程进行分析 发现其中sqrt和add_function占用CPU过多， 此时查看源码找到原来是sqrt中在发布前没有删除测试代码段，存在一个百万次的循环导致。 将该无用代码删除后发现nginx负载能力明显提升
系统的CPU使用率很高，为什么找不到高CPU的应用？ sudo docker run --name nginx -p 10000:80 -itd feisky/nginx:sp sudo docker run --name phpfpm -itd --network container:nginx feisky/php-fpm:sp ab -c 100 -n 1000 http://XXX.XXX.XXX.XXX:10000/ #并发100个请求测试 实验结果中每秒请求数依旧不高，我们将并发请求数降为5后，nginx负载能力依旧很低。
此时用top和pidstat发现系统CPU使用率过高，但是并没有发现CPU使用率高的进程。
出现这种情况一般时我们分析时遗漏的什么信息，重新运行top命令并观察一会。发现就绪队列中处于Running状态的进行过多，超过了我们的并发请求次数5. 再仔细查看进程运行数据，发现nginx和php-fpm都处于sleep状态，真正处于运行的却是几个stress进程。
下一步就利用pidstat分析这几个stress进程，发现没有任何输出。用ps aux交叉验证发现依旧不存在该进程。说明不是工具的问题。再top查看发现stress进程的进程号变化了，此时有可能时以下两种原因导致：
 进程不停的崩溃重启（如段错误/配置错误等），此时进程退出后可能又被监控系统重启； 短时进程导致，即其他应用内部通过exec调用的外面命令，这些命令一般只运行很短时间就结束，很难用top这种间隔较长的工具来发现  可以通过pstree来查找 stress的父进程，找出调用关系。
pstree | grep stress 发现是php-fpm调用的该子进程，此时去查看源码可以看出每个请求都会调用一个stress命令来模拟I/O压力。 之前top显示的结果是CPU使用率升高，是否真的是由该stress命令导致的，还需要继续分析。 代码中给每个请求加了verbose=1的参数后可以查看stress命令的输出，在中断测试该命令结果显示stress命令运行时存在因权限问题导致的文件创建失败的bug。
此时依旧只是猜测，下一步继续通过perf工具来分析。性能报告显示确实时stress占用了大量的CPU，通过修复权限问题来优化解决即可.
系统中出现大量不可中断进程和僵尸进程怎么办？ 进程状态  R Running/Runnable，表示进程在CPU的就绪队列中，正在运行或者等待运行； D Disk Sleep，不可中断状态睡眠，一般表示进程正在跟硬件交互，并且交互过程中不允许被其他进程中断； Z Zombie，僵尸进程，表示进程实际上已经结束，但是父进程还没有回收它的资源； S Interruptible Sleep，可中断睡眠状态，表示进程因为等待某个事件而被系统挂起，当等待事件发生则会被唤醒并进入R状态； I Idle，空闲状态，用在不可中断睡眠的内核线程上。 该状态不会导致平均负载升高； T Stop/Traced，表示进程处于暂停或跟踪状态（SIGSTOP/SIGCONT， GDB调试）； X Dead，进程已经消亡，不会在top/ps中看到。  对于不可中断状态，一般都是在很短时间内结束，可忽略。但是如果系统或硬件发生故障，进程可能会保持不可中断状态很久，甚至系统中出现大量不可中断状态，此时需注意是否出现了I/O性能问题。
僵尸进程一般多进程应用容易遇到，父进程来不及处理子进程状态时子进程就提前退出，此时子进程就变成了僵尸进程。大量的僵尸进程会用尽PID进程号，导致新进程无法建立。
磁盘O_DIRECT问题 sudo docker run --privileged --name=app -itd feisky/app:iowait ps aux | grep &#39;/app&#39; 可以看到此时有多个app进程运行，状态分别时Ss+和D+。其中后面s表示进程是一个会话的领导进程，+号表示前台进程组。
其中进程组表示一组相互关联的进程，子进程是父进程所在组的组员。 会话指共享同一个控制终端的一个或多个进程组。
用top查看系统资源发现：1）平均负载在逐渐增加，且1分钟内平均负载达到了CPU个数，说明系统可能已经有了性能瓶颈；2）僵尸进程比较多且在不停增加；3）us和sys CPU使用率都不高，iowait却比较高；4）每个进程CPU使用率也不高，但有两个进程处于D状态，可能在等待IO。
分析目前数据可知：iowait过高导致系统平均负载升高，僵尸进程不断增长说明有程序没能正确清理子进程资源。
用dstat来分析，因为它可以同时查看CPU和I/O两种资源的使用情况，便于对比分析。
dstat 1 10 #间隔1秒输出10组数据 可以看到当wai（iowait）升高时磁盘请求read都会很大，说明iowait的升高和磁盘的读请求有关。接下来分析到底时哪个进程在读磁盘。
之前top查看的处于D状态的进程号，用pidstat -d -p XXX 展示进程的I/O统计数据。发现处于D状态的进程都没有任何读写操作。 在用pidstat -d 查看所有进程的I/O统计数据，看到app进程在进行磁盘读操作，每秒读取32MB的数据。进程访问磁盘必须使用系统调用处于内核态，接下来重点就是找到app进程的系统调用。
sudo strace -p XXX #对app进程调用进行跟踪 报错没有权限，因为已经时root权限了。所以遇到这种情况，首先要检查进程状态是否正常。 ps命令查找该进程已经处于Z状态，即僵尸进程。
这种情况下top pidstat之类的工具无法给出更多的信息，此时像第5篇一样，用perf record -d和perf report进行分析，查看app进程调用栈。
看到app确实在通过系统调用sys_read()读取数据，并且从new_sync_read和blkdev_direct_IO看出进程时进行直接读操作，请求直接从磁盘读，没有通过缓存导致iowait升高。
通过层层分析后，root cause是app内部进行了磁盘的直接I/O。然后定位到具体代码位置进行优化即可。
僵尸进程 上述优化后iowait显著下降，但是僵尸进程数量仍旧在增加。首先要定位僵尸进程的父进程，通过pstree -aps XXX，打印出该僵尸进程的调用树，发现父进程就是app进程。
查看app代码，看看子进程结束的处理是否正确（是否调用wait()/waitpid(),有没有注册SIGCHILD信号的处理函数等）。
碰到iowait升高时，先用dstat pidstat等工具确认是否存在磁盘I/O问题，再找是哪些进程导致I/O，不能用strace直接分析进程调用时可以通过perf工具分析。
对于僵尸问题，用pstree找到父进程，然后看源码检查子进程结束的处理逻辑即可。
CPU性能指标   CPU使用率
 用户CPU使用率, 包括用户态(user)和低优先级用户态(nice). 该指标过高说明应用程序比较繁忙. 系统CPU使用率, CPU在内核态运行的时间百分比(不含中断). 该指标高说明内核比较繁忙. 等待I/O的CPU使用率, iowait, 该指标高说明系统与硬件设备I/O交互时间比较长. 软/硬中断CPU使用率, 该指标高说明系统中发生大量中断. steal CPU / guest CPU, 表示虚拟机占用的CPU百分比.    平均负载
理想情况下平均负载等于逻辑CPU个数,表示每个CPU都被充分利用. 若大于则说明系统负载较重.
  进程上下文切换
包括无法获取资源的自愿切换和系统强制调度时的非自愿切换. 上下文切换本身是保证Linux正常运行的一项核心功能. 过多的切换则会将原本运行进程的CPU时间消耗在寄存器,内核占及虚拟内存等数据保存和恢复上
  CPU缓存命中率
CPU缓存的复用情况,命中率越高性能越好. 其中L1/L2常用在单核,L3则用在多核中
  性能工具  平均负载案例  先用uptime查看系统平均负载 判断负载在升高后再用mpstat和pidstat分别查看每个CPU和每个进程CPU使用情况.找出导致平均负载较高的进程.   上下文切换案例  先用vmstat查看系统上下文切换和中断次数 再用pidstat观察进程的自愿和非自愿上下文切换情况 最后通过pidstat观察线程的上下文切换情况   进程CPU使用率高案例  先用top查看系统和进程的CPU使用情况,定位到进程 再用perf top观察进程调用链,定位到具体函数   系统CPU使用率高案例  先用top查看系统和进程的CPU使用情况,top/pidstat都无法找到CPU使用率高的进程 重新审视top输出 从CPU使用率不高,但是处于Running状态的进程入手 perf record/report发现短时进程导致 (execsnoop工具)   不可中断和僵尸进程案例  先用top观察iowait升高,发现大量不可中断和僵尸进程 strace无法跟踪进程系统调用 perf分析调用链发现根源来自磁盘直接I/O   软中断案例  top观察系统软中断CPU使用率高 查看/proc/softirqs找到变化速率较快的几种软中断 sar命令发现是网络小包问题 tcpdump找出网络帧的类型和来源, 确定SYN FLOOD攻击导致    根据不同的性能指标来找合适的工具:
在生产环境中往往开发者没有权限安装新的工具包,只能最大化利用好系统中已经安装好的工具. 因此要了解一些主流工具能够提供哪些指标分析.
先运行几个支持指标较多的工具, 如top/vmstat/pidstat,根据它们的输出可以得出是哪种类型的性能问题. 定位到进程后再用strace/perf分析调用情况进一步分析. 如果是软中断导致用/proc/softirqs
CPU优化   应用程序优化
 编译器优化: 编译阶段开启优化选项, 如gcc -O2 算法优化 异步处理: 避免程序因为等待某个资源而一直阻塞,提升程序的并发处理能力. (将轮询替换为事件通知) 多线程代替多进程: 减少上下文切换成本 善用缓存: 加快程序处理速度    系统优化
 CPU绑定: 将进程绑定要1个/多个CPU上,提高CPU缓存命中率,减少CPU调度带来的上下文切换 CPU独占: CPU亲和性机制来分配进程 优先级调整:使用nice适当降低非核心应用的优先级 为进程设置资源显示: cgroups设置使用上限,防止由某个应用自身问题耗尽系统资源 NUMA优化: CPU尽可能访问本地内存 中断负载均衡: irpbalance,将中断处理过程自动负载均衡到各个CPU上    TPS、QPS、系统吞吐量的区别和理解
  QPS (Queries Per Second)每秒查询率,一台服务器每秒能够响应的查询次数.
  TPS (Transactions Per Second)每秒事务数,软件测试的结果.
  用户请求服务器
  服务器内部处理
  服务器返回给客户
QPS类似TPS,但是对于一个页面的访问形成一个TPS,但是一次页面请求可能包含多次对服务器的请求,可能计入多次QPS
    系统吞吐量, 包括几个重要参数:
  QPS(TPS)
  并发数
  响应时间
QPS(TPS)=并发数/平均相应时间
      内存 Linux内存是怎么工作的 内存映射 大多数计算机用的主存都是动态随机访问内存(DRAM)，只有内核才可以直接访问物理内存。Linux内核给每个进程提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样进程就可以很方便的访问内存(虚拟内存)。
虚拟地址空间的内部分为内核空间和用户空间两部分，不同字长的处理器地址空间的范围不同。32位系统内核空间占用1G，用户空间占3G。 64位系统内核空间和用户空间都是128T，分别占内存空间的最高和最低处，中间部分为未定义。
并不是所有的虚拟内存都会分配物理内存，只有实际使用的才会。分配后的物理内存通过内存映射管理。为了完成内存映射，内核为每个进程都维护了一个页表，记录虚拟地址和物理地址的映射关系。页表实际存储在CPU的内存管理单元MMU中，处理器可以直接通过硬件找出要访问的内存。
当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入内核空间分配物理内存，更新进程页表，再返回用户空间恢复进程的运行。
MMU以页为单位管理内存，页大小4KB。为了解决页表项过多问题Linux提供了多级页表和HugePage的机制。
虚拟内存空间分布 用户空间内存从低到高是五种不同的内存段：
 只读段 代码和常量等 数据段 全局变量等 堆 动态分配的内存，从低地址开始向上增长 文件映射 动态库、共享内存等，从高地址开始向下增长 栈 包括局部变量和函数调用的上下文等，栈的大小是固定的。一般8MB  内存分配与回收 分配 malloc对应到系统调用上有两种实现方式：
 brk() 针对小块内存(&lt;128K)，通过移动堆顶位置来分配。内存释放后不立即归还内存，而是被缓存起来。 **mmap()**针对大块内存(&gt;128K)，直接用内存映射来分配，即在文件映射段找一块空闲内存分配。  前者的缓存可以减少缺页异常的发生，提高内存访问效率。但是由于内存没有归还系统，在内存工作繁忙时，频繁的内存分配/释放会造成内存碎片。
后者在释放时直接归还系统，所以每次mmap都会发生缺页异常。在内存工作繁忙时，频繁内存分配会导致大量缺页异常，使内核管理负担增加。
上述两种调用并没有真正分配内存，这些内存只有在首次访问时，才通过缺页异常进入内核中，由内核来分配
回收 内存紧张时，系统通过以下方式来回收内存：
  回收缓存： LRU算法回收最近最少使用的内存页面；
  回收不常访问内存： 把不常用的内存通过交换分区写入磁盘
  杀死进程： OOM内核保护机制 （进程消耗内存越大oom_score越大，占用CPU越多oom_score越小，可以通过/proc手动调整oom_adj）
echo -16 &gt; /proc/$(pidof XXX)/oom_adj   如何查看内存使用情况 free来查看整个系统的内存使用情况
top/ps来查看某个进程的内存使用情况
 VIRT 进程的虚拟内存大小 RES 常驻内存的大小，即进程实际使用的物理内存大小，不包括swap和共享内存 SHR 共享内存大小，与其他进程共享的内存，加载的动态链接库以及程序代码段 %MEM 进程使用物理内存占系统总内存的百分比  怎样理解内存中的Buffer和Cache？ buffer是对磁盘数据的缓存，cache是对文件数据的缓存，它们既会用在读请求也会用在写请求中
如何利用系统缓存优化程序的运行效率 缓存命中率 缓存命中率是指直接通过缓存获取数据的请求次数，占所有请求次数的百分比。命中率越高说明缓存带来的收益越高，应用程序的性能也就越好。
安装bcc包后可以通过cachestat和cachetop来监测缓存的读写命中情况。
安装pcstat后可以查看文件在内存中的缓存大小以及缓存比例
#首先安装Go export GOPATH=~/go export PATH=~/go/bin:$PATH go get golang.org/x/sys/unix go ge github.com/tobert/pcstat/pcstat dd缓存加速 dd if=/dev/sda1 of=file bs=1M count=512 #生产一个512MB的临时文件 echo 3 &gt; /proc/sys/vm/drop_caches #清理缓存 pcstat file #确定刚才生成文件不在系统缓存中，此时cached和percent都是0 cachetop 5 dd if=file of=/dev/null bs=1M #测试文件读取速度 #此时文件读取性能为30+MB/s，查看cachetop结果发现并不是所有的读都落在磁盘上，读缓存命中率只有50%。 dd if=file of=/dev/null bs=1M #重复上述读文件测试 #此时文件读取性能为4+GB/s，读缓存命中率为100% pcstat file #查看文件file的缓存情况，100%全部缓存 O_DIRECT选项绕过系统缓存 cachetop 5 sudo docker run --privileged --name=app -itd feisky/app:io-direct sudo docker logs app #确认案例启动成功 #实验结果表明每读32MB数据都要花0.9s，且cachetop输出中显示1024次缓存全部命中 但是凭感觉可知如果缓存命中读速度不应如此慢，读次数时1024，页大小为4K，五秒的时间内读取了1024*4KB数据，即每秒0.8MB，和结果中32MB相差较大。说明该案例没有充分利用缓存，怀疑系统调用设置了直接I/O标志绕过系统缓存。因此接下来观察系统调用.
strace -p $(pgrep app) #strace 结果可以看到openat打开磁盘分区/dev/sdb1，传入参数为O_RDONLY|O_DIRECT 这就解释了为什么读32MB数据那么慢，直接从磁盘读写肯定远远慢于缓存。找出问题后我们再看案例的源代码发现flags中指定了直接IO标志。删除该选项后重跑，验证性能变化。
内存泄漏，如何定位和处理？ 对应用程序来说，动态内存的分配和回收是核心又复杂的一个逻辑功能模块。管理内存的过程中会发生各种各样的“事故”：
 没正确回收分配的内存，导致了泄漏 访问的是已分配内存边界外的地址，导致程序异常退出  内存的分配与回收 虚拟内存分布从低到高分别是只读段，数据段，堆，内存映射段，栈五部分。其中会导致内存泄漏的是：
 堆： 由应用程序自己来分配和管理，除非程序退出这些堆内存不会被系统自动释放。 内存映射段：包括动态链接库和共享内存，其中共享内存由程序自动分配和管理  内存泄漏的危害比较大，这些忘记释放的内存，不仅应用程序自己不能访问，系统也不能把它们再次分配给其他应用。 内存泄漏不断累积甚至会耗尽系统内存.
如何检测内存泄漏 预先安装systat，docker，bcc
sudo docker run --name=app -itd feisky/app:mem-leak sudo docker logs app vmstat 3 可以看到free在不断下降，buffer和cache基本保持不变。说明系统的内存一致在升高。但并不能说明存在内存泄漏。此时可以通过memleak工具来跟踪系统或进程的内存分配/释放请求
/usr/share/bcc/tools/memleak -a -p $(pidof app) 从memleak输出可以看到，应用在不停地分配内存，并且这些分配的地址并没有被回收。通过调用栈看到是fibonacci函数分配的内存没有释放。定位到源码后查看源码来修复增加内存释放函数即可.
为什么系统的Swap变高 系统内存资源紧张时通过内存回收和OOM杀死进程来解决。其中可回收内存包括：
 缓存/缓冲区，属于可回收资源，在文件管理中通常叫做文件页  被应用程序修改过暂时没写入磁盘的数据(脏页)，要先写入磁盘然后才能内存释放  在应用程序中通过fsync将脏页同步到磁盘 交给系统，内核线程pdflush负责这些脏页的刷新     内存映射获取的文件映射页，也可以被释放掉，下次访问时从文件重新读取  对于程序自动分配的堆内存，也就是我们在内存管理中的匿名页，虽然这些内存不能直接释放，但是Linux提供了Swap机制将不常访问的内存写入到磁盘来释放内存，再次访问时从磁盘读取到内存即可。
Swap原理 Swap本质就是把一块磁盘空间或者一个本地文件当作内存来使用，包括换入和换出两个过程：
 换出： 将进程暂时不用的内存数据存储到磁盘中，并释放这些内存 换入： 进程再次访问内存时，将它们从磁盘读到内存中  Linux如何衡量内存资源是否紧张？
  直接内存回收 新的大块内存分配请求，但剩余内存不足。此时系统会回收一部分内存；
  kswapd0 内核线程定期回收内存。为了衡量内存使用情况，定义了pages_min,pages_low,pages_high三个阈值，并根据其来进行内存的回收操作。
  剩余内存 &lt; pages_min，进程可用内存耗尽了，只有内核才可以分配内存
  pages_min &lt; 剩余内存 &lt; pages_low,内存压力较大，kswapd0执行内存回收，直到剩余内存 &gt; pages_high
  pages_low &lt; 剩余内存 &lt; pages_high，内存有一定压力，但可以满足新内存请求
  剩余内存 &gt; pages_high，说明剩余内存较多，无内存压力
pages_low = pages_min 5 / 4 pages_high = pages_min 3 / 2
    NUMA 与 SWAP 很多情况下系统剩余内存较多，但SWAP依旧升高，这是由于处理器的NUMA架构。
在NUMA架构下多个处理器划分到不同的Node，每个Node都拥有自己的本地内存空间。在分析内存的使用时应该针对每个Node单独分析
numactl --hardware #查看处理器在Node的分布情况，以及每个Node的内存使用情况 内存三个阈值可以通过/proc/zoneinfo来查看，该文件中还包括活跃和非活跃的匿名页/文件页数。
当某个Node内存不足时，系统可以从其他Node寻找空闲资源，也可以从本地内存中回收内存。 通过/proc/sys/vm/zone_raclaim_mode来调整。
 0表示既可以从其他Node寻找空闲资源，也可以从本地回收内存 1，2，4表示只回收本地内存，2表示可以会回脏数据回收内存，4表示可以用Swap方式回收内存。  swappiness 在实际回收过程中Linux根据/proc/sys/vm/swapiness选项来调整使用Swap的积极程度，从0-100，数值越大越积极使用Swap，即更倾向于回收匿名页；数值越小越消极使用Swap，即更倾向于回收文件页。
注意：这只是调整Swap积极程度的权重，即使设置为0，当剩余内存+文件页小于页高阈值时，还是会发生Swap。
Swap升高时如何定位分析 free #首先通过free查看swap使用情况，若swap=0表示未配置Swap #先创建并开启swap fallocate -l 8G /mnt/swapfile chmod 600 /mnt/swapfile mkswap /mnt/swapfile swapon /mnt/swapfile free #再次执行free确保Swap配置成功 dd if=/dev/sda1 of=/dev/null bs=1G count=2048 #模拟大文件读取 sar -r -S 1 #查看内存各个指标变化 -r内存 -S swap #根据结果可以看出，%memused在不断增长，剩余内存kbmemfress不断减少，缓冲区kbbuffers不断增大，由此可知剩余内存不断分配给了缓冲区 #一段时间之后，剩余内存很小，而缓冲区占用了大部分内存。此时Swap使用之间增大，缓冲区和剩余内存只在小范围波动 停下sar命令 cachetop5 #观察缓存 #可以看到dd进程读写只有50%的命中率，未命中数为4w+页，说明正式dd进程导致缓冲区使用升高 watch -d grep -A 15 ‘Normal’ /proc/zoneinfo #观察内存指标变化 #发现升级内存在一个小范围不停的波动，低于页低阈值时会突然增大到一个大于页高阈值的值 说明剩余内存和缓冲区的波动变化正是由于内存回收和缓存再次分配的循环往复。有时候Swap用的多，有时候缓冲区波动更多。此时查看swappiness值为60，是一个相对中和的配置，系统会根据实际运行情况来选去合适的回收类型.
如何“快准狠”找到系统内存存在的问题 内存性能指标 系统内存指标
 已用内存/剩余内存 共享内存 （tmpfs实现） 可用内存： 包括剩余内存和可回收内存 缓存：磁盘读取文件的页缓存，slab分配器中的可回收部分 缓冲区： 原始磁盘块的临时存储，缓存将要写入磁盘的数据  进程内存指标
 虚拟内存： 5大部分 常驻内存： 进程实际使用的物理内存，不包括Swap和共享内存 共享内存： 与其他进程共享的内存，以及动态链接库和程序的代码段 Swap内存： 通过Swap换出到磁盘的内存  缺页异常
 可以直接从物理内存中分配，次缺页异常 需要磁盘IO介入(如Swap)，主缺页异常。 此时内存访问会慢很多  内存性能工具 根据不同的性能指标来找合适的工具:
内存分析工具包含的性能指标:
如何迅速分析内存的性能瓶颈 通常先运行几个覆盖面比较大的性能工具，如free，top，vmstat，pidstat等
 先用free和top查看系统整体内存使用情况 再用vmstat和pidstat，查看一段时间的趋势，从而判断内存问题的类型 最后进行详细分析，比如内存分配分析，缓存/缓冲区分析，具体进程的内存使用分析等  常见的优化思路：
 最好禁止Swap，若必须开启则尽量降低swappiness的值 减少内存的动态分配，如可以用内存池，HugePage等 尽量使用缓存和缓冲区来访问数据。如用堆栈明确声明内存空间来存储需要缓存的数据，或者用Redis外部缓存组件来优化数据的访问 cgroups等方式来限制进程的内存使用情况，确保系统内存不被异常进程耗尽 /proc/pid/oom_adj调整核心应用的oom_score，保证即使内存紧张核心应用也不会被OOM杀死  vmstat使用详解 vmstat命令是最常见的Linux/Unix监控工具，可以展现给定时间间隔的服务器的状态值,包括服务器的CPU使用率，内存使用，虚拟内存交换情况,IO读写情况。可以看到整个机器的CPU,内存,IO的使用情况，而不是单单看到各个进程的CPU使用率和内存使用率(使用场景不一样)。
vmstat 2 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 1379064 282244 11537528 0 0 3 104 0 0 3 0 97 0 0 0 0 0 1372716 282244 11537544 0 0 0 24 4893 8947 1 0 98 0 0 0 0 0 1373404 282248 11537544 0 0 0 96 5105 9278 2 0 98 0 0 0 0 0 1374168 282248 11537556 0 0 0 0 5001 9208 1 0 99 0 0 0 0 0 1376948 282248 11537564 0 0 0 80 5176 9388 2 0 98 0 0 0 0 0 1379356 282256 11537580 0 0 0 202 5474 9519 2 0 98 0 0 1 0 0 1368376 282256 11543696 0 0 0 0 5894 8940 12 0 88 0 0 1 0 0 1371936 282256 11539240 0 0 0 10554 6176 9481 14 1 85 1 0 1 0 0 1366184 282260 11542292 0 0 0 7456 6102 9983 7 1 91 0 0 1 0 0 1353040 282260 11556176 0 0 0 16924 7233 9578 18 1 80 1 0 0 0 0 1359432 282260 11549124 0 0 0 12576 5495 9271 7 0 92 1 0 0 0 0 1361744 282264 11549132 0 0 0 58 8606 15079 4 2 95 0 0 1 0 0 1367120 282264 11549140 0 0 0 2 5716 9205 8 0 92 0 0 0 0 0 1346580 282264 11562644 0 0 0 70 6416 9944 12 0 88 0 0 0 0 0 1359164 282264 11550108 0 0 0 2922 4941 8969 3 0 97 0 0 1 0 0 1353992 282264 11557044 0 0 0 0 6023 8917 15 0 84 0 0 # 结果说明 - r 表示运行队列(就是说多少个进程真的分配到CPU)，我测试的服务器目前CPU比较空闲，没什么程序在跑，当这个值超过了CPU数目，就会出现CPU瓶颈了。这个也和top的负载有关系，一般负载超过了3就比较高，超过了5就高，超过了10就不正常了，服务器的状态很危险。top的负载类似每秒的运行队列。如果运行队列过大，表示你的CPU很繁忙，一般会造成CPU使用率很高。 - b 表示阻塞的进程,这个不多说，进程阻塞，大家懂的。 - swpd 虚拟内存已使用的大小，如果大于0，表示你的机器物理内存不足了，如果不是程序内存泄露的原因，那么你该升级内存了或者把耗内存的任务迁移到其他机器。 - free 空闲的物理内存的大小，我的机器内存总共8G，剩余3415M。 - buff Linux/Unix系统是用来存储，目录里面有什么内容，权限等的缓存，我本机大概占用300多M - cache cache直接用来记忆我们打开的文件,给文件做缓冲，我本机大概占用300多M(这里是Linux/Unix的聪明之处，把空闲的物理内存的一部分拿来做文件和目录的缓存，是为了提高 程序执行的性能，当程序使用内存时，buffer/cached会很快地被使用。) - si 每秒从磁盘读入虚拟内存的大小，如果这个值大于0，表示物理内存不够用或者内存泄露了，要查找耗内存进程解决掉。我的机器内存充裕，一切正常。 - so 每秒虚拟内存写入磁盘的大小，如果这个值大于0，同上。 - bi 块设备每秒接收的块数量，这里的块设备是指系统上所有的磁盘和其他块设备，默认块大小是1024byte，我本机上没什么IO操作，所以一直是0，但是我曾在处理拷贝大量数据(2-3T)的机器上看过可以达到140000/s，磁盘写入速度差不多140M每秒 - bo 块设备每秒发送的块数量，例如我们读取文件，bo就要大于0。bi和bo一般都要接近0，不然就是IO过于频繁，需要调整。 - in 每秒CPU的中断次数，包括时间中断 - cs 每秒上下文切换次数，例如我们调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好，太大了，要考虑调低线程或者进程的数目,例如在apache和nginx这种web服务器中，我们一般做性能测试时会进行几千并发甚至几万并发的测试，选择web服务器的进程可以由进程或者线程的峰值一直下调，压测，直到cs到一个比较小的值，这个进程和线程数就是比较合适的值了。系统调用也是，每次调用系统函数，我们的代码就会进入内核空间，导致上下文切换，这个是很耗资源，也要尽量避免频繁调用系统函数。上下文切换次数过多表示你的CPU大部分浪费在上下文切换，导致CPU干正经事的时间少了，CPU没有充分利用，是不可取的。 - us 用户CPU时间，我曾经在一个做加密解密很频繁的服务器上，可以看到us接近100,r运行队列达到80(机器在做压力测试，性能表现不佳)。 - sy 系统CPU时间，如果太高，表示系统调用时间长，例如是IO操作频繁。 - id 空闲CPU时间，一般来说，id + us + sy = 100,一般我认为id是空闲CPU使用率，us是用户CPU使用率，sy是系统CPU使用率。 - wt 等待IO CPU时间 pidstat 使用详解 pidstat主要用于监控全部或指定进程占用系统资源的情况,如CPU,内存、设备IO、任务切换、线程等。
使用方法：
 pidstat –d interval times 统计各个进程的IO使用情况 pidstat –u interval times 统计各个进程的CPU统计信息 pidstat –r interval times 统计各个进程的内存使用信息 pidstat -w interval times 统计各个进程的上下文切换 p PID 指定PID  1、统计IO使用情况
pidstat -d 1 10 03:02:02 PM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command 03:02:03 PM 0 816 0.00 918.81 0.00 jbd2/vda1-8 03:02:03 PM 0 1007 0.00 3.96 0.00 AliYunDun 03:02:03 PM 997 7326 0.00 1904.95 918.81 java 03:02:03 PM 997 8539 0.00 3.96 0.00 java 03:02:03 PM 0 16066 0.00 35.64 0.00 cmagent 03:02:03 PM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command 03:02:04 PM 0 816 0.00 1924.00 0.00 jbd2/vda1-8 03:02:04 PM 997 7326 0.00 11156.00 1888.00 java 03:02:04 PM 997 8539 0.00 4.00 0.00 java  UID PID kB_rd/s: 每秒进程从磁盘读取的数据量 KB 单位 read from disk each second KB kB_wr/s: 每秒进程向磁盘写的数据量 KB 单位 write to disk each second KB kB_ccwr/s: 每秒进程向磁盘写入，但是被取消的数据量，This may occur when the task truncates some dirty pagecache. iodelay: Block I/O delay, measured in clock ticks Command: 进程名 task name  2、统计CPU使用情况
# 统计CPU pidstat -u 1 10 03:03:33 PM UID PID %usr %system %guest %CPU CPU Command 03:03:34 PM 0 2321 3.96 0.00 0.00 3.96 0 ansible 03:03:34 PM 0 7110 0.00 0.99 0.00 0.99 4 pidstat 03:03:34 PM 997 8539 0.99 0.00 0.00 0.99 5 java 03:03:34 PM 984 15517 0.99 0.00 0.00 0.99 5 java 03:03:34 PM 0 24406 0.99 0.00 0.00 0.99 5 java 03:03:34 PM 0 32158 3.96 0.00 0.00 3.96 2 ansible  UID PID %usr: 进程在用户空间占用 cpu 的百分比 %system: 进程在内核空间占用 CPU 百分比 %guest: 进程在虚拟机占用 CPU 百分比 %wait: 进程等待运行的百分比 %CPU: 进程占用 CPU 百分比 CPU: 处理进程的 CPU 编号 Command: 进程名  3、统计内存使用情况
# 统计内存 pidstat -r 1 10 Average: UID PID minflt/s majflt/s VSZ RSS %MEM Command Average: 0 1 0.20 0.00 191256 3064 0.01 systemd Average: 0 1007 1.30 0.00 143256 22720 0.07 AliYunDun Average: 0 6642 0.10 0.00 6301904 107680 0.33 java Average: 997 7326 10.89 0.00 13468904 8395848 26.04 java Average: 0 7795 348.15 0.00 108376 1233 0.00 pidstat Average: 997 8539 0.50 0.00 8242256 2062228 6.40 java Average: 987 9518 0.20 0.00 6300944 1242924 3.85 java Average: 0 10280 3.70 0.00 807372 8344 0.03 aliyun-service Average: 984 15517 0.40 0.00 6386464 1464572 4.54 java Average: 0 16066 236.46 0.00 2678332 71020 0.22 cmagent Average: 995 20955 0.30 0.00 6312520 1408040 4.37 java Average: 995 20956 0.20 0.00 6093764 1505028 4.67 java Average: 0 23936 0.10 0.00 5302416 110804 0.34 java Average: 0 24406 0.70 0.00 10211672 2361304 7.32 java Average: 0 26870 1.40 0.00 1470212 36084 0.11 promtail  UID PID Minflt/s : 每秒次缺页错误次数 （minor page faults），虚拟内存地址映射成物理内存地址产生的 page fault 次数 Majflt/s : 每秒主缺页错误次数 (major page faults), 虚拟内存地址映射成物理内存地址时，相应 page 在 swap 中 VSZ virtual memory usage : 该进程使用的虚拟内存 KB 单位 RSS : 该进程使用的物理内存 KB 单位 %MEM : 内存使用率 Command : 该进程的命令 task name  4、查看具体进程使用情况
pidstat -T ALL -r -p 20955 1 10 03:12:16 PM UID PID minflt/s majflt/s VSZ RSS %MEM Command 03:12:17 PM 995 20955 0.00 0.00 6312520 1408040 4.37 java 03:12:16 PM UID PID minflt-nr majflt-nr Command 03:12:17 PM 995 20955 0 0 java ]]></content>
  </entry>
  
  <entry>
    <title>一文详解IPv4与IPv6协议</title>
    <url>/post/datacenter/a-detailed-explanation-of-ipv4-and-ipv6-protocols.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>IPv4</tag>
      <tag>IPv6</tag>
    </tags>
    <content type="html"><![CDATA[前段时间的工作大多与通信协议相关，随着协议相关工作的不断深入，相关数据包的分析占据了不少工作时间。
在数据报文分析中，发现大学期间IP协议内容已经重新还给了老师，相关知识完全没有了印象，这篇文章算是一篇复习文，对相关IP协议进行重学习。
 IPv4 协议详解 IPv6 协议详解 IPv4 IPv6 报文对比  IPv4  IPv4 协议简介 IPv4 地址数量 IPv4 协议特点 IPv4 报文结构 IPv4 报文最大长度  IPv4 简介 IPv4（Internet Protocol version 4）是网际协议的第四个修订版本，也是该协议第一个被广泛部署和使用的版本。其在1981年9月由IETF发布的RFC791中被描述，是一种面向无连接的协议，可以在使用分组交换的链路层（如以太网）上运行。在数据传输方面，IPv4协议会尽最大努力交付数据包，但不能保证所有数据包能够成功到达目的地，或者按照正确的顺序到达，这些方面由上层的传输协议（如TCP协议）处理。
IPV4协议报文结构
IPv4 地址数量 IPv4协议使用32位（4字节）地址，其地址空间为4,294,967,296（2^32）个。其中一些地址被保留用于特定用途，如专用网络（约1800万个地址）和多播地址（约2.7亿个地址），这减少了可供互联网路由的地址数量。 随着地址被分配给最终用户，IPv4地址枯竭问题也日益严重。虽然基于分类网络、无类别域间路由和网络地址转换的地址结构重构减缓了地址枯竭的速度，但在2019年11月26日，全球近43亿个IPv4地址已分配完毕。
IPv4地址数量的限制刺激了IPv6的部署，IPv6是唯一的长期解决方案。 IPv6使用128位地址空间，提供了更多的地址，以及更好的安全性和性能。IPv6的广泛部署需要时间和努力，但已经成为解决IPv4地址短缺问题的主要途径。
IPv4 协议特点  面向无连接：  IPv4是一种面向无连接的协议，每个数据包都是独立的，数据包的传输不需要建立和维护连接状态。这使得IPv4的数据包传输速度较快，但同时也增加了数据包传输的可靠性和安全性方面的挑战。
 分组交换：  IPv4协议采用分组交换的技术，将数据分割成一系列小的数据包进行传输，每个数据包都包含了目标地址和源地址等必要的控制信息，这使得数据传输更加高效和灵活。同时，IPv4协议还支持多种传输协议，如TCP、UDP等，可以适应不同的数据传输需求。
 简单、可靠、稳定：  IPv4协议的设计非常简单、可靠、稳定，已经被广泛应用于互联网和局域网等各种网络环境中，具有良好的兼容性和稳定性。
 地址格式：  IPv4地址是一个32位的二进制数，通常用点分十进制表示法来表示，被分为四段，每段可以取0-255之间的整数。IPv4地址的短缺成为了一个问题，因此引入了私有地址和网络地址转换等技术来缓解IPv4地址短缺的问题。
 安全性：  IPv4协议的安全性较低，容易受到各种网络攻击，如IP欺骗、数据包伪造等。因此，为了提高IPv4协议的安全性，通常需要通过路由器、防火墙等网络安全设备来进行加强和保护。
IPv4 报文结构 IPv4报文的最大长度是65,535字节，这个长度是由IP报文中的16位总长度字段决定的，下图为IPv4报文的结构：
IPv4协议首部报文结构
IPV4协议首部报文抓包
  版本(Version)： 占用4比特位，表示IP协议的版本号，IPv4的值为4。
  首部长度(Internet Header Length)： 占用4比特位，表示IP首部的长度，首部长度说明首部有多少32位字(4字节，也就是说单位为4字节)。这个字段的最小值是5(二进制0101)，相当于54=20字节；最大十进制值是15，相当于154=60字节
  服务类型(Type of Service，TOS)： 占用8比特位，表示IP报文的服务类型，用于指定QoS(Quality of Service)和流量控制等参数。
  总长度(Total Length)： 占用16比特位，表示整个IP数据报的长度，包括IP首部和数据部分，单位为字节。这个字段的最小值是20（20字节首部+0字节数据），最大值是2^16-1=65,535。
  标识(Identification)： 占用16比特位，这个字段主要被用来唯一地标识一个报文的所有分片，因为分片不一定按序到达，所以在重组时需要知道分片所属的报文。每产生一个数据报，计数器加1，并赋值给此字段。
  标志(Flags)：占用3比特位，用于标识IP分片的状态。
  ** 位0：保留，必须为0； ** 位1：禁止分片（Don’t Fragment，DF），当DF=0时才允许分片； ** 位2：更多分片（More Fragment，MF），MF=1代表后面还有分片，MF=0 代表已经是最后一个分片。
  分片偏移(Fragment Offset)： 占用13比特位，用于表示分片相对于原始数据报的偏移量。
  生存时间(Time to Live)： 占用8比特位，表示数据报在网络中最多可以被经过的路由器数量，用于防止数据报在网络中无限循环。
  协议(Protocol)： 占用8比特位，表示数据报中的数据部分使用的协议类型，例如TCP、UDP、ICMP等。
  校验和(Header Checksum)： 占用16比特位，用于检测IP头部在传输过程中是否出现了错误。
  源地址(Source Address)： 占用32比特位，表示数据报的发送者IP地址。
  目标地址(Destination Address)： 占用32比特位，表示数据报的接收者IP地址。
  选项(Options) 附加的首部字段可选的跟在目的地址之后，但这并不被经常使用，从1到40个字节不等。如果首部长度大于5，那么选项字段必然存在。
  IPv4 报文长度 IPv4报文的最大长度是65,535字节，这个长度是由IP报文中的16位总长度字段决定的。该字段的最大值是65535，因为它是一个16位无符号整数，所以IP报文的最大长度不能超过该值。
需要注意的是，在实际情况下，IP报文的长度通常会受到网络设备（如路由器、防火墙等）和网络链路的限制，另外，由于网络传输存在MTU(Maximum Transmission Unit)的限制，实际上能够传输的最大数据长度通常不会超过MTU值，一般为1500个字节左右，因此实际传输的IP报文长度可能会比最大长度小得多。
网络传输MTU(Maximum Transmission Unit)大小并不是固定的，它的大小取决于底层网络传输协议和网络设备的配置。不同的网络传输协议和设备可能会有不同的MTU大小限制。
以太网是最常见的网络传输协议之一，其MTU大小通常为1500字节。在以太网上传输的数据包如果超过1500字节，就会被分割成多个小块进行传输。其他网络传输协议的MTU大小可能会有所不同，例如PPP协议的MTU大小通常为1480字节，ATM网络的MTU大小通常为48字节等。
此外，MTU还受到网络设备的配置影响。例如，路由器和交换机等网络设备可以通过配置MTU大小来优化网络传输效率和减少延迟。在实际应用中，为了保证网络传输的稳定性和效率，需要根据具体的网络环境和需求来设置MTU大小，并进行必要的优化和调整。
IPv6  IPv6 协议简介 IPv6 地址数量 IPv6 协议特点 IPv6 报文结构 IPv6 载荷长度  IPv6 简介 IPv6（Internet Protocol version 6）是网际协议的最新版本，主要是为了解决IPv4地址枯竭问题，同时它也在其他方面对于IPv4有许多改进，协议由1998年12月公布的 RFC2960 定义。
IPv6的设计目的是取代IPv4，然而长期以来IPv4在互联网流量中仍占据主要地位，IPv6的使用增长缓慢。在2022年4月，通过IPv6使用Google服务的用户百分率首次超过40%。
虽然IPv6在1994年就已被IETF指定作为IPv4的下一代标准，由于早期的路由器、防火墙及相关应用程序皆须改写，所以在世界范围内使用IPv6部署的网络服务与IPv4相比相对较少，技术上仍以双架构并存居多。
IPv6 地址数量 IPv6地址总长度为128比特位(16字节)，分为8组(每组2个字节)，每组以4个十六进制数形式表示，组间用冒号分隔。例如：FC00:0000:130F:0000:0000:09C0:876A:130B
因为IPv6地址使用128位（16字节）表示，其可以支持约3.4×10²³（2^128）个唯一地址。这个数量比IPv4地址空间（43亿个地址）大得多，可以满足未来数十年互联网的发展需求。IPv6地址空间的巨大规模不仅可以支持更多的设备连接到互联网，而且还可以提供更好的网络安全性和性能。
IPv6 协议特点   更大的地址空间： IPv6地址使用128位长度表示，可以支持约3.4×10²³（2^128）个唯一地址，这个数量比IPv4地址空间（43亿个地址）大得多，可以满足未来数十年互联网的发展需求。
  改进的寻址和路由机制： IPv6协议引入了一些新的寻址和路由机制，包括多播寻址、任播寻址和移动IPv6等，使得网络路由更加高效和灵活。
  简化的头部结构： IPv6协议头部长度固定为40字节，相比于IPv4头部结构更加简化，可以提高网络数据传输效率。
  可选的扩展首部： IPv6定义了许多可选的的扩展首部，不仅可提供比IPv4更多的功能，而且还可以提高路由器的处理效率，因为路由器对逐跳扩展首部外的其他扩展首部都不进行处理。
  更好的安全性和隐私保护： IPv6协议提供了更好的安全性和隐私保护，包括IPsec协议的强制支持、地址隐私扩展等，可以有效地保护网络和用户的隐私。
  更好的流量控制和服务质量： IPv6协议引入了流量控制和服务质量（QoS）机制，可以更好地管理网络流量和提供不同的服务质量，提高用户体验。
  IPv6 报文结构 IPv6数据报首部长度为固定的40字节，在IPv6中所有的扩展首部并不属于IPv6数据报的首部，扩展首部与其后面的数据部分合起来构成有效载荷。
IPv6报文首部与有效载荷
IPv6协议首部报文结构
IPV6协议首部报文抓包
  版本号(Version)： 占用4比特位，用于指示报文使用的IPv6协议版本号，固定为6。
  流量类别(Traffic Class)： 占用8比特位，用于区分不同的IPv6数据报的类别或优先级。。
  流量标签(Flow Label)： 占用20比特位，IPv6提出了流的抽象概念，流就是因特网上从特定源点到特定终点（单播或多播）的一系列IPv6数据报（如实时音视频数据的传送）。所有属于同一个流的IPv6数据报都具有同样的流量标签（相同的流量标签可进行同样的数据优先级设定）。因此，流标号对于实时音视频数据的传送特别有用，对于传统的非实时数据，流标号用处不大。
  负载长度(Payload Length)： 占用16比特位，用于指示IPv6报文中载荷(Payload)的长度，不包括IPv6头部的长度。
  下一个报头(Next Header)： 占用8比特位，用于指示IPv6头部后面的下一个报头类型，如TCP报头、UDP报头、ICMPv6报头等。
  跳数限制(Hop Limit)： 占用8比特位，类似于IPv4中的生存时间(TTL)字段，用于限制报文在网络中经过的最大跳数。
  源地址(Source Address)： 占用128比特位，表示发送端的IPv6地址。
  目标地址(Destination Address)： 占用128比特位，表示接收端的IPv6地址。
  IPv6 载荷长度 关于 IPv6 有效载荷长度：
IPv6报文有效载荷长度主要由Payload Length字段决定，Payload Length字段占用16比特位，用于表示载荷Payload的长度，即除去IPv6报头(固定为40字节)之外的部分。鉴于此字段为16比特，其最大值为2^16 - 1，即65,535字节。
然而，IPv6还支持一种叫做Jumbo Payload的选项。当使用这个选项时，载荷长度可以通过一个名为 Jumbo Payload Option 的扩展报头表示，该扩展报头中有一个32比特(4字节)的字段表示载荷长度。因此，最大载荷长度可以达到2^32 - 1，即4,294,967,295字节。
尽管IPv6有效载荷最大长度可达到4294967295字节，但其承载的传输层协议数据(如TCP、UDP)仍然受到IPv6网络中MTU的限制，因此仍然需要遵循最大报文长度65,535字节的限制。
IPv4、IPv6 报文比较 IPv4报文首部结构
IPv6报文首部与有效载荷
IPv6数据报首部长度为固定的40字节，所有的扩展首部并不属于IPv6数据报的首部，扩展首部与其后面的数据部分合起来构成有效载荷。
由于IPv6地址的长度扩展到了128比特位，使得IPv6数据报基本首部的长度增大到了40字节，比IPv4数据报首部固定部分的长度（20字节）增大了20字节。
其相比于IPv4报文：
取消了首部长度字段：IPv6数据报的首部长度是固定的40字节。
取消了服务类型字段：IPv6数据报首部中的流量类别和流量标签字段实现了区分服务字段的功能。
取消了总长度字段：改用有效载荷长度字段。这是因为IPv6数据报的首部长度是固定的40字节，只有其后面的有效载荷长度是可变的。
取消了标识、标志和片偏移字段：这些功能已包含在IPv6数据报的分片扩展首部中。
把生存时间TTL字段改称为跳数限制字段：这样名称与作用更加一致。
取消了协议字段：改用下一个首部字段。
取消了首部检验和字段：可以加快路由器处理IPv6数据报的速度。
取消了选项字段：改用扩展首部来实现选项功能。
参考 百科IPv4: https://zh.wikipedia.org/wiki/IPv4  
RFC791 IPV4: https://datatracker.ietf.org/doc/html/rfc791  
百科IPv6: https://zh.wikipedia.org/wiki/IPv4  
RFC2460 IPV6: https://datatracker.ietf.org/doc/html/rfc2460  
]]></content>
  </entry>
  
  <entry>
    <title>图文详解PCB术语</title>
    <url>/post/hardware/detailed-explanation-of-PCB-terminology-with-pictures-and-texts.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCB, Gold Plating, Silver Plating</tag>
    </tags>
    <content type="html"><![CDATA[文章将会详细解释PCB的构成，以及在PCB的领域里面常用的一些术语，简要的组装方法，以及简介PCB的设计过程。
What&rsquo;s a PCB? PCB(Printed circuit board)是一个最普遍的叫法，也可以叫做“printed wiring boards” 或者 “printed wiring cards”。在PCB出现之前，电路是通过点到点的接线组成的。这种方法的可靠性很低，因为随着电路的老化，线路的破裂会导致线路节点的断路或者短路。
绕线技术是电路技术的一个重大进步，这种方法通过将小口径线材绕在连接点的柱子上，提升了线路的耐久性以及可更换性。
(1977年Z80计算机的绕线背板)
当电子行业从真空管、继电器发展到硅半导体以及集成电路的时候，电子元器件的尺寸和价格也在下降。电子产品越来越频繁的出现在了消费领域，促使厂商去寻找更小以及性价比更高的方案。于是，PCB诞生了。
Composition(组成) PCB看上去像多层蛋糕或者千层面&ndash;制作中将不同的材料的层，通过热量和粘合剂压制到一起。
从中间层开始吧。
FR4 PCB的基材一般都是玻璃纤维。大多数情况下，PCB的玻璃纤维基材一般就指&quot;FR4&quot;这种材料。&ldquo;FR4&quot;这种固体材料给予了PCB硬度和厚度。除了FR4这种基材外，还有柔性高温塑料(聚酰亚胺或类似)上生产的柔性电路板等等。
你可能会发现有不同厚度的PCB;然而 SparkFun的产品的厚度大部分都是1.6mm(0.063'')。有一些产品也采用了其它厚度，比如 LilyPad、Arudino Pro Micro boards采用了0.8mm的板厚。
廉价的PCB和洞洞板(见上图)是由环氧树脂或酚这样的材料制成，缺乏 FR4那种耐用性，但是却便宜很多。当在这种板子上焊接东西时，将会闻到很大的异味。这种类型的基材，常常被用在很低端的消费品里面。酚类物质具有较低的热分解温度，焊接时间过长会导致其分解碳化，并且散发出难闻的味道。
Copper (露铜的PCB，无阻焊&amp;丝印)
接下来介绍是很薄的铜箔层，生产中通过热量以及黏合剂将其压制倒基材上面。在双面板上，铜箔会压制到基材的正反两面。在一些低成本的场合，可能只会在基材的一面压制铜箔。当我们提及到”双面板“或者”两层板“的时候，指的是我们的千层面上有两层铜箔。当然，不同的PCB设计中，铜箔层的数量可能是1层这么少，或者比16层还多。
铜层的厚度种类比较多，而且是用重量做单位的，一般采用铜均匀的覆盖一平方英尺的重量(盎司oz)来表示。大部分PCB的铜厚是1oz，但是有一些大功率的PCB可能会用到2oz或者3oz的铜厚。将盎司(oz)每平方英尺换算一下，大概是 35um或者1.4mil的铜厚。
Soldermask(阻焊) 在铜层上面的是阻焊层。这一层让PCB看起来是绿色的(或者是SparkFun的红色)。阻焊层覆盖住铜层上面的走线，防止PCB上的走线和其他的金属、焊锡或者其它的导电物体接触导致短路。阻焊层的存在，使大家可以在正确的地方进行焊接 ，并且防止了焊锡搭桥。
在上图这个例子里，我们可以看到阻焊覆盖了PCB的大部分(包括走线)，但是露出了银色的孔环以及SMD焊盘，以方便焊接。
一般来说，阻焊都是绿色的，但几乎所有的颜色可以用来做阻焊。SparkFun的板卡大部分是红色的，但是IOIO板卡用了白色，LilyPad板卡是紫色的。
Silkscreen(丝印) 在阻焊层上面，是白色的丝印层。在PCB的丝印层上印有字母、数字以及符号，这样可以方便组装以及指导大家更好地理解板卡的设计。我们经常会用丝印层的符号标示某些管脚或者LED的功能等。
丝印层是最最常见的颜色是白色，同样，丝印层几乎可以做成任何颜色。黑色，灰色，红色甚至是黄色的丝印层并不少见。然而，很少见到单个板卡上有多种丝印层颜色。
Terminology(术语) 现在你知道了PCB的结构组成，下面我们来看一下PCB相关的术语吧。 孔环 &ndash; PCB上的金属化孔上的铜环。
Examples of annular rings. 孔环的例子
DRC &ndash; 设计规则检查。一个检查设计是否包含错误的程序，比如，走线短路，走线太细，或者钻孔太小。
钻孔命中 &ndash; 用来表示设计中要求的钻孔位置和实际的钻孔位置的偏差。钝钻头导致的不正确的钻孔中心是PCB制造里的普遍问题。
不是太准确的drill hit示意图
(金)手指 &ndash; 在板卡边上裸露的金属焊盘，一般用做连接两个电路板。比如计算机的扩展模块的边缘、内存条以及老的游戏卡。
邮票孔 &ndash; 除了V-Cut外，另一种可选择的分板设计方法。用一些连续的孔形成一个薄弱的连接点，就可以容易将板卡从拼版上分割出来。SparkFun的Protosnap板卡是一个比较好的例子。
ProtoSnap上的邮票孔使PCB能简单的弯折下来。
焊盘 &ndash; 在PCB表面裸露的一部分金属，用来焊接器件。
上边是 插件焊盘，下边是贴片焊盘
拼板 &ndash; 一个由很多可分割的小电路板组成的大电路板。自动化的电路板生产设备在生产小板卡的时候经常会出问题，将几个小板卡组合到一起，可以加快生产速度。
钢网 &ndash; 一个薄金属模板(也可以是塑料)，在组装的时候，将其放在PCB上让焊锡透过某些特定部位。
钢网(原图挂了，自己找的配图)
Pick-and-place - 将元器件放到线路板上的机器或者流程。
平面 &ndash; 线路板上一段连续的铜皮。一般是由边界来定义，而不是 路径。也称作”覆铜“
图示PCB上大部分地方没有走线，但是有地的覆铜
金属化过孔 &ndash; PCB上的一个孔，包含孔环以及电镀的孔壁。金属化过孔可能是一个插件的连接点，信号的换层处，或者是一个安装孔。
FABFM PCB上的一个插件电阻。电阻的两个腿已经穿过了PCB的过孔。电镀的孔壁可以使PCB正反两面的走线连接到一起。
Pogo pin &ndash; 一个弹簧支撑的临时接触点，一般用作测试或烧录程序。
有尖头的pogo pin, 在测试针床中用的很多。
回流焊 &ndash; 将焊锡融化，使焊盘(SMD)和器件管脚连接到一起。
丝印 &ndash; 在PCB板上的字母、数字、符号或者图形等。基本上每个板卡上只有一种颜色，并且分辨率相对比较低。
丝印指出了这个LED是电源指示灯。
开槽 &ndash; 指的是PCB上任何不是圆形的洞。开槽可以电镀也可以不电镀。由于开槽需要额外的切割时间，有时会增加板卡的成本。
在ProtoSnap - Pro Mini板卡上的复杂开槽。同样有很多邮票孔。注意: 由于开槽的刀具是圆形的，开槽的边缘不能完全做成直角。
锡膏层 &ndash; 在往PCB上放置元器件之前，会通过钢网在表贴器件的焊盘上形成的一定厚度的锡膏层。在回流焊过程中，锡膏融化，在焊盘和器件管脚间建立可靠的电气和机械连接。
在放置元器件之前，PCB上短暂的锡膏层，记得去了解一下钢网的定义。
焊锡炉 &ndash; 焊接插件的炉子。一般里面有少量的熔融的焊锡，板卡在上面迅速的通过，就可以将暴露的管脚上锡焊接好。
阻焊 &ndash; 为了防止短路、腐蚀以及其它问题，铜上面会覆盖一层保护膜。保护膜一般是绿色，也可能是其它颜色(SparkFun红色，Arduino蓝色，或者Apple黑色)。一般称作“阻焊”。
Solder mask covers up the signal traces but leaves the pads to solder to.阻焊覆盖了信号线，但是露出了焊盘以便于焊接。
连锡 &ndash; 器件上的两个相连的管脚，被一小滴焊锡错误的连接到了一起。
表面贴装 &ndash; 一种组装的方法，器件只需要简单的放在板卡上，不需要将器件管脚穿过板卡上的过孔。
热焊盘 &ndash; 指的是连接焊盘到平面间的一段短走线。如果焊盘没有做恰当的散热设计，焊接时很难将焊盘加热到足够的焊接温度。不恰当的散热焊盘设计，会感觉焊盘比较黏，并且回流焊的时间相对比较长。(译者注，一般热焊盘做在插件与波峰焊接触的一面。不知道这个文章里面为什么会提到reflow，reflow主要要考虑的是热平衡，防止立碑。)
在左边，焊盘通过两个短走线(热焊盘)连接到地平面。在右边，过孔直接连接到地平面，没有采用热焊盘。
走线 &ndash; 在电路板上，一般连续的铜的路径。
一段连接复位点和板卡上其它地方的细走线。一个相对粗一点的走线连接了5V电源点。
V-score &ndash; 将板卡进行一条不完全的切割，可以将板卡通过这条直线折断。(译者注：国内常叫做“V-CUT”)
过孔 &ndash; 在板卡上的一个洞，一般用来将信号从一层切换到另外一层。塞孔指的是在过孔上覆盖阻焊，以防被焊接。连接器或者器件管脚过孔，因为需要焊接，一般不会进行塞孔。
同一个PCB上塞孔的正反两面。这个过孔将正面的信号，通过在板卡上的钻孔，传输到了背面。
波峰焊 &ndash; 一个焊接插件器件的方法。将板卡匀速的通过一个产生稳定波峰的熔融焊锡炉，焊锡的波峰会将器件管脚和暴露的焊盘焊接到一起。
简要的介绍一下如何设计自己的PCB板卡。
Designing your own! 设计自己的! 你希望开始设计自己的PCB吗。在PCB设计中的曲曲弯弯在这边说太复杂了。不过，如果你真的想开始，下面有几个要点。
 找到一个CAD的工具：在PCB设计的市场里，有很多低价或者免费的选择。当找一个工具的时候，可以考虑以下几点。 论坛支持：有没有很多人使用这个工具?越多的人使用，你越容易找到你需要的器件的已经设计好的封装库。 很容易用。如果不好用的话，你也不会用。 性能：很多程序对设计有限制，比如层数，器件数，以及板卡尺寸等。大部分需要你去购买授权去升级性能。 可抑制性：一些免费的程序不允许导出或者迁移到其它软件，将你限制在唯一的供应商上。可能软件的低价以及便捷性值得这样的付出，但有时候不太值得。 去看看其他人的布板设计。开源硬件让这个事情越来越容易。 练习，练习，还是练习。 保持低的期望值。你设计的第一个板卡可能有很多问题，但是第20个可能就少很多，但是还会有一些问题。但是你很难将所有问题清除。 原理图相当重要。尝试去设计一个没有好的原理图支持的PCB板卡是徒劳的。 ]]></content>
  </entry>
  
  <entry>
    <title>芯片行业最大的收购，英国批准了</title>
    <url>/post/news/UK-has-approved-broadcom-69-billion-takeover-of-VMware.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Broadcom</tag>
      <tag>VMware</tag>
    </tags>
    <content type="html"><![CDATA[据路透社报道，英国竞争监管机构周三暂时批准了美国科技公司博通以690亿美元收购VMware 的交易，称该交易不会削弱关键计算机服务器产品供应的竞争。
竞争与市场管理局 (CMA) 临时批准了博通有史以来规模最大的收购，此前博通向一些竞争对手提供了互操作性补救措施以解决问题，随后欧盟监管机构上周批准了此次收购。
CMA 表示：“在审查了从 Broadcom、VMware 和其他相关方收集的证据后，独立的 CMA 小组暂时发现该交易不会大幅减少英国服务器硬件组件供应的竞争。”
拟议中的交易凸显了芯片制造商博通向企业软件领域多元化发展的目标，但与此同时，全球监管机构加大了对大型科技公司交易的审查力度。
英国监管机构曾在 3 月份表示担心该交易可能会使服务器变得更加昂贵，并表示现在将就其临时批准进行咨询，然后在 9 月 12 日之前发布最终报告。
博通对无条件批准表示欢迎，并表示预计将在本财年完成交易。
CMA 表示，它探讨了这样的担忧：如果合并后的博通公司的产品与 VMware 的服务器虚拟化软件的配合效果较差，该交易可能会损害博通竞争对手的竞争能力。
“(CMA) 小组认为，该交易不太可能损害创新，特别是因为有关新产品调整的信息只需要在为时已晚而无法为博通带来商业利益的阶段与 VMware 共享。”
这笔 690 亿美元的交易包括 610 亿美元的股本和其余的债务，也正在接受美国联邦贸易委员会的审查。
CMA 今年成为第一个阻止微软收购《使命召唤》制造商动视暴雪交易的主要监管机构，但此后表示可能会再次考虑修改后的提议。目前，有关各方正在努力解决争端。
全球最大的半导体收购案，取得重要进展 据彭博社报道，博通公司以 610 亿美元收购云计算公司VMware Inc.的交易即将获得欧盟合并官员的批准，这为全球有史以来最大的半导体制造商收购案铺平了道路。
据不愿透露姓名的知情人士透露，在与这些公司进行数月谈判后，欧盟委员会最快将于周三表示同意。在会谈期间，博通签署了行为补救措施，包括承诺将互操作性标准引入其技术，以允许竞争对手更公平地竞争。
金融时报指出，四位知情人士表示，欧盟执行机构欧盟委员会将于周三表示，它已接受博通的让步，即 VMware 的软件将继续与竞争对手的硬件兼容。这些人士表示，事实证明，这一措施足以解决欧盟竞争主管机构的担忧，而博通无需出售部分 VMware 业务。
在此前，欧盟调查芯片及设备制造商博通（Broadcom）和VMware的610亿美元收购案，按照欧盟当初发布的初步结果显示，收购案可能造成博通网络及存储方面竞争对手无法取得VMware软件，或是硬件与VMware无法兼容。
在博通宣布以610亿买下VMware，并预计自身公司里包含基础设施及安全软件的部门也会被更名为VMware。但消息公布后引起欧盟关注，一些VMware用户也根据博通过去收购CA及赛门铁克的黑历史，担心未来可能会被迫购买博通硬件。
欧盟在去年接到收购案的通报，欧盟委员会展开调查，初步调查结果显示，这桩交易可能允许博通在网卡和存储方面市场限制竞争。委员会认为，这可能降低VMware的服务器虚拟化软件和博通竞争者硬件的兼容性，或转向有利于博通硬件，或不允许竞争对手硬件使用VMware软件，或减少其获取渠道，以排斥竞争者。而上述情形将造成解决方案的价格上升、品质下降、影响企业客户的创新，最终伤害消费者。
此外，欧盟委员会也将调查博通收购VMware是否会影响其他厂商开发SmartNIC（智能网卡）。2020年，VMware和三家SmartNIC厂商（英伟达、英特尔及AMD）发起Project Monterey。
欧盟当初表示，一旦博通买下VMware，可能限制VMware参与Project Monterey来保护自身网卡营收，阻止其进行技术创新。此外欧盟也会考量，博通是否开始将VMware虚拟化软件和自家软件（主要是大型主机及安全软件）的綑绑销售，而不再单独销售VMware，造成市场选择减少，且排挤竞争软件供应商。
报道表示，尽管布鲁塞尔监管机构将与加拿大、巴西和南非监管机构一起批准该交易，但此次收购仍面临美国、英国和中国的竞争调查。
该交易早些时候曾面临严格审查，委员会在 4 月份强调了阻止该交易的潜在原因，除非有足够的补救措施。该公司警告称，此次交易可能会导致企业客户“价格上涨、质量下降、创新减少”。
英国竞争与市场管理局将于本月晚些时候公布博通创纪录交易的临时调查结果，法定截止日期为 9 月 12 日 。英国监管机构发现自己在科技交易审查方面越来越孤立，对企业未来的行为承诺采取了更强硬的立场，例如在对微软公司以 690 亿美元收购动视暴雪公司的调查中。
博通不再是半导体公司！ Broadcom 的存在是从一个衍生产品衍生出来的。大约二十年前，惠普开始了它的小型化进程。首先分拆安捷伦，其中包含与 PC 或打印机无关的大杂烩业务。安捷伦又将自己拆分成几个部分，其中之一是惠普曾经的内部芯片业务，重新命名为 Avago。作为卖方分析师，我们多年来一直密切关注 Avago。深埋在惠普内部，我们知道它销售的过滤器可以进入手机，偶尔会提供一些非常有趣但晦涩的信息。然后分拆发生了。
Avago 通过私募股权获得了生命，就起源故事而言，这是接下来会发生什么的关键线索。Avago 首席执行官 Hock Tan 很早就意识到了大多数其他半导体首席执行官没有意识到的事情——半导体已经不再是一个增长型行业。
80 年代和 90 年代的繁荣时期已经结束，现在半决赛竞争激烈，周期性严重，利润率低。因此，Avago 开始疯狂收购，收购的公司数不胜数，并在 15 年里推动股价上涨 3,000%。
这一成功的秘诀是一本相当简单的剧本。收购在市场上处于领先地位而竞争对手很少的公司。然后剥离出售给竞争行业的细分市场，削减管理和公司管理费用，并推动现金流——这为下一次收购提供了更多的杠杆和火力。并重复。
该公司在这方面取得了巨大的成功，并有效地促进了美国半导体行业的彻底整合，该行业从 20 年前的 2000 家公司增加到今天的 200 家左右。
对于被收购的公司来说，整合过程是支撑。新的管理团队将消除他们能找到的所有费用——不再有公司的纪念品，不再有免费的咖啡，众所周知的是没有企业 IT 部门。对于从裁员中幸存下来的初级经理来说，这太棒了。他们被赋予了自主权，消除了官僚主义，丰富的选择包和繁重的工作量。
随着时间的推移，另一个趋势也变得明显——公司将大幅降低研发，我们将在下面回到这个话题。Avago 的另一项重要技能是其 CEO 和交易团队成为专家，并找到能够说服目标董事会出售非财务工具。有时这意味着离任高管的退休协议，创始人的职位和头衔，或公司名称的保留。因此，当他们在 2015 年收购博通时，Avago 更改了名称，因为这是完成交易的必要条件。
故事是这样的，当亚历山大大帝到达印度河时，他抽泣着，感叹没有更多的土地可以征服。由于美国政府CFIUS 最后一刻的模糊命令，Broadcom 于 2019 年未能收购高通，从而进入了印度河。像所有其他成功的汇总故事一样，博通需要不断收购更大的业务以保持机器运转。到 2019 年，实际上只有两家大到足以推动博通前进的公司——高通和英特尔。高通已经失败了，而英特尔（当时）太大了，无法考虑。
于是博通转向了软件公司。这个领域有很多大目标。这些公司不一定像博通的半导体目标那样在市场上占据主导地位，但它们确实与那些被锁定在长期合同和多重 IT 依赖关系的客户之间有着非常密切的关系。这意味着稳定的现金流。
事实上，博通不是一家半导体公司。它也不是一家软件公司。它是一只私募股权基金，通过无休止的一系列收购将现金流最大化。这让半导体行业的许多人感到沮丧，并且可能让软件行业的人感到困惑。对于现在必须掌握 SaaS 指标的卖方半成品分析师来说，这无疑是一个挑战。但对于股东来说，它仍然是一个引人注目的模式。
多年来，我们了解到博通的一件事是，他们一直在寻找新的交易。所以我们可以根据经验说，在 VMWare 收购结束的那一天，银行家会接到电话询问“下一步是什么？”
这种情况能持续多久？汇总有几个大问题。一是我们上面提到的对新交易的不断追求。二是融合。在某些时候，这些组织变得如此庞大，以至于他们开始绊倒自己。博通通过将如此多的自主权下放到各个部门，在很大程度上避免了这个问题，但在某些时候，这不得不陷入困境。尤其是当一切都在一家上市公司的保护伞下时。
另一个问题是整个模型依赖于来自基础业务的稳定现金流。这就是私募股权公司长期避开技术的原因，这些公司存在技术风险，而私募股权基金通常青睐的更传统的公司不一定存在技术风险。这就是我们关注博通半导体业务研发的原因。今天没有明显的缺陷——它们在网络领域仍然非常强大，并且是围绕手机 BAW 射频滤波器双头垄断的主导方。
话虽如此，我们越来越多地从网络业务人员那里听到，他们对博通的发展速度感到失望。新的芯片和功能需要越来越长的时间才能到达，这为超大规模企业中的初创企业和内部解决方案打开了大门。在射频领域，至少部分 BAW 滤波器市场存在技术中断的真正可能性，高通和村田似乎对此有强烈的感觉。
诚然，博通的团队非常精明。但如果他们的任何业务开始出现见顶迹象，他们会毫不犹豫地出售或分拆。当然，大多数买家可能都知道这一点，到目前为止，除了偶尔在合并后出售不受欢迎的业务线之外，博通还没有从其投资组合中退出一次。
就目前的情况来看，博通可能还能让这种模式持续很长时间。那里有大量的软件目标，而且本月它们都变得更便宜了。我们最好的猜测是，唯一能让博通放慢脚步的是高管团队的能量水平。Hock Tan 现在已经 70 岁左右，我们猜测他近期没有兴趣退休在纳帕谷种植葡萄酒。
]]></content>
  </entry>
  
  <entry>
    <title>GPU和CPU芯片谁更复杂</title>
    <url>/post/datacenter/which-is-more-complex-GPU-or-CPU-chip.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>CPU</tag>
      <tag>GPU</tag>
    </tags>
    <content type="html"><![CDATA[高端的GPU，如NVIDIA的A100或AMD的Radeon Instinct MI100，包含了大量的CUDA核心或流处理器，以支持大规模并行计算。
高端的CPU，如Intel的Xeon系列或AMD的EPYC系列，通常具有更多的核心、更高的频率和更复杂的超线程技术，以提高性能。
那么GPU和CPU到底哪个更复杂呢？我们用应用场景、晶体管数量、架构设计几个维度来看看。
应用场景 GPU具有大量的计算核心、专用的存储器和高速数据传输通道。GPU的设计注重于大规模并行计算、内存访问和图形数据流处理等方面，以满足图形渲染和计算密集型应用的要求。
GPU 的核心设计理念是并行处理。相比于 CPU，GPU 拥有更多的处理单元，因此，它可以同时执行大量的并行任务。这使得 GPU 特别适合处理可以并行化的工作负载，如图形渲染、科学计算和深度学习等。
CPU则专注于通用计算和广泛的应用领域。这些CPU通常具有多个处理核心、高速缓存层次和复杂的指令集体系结构。
晶体管数量 从晶体管的数量来看，顶级的GPU通常包含更多的晶体管，这主要是因为它们需要大量的并行处理单元。例如，
CPU: NVIDIA A100 GPU  包含了540亿个晶体管。
CPU: AMD的EPYC 7742，包含约390亿个晶体管。
架构设计 从架构和设计的角度来看，CPU可能会被认为更复杂。CPU需要处理各种各样的任务，并且需要优化以尽可能快地执行这些任务。为了达到这个目标，CPU使用了许多复杂的技术，如流水线、乱序执行、分支预测、超线程等。
顶级的GPU可能在硬件规模（例如，晶体管数量）上更大，而顶级的CPU在架构和设计上可能更复杂。
GPU架构
GPU 的一些关键架构特性：
 大量的并行处理单元（核心）：GPU 中的每一个处理单元可以被看作是一个微型的 CPU，它们可以同时执行指令。例如，NVIDIA 的一种 GPU 架构，叫做 Turing，有数千个并行处理单元（被称为 CUDA 核心）。 分层的内存架构：GPU 有一个复杂的内存架构，包括全局内存、共享内存、本地内存和常量内存等。全局内存可以被所有核心访问，而其他类型的内存则用于缓存数据，以减少对全局内存的访问延迟。 线程调度和执行：GPU 使用硬件进行线程调度，这使得它可以在执行大量线程时保持高效率。在 NVIDIA 的 GPU 中，线程是以 warp （32个线程）的形式进行调度和执行的。 特殊功能单元：除了标准的计算核心外，GPU 还有一些特殊的功能单元，如纹理单元和光栅化单元，这些都是为图形渲染特化的。在最新的 GPU 中，还有一些专门为深度学习和人工智能设计的单元，如张量核心和RT核心。  流多处理器和 SIMD 架构：GPU 使用了 SIMD（单指令多数据流）架构，这意味着在一个时钟周期内，一条指令可以在多个数据上并行执行。在 NVIDIA 的 GPU 中，每个流多处理器（SM）包含了数百个 CUDA 核心，以及其他资源如寄存器、缓存和功能单元。
具体的 GPU 架构设计会根据制造商和产品线的不同而有所不同。例如，NVIDIA 的架构（如 Turing 和 Ampere）和 AMD 的架构（如 RDNA）有一些关键的差异。然而，所有的 GPU 架构都遵循并行处理的基本理念。
CPU架构 CPU（中央处理单元）的架构设计涉及众多领域，包括硬件设计、微体系结构、指令集设计等等。
 指令集架构（ISA）：这是 CPU 的基础，定义了 CPU 可以执行哪些操作（例如，加法、乘法、逻辑操作等），以及如何编码这些操作。常见的 ISA 包括 x86（Intel 和 AMD）、ARM、RISC-V 等。 流水线：在现代的 CPU 中，指令被分解为多个阶段，例如，取指、译码、执行、访存和写回。这些阶段被组织成一个流水线，这样每个时钟周期内，可以有多个指令在不同阶段同时进行，从而提高了指令的吞吐量。 缓存和内存层次结构：为了减少访问内存的延迟，CPU 包含了一套复杂的缓存系统。这通常包括 L1、L2、L3 缓存等多个级别。除此之外，还有 TLB（转译后援缓冲器）等机制来加速虚拟地址到物理地址的转换。 乱序执行和寄存器重命名：这些是现代 CPU 的关键优化手段。乱序执行允许 CPU 在等待某些慢指令（如内存访问）完成时，先执行其他无关的指令。寄存器重命名则是解决数据冒险的一种方法，它允许 CPU 重新排列指令的执行顺序，而不会影响最后的结果。 分支预测：分支预测是一种优化方法，用于预测条件跳转指令的结果。如果预测正确，CPU 可以提前取指和执行后续的指令，从而避免了因为等待跳转结果而产生的停顿。 多核和多线程：现代的 CPU 通常包含多个处理核心，每个核心都可以独立执行指令。此外，一些 CPU 还支持多线程技术（如 Intel 的超线程），可以让一个核心同时执行多个线程，从而提高了核心的利用率。  以上只是 CPU 架构设计的一部分。实际上，CPU 的设计是一个极其复杂的过程，需要考虑的因素非常多，包括性能、能耗、面积、成本、可靠性等等。
]]></content>
  </entry>
  
  <entry>
    <title>AMD发布了最新款AI芯片Instinct MI300</title>
    <url>/post/datacenter/instinct-MI300-announced-by-amd.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>AMD</tag>
      <tag>MI300</tag>
      <tag>GPU</tag>
      <tag>CPU</tag>
    </tags>
    <content type="html"><![CDATA[今年6月，AMD发布了一款专门针对AI需求的最新款芯片：Instinct MI300。
MI300将CPU、GPU和内存封装在了一起，晶体管数量高达1460亿个，接近英伟达H100的两倍。其搭载的HBM（高带宽内存）密度也达到了H100的2.4倍。也就是说，MI300在理论上可以运行比H100更大的AI模型。
受益于AI训练的增长，GPU需求肉眼可见的从游戏市场向高性能计算领域倾斜，就连刚开启GPU产品线的英特尔，也迫不及待的PPT首发了面向高性能计算场景的Falcon Shores架构芯片。
伴随英伟达一路冲向万亿美元市值，资本市场对GPU行业老二的期待值也达到了顶峰。今年以来，AMD股价累计上涨已经超过90%。
英特尔的Falcon Shores，预计2024年推出
然而MI300发布会结束，AMD股价下跌3.6%，反倒是英伟达上涨3.9%。资本市场表达好恶，向来是这么冷酷无情。
原因可能在于，AMD没有在发布会中透露这款芯片的客户，这也是市场对英伟达以外的AI芯片最大的担心。
长期以来，AMD在GPU市场一直被英伟达按在地上反复摩擦，Instinct产品线其实已经迭代了好几年，但相比英伟达的连战连捷，AMD在高性能计算领域的存在感一直比较稀薄。
AI训练打开的市场空间，一度被市场视为AMD与英伟达拉进距离的机会，但事情似乎没那么简单。
离不开CPU，但离得开英特尔 虽说在AI训练上，更擅长大规模并行计算的GPU承担了大部分计算工作，而整个系统仍需要CPU进行调度和统筹。也就是说，尽管GPU的需求量大幅度增加，但CPU仍是必需品。
作为一家同时拥有CPU和GPU设计能力的芯片公司，AMD被看好也不意外。更何况过去几年，AMD在CPU市场连战连捷。
AMD现任CEO苏姿丰在2014年接手，时值推土机架构性能孱弱，让英特尔心安理得的挤牙膏。而在卖掉Imageon后，AMD和拒绝为苹果设计iPhone芯片的英特尔一起，完美错过了智能手机的浪潮，公司一片风雨飘摇。
面临多条战线的失血，苏姿丰只能将有限的资源集中在核心的CPU业务上，从苹果请回了架构大师吉姆·凯勒，开始Zen架构处理器的研发。
2017年，Zen架构处理器横空出世，把挤牙膏上瘾的英特尔打了个措手不及。2019年，Zen处理器更换为台积电7nm工艺，此时英特尔10nm工艺姗姗来迟。
虽然英特尔还占据着大部分市场份额，但AMD的反攻速度实在太快，尤其是在服务器市场，几乎是从0杀到了接近20%的市占率。
2023年Q1，AMD的x86处理器市场份额达到了34.6%这一历史峰值[2]，这也是AMD市值超过英特尔的重要背景。
今年5月，全球超级计算机Top500强公布：前500强中，使用AMD CPU进行驱动的超算达到121台，使用英特尔CPU的超算则从2016年的454台下降至360台，虽然看着不少，但其中很多是英特尔10年前的家底——至强（Xeon）处理器[3]。
但同一时期，AMD与英伟达差距也越来越大。
难以逾越的CUDA 英伟达不仅是一流的硬件公司，更是一流的软件公司。
虽然在理论性能上，MI300的一些参数甚至领先于英伟达，但市场对英伟达对手们最大的担心往往在于，就算硬件性能可以跟英伟达比肩，但是软件解决方案仍难以与英伟达的CUDA对抗。
2006年，英伟达推出了CUDA平台，让开发者能够给予GPU进行编程和开发，最终形成了一个庞大稳固的生态。在推出CUDA之前，全球能用GPU进行编程的不足100人，目前CUDA的使用者超过400万。
每一个成功的硬件公司背后，往往都有一个更强大的软件团队，苹果和英伟达都是如此。即便是光刻机制造商ASML也不例外，他们的官方网站上有这样一段话：
您可能将ASML视为一家硬件公司，但实际上我们拥有世界上最大、最具开创性的软件社区之一。如果没有我们开发的软件，我们的客户就不可能制造出10纳米或更小的尺寸的芯片。
想要芯片真正在具体场景的满足各种需求，就需要开发者对硬件进行编程以实现各种功能。如果说硬件编程的过程相当于进行各种复杂计算，那么CUDA就是提供给使用者的一部计算器。
无论对英伟达的刀法多么怀恨在心，都不能否认黄仁勋对通用计算和人工智能的超前押注。
AMD显然深知软件和生态的重要性，但对标CUDA的ROCm在2016年推出时，就已经比英伟达晚了十年。
直到2023年4月，ROCm都仅支持Linux平台；而CUDA自问世以来，就提供Windows和Linux两个版本，后期还为苹果用户增设Mac OS版本。
相比英伟达不遗余力的推广和洗脑，AMD在生态建设上也显得投入不足，据说早年英伟达对项目的GPU试用申请几乎是有求必应，动不动就去高校实验室发显卡。深度学习大神杰夫·辛顿带着学生训练AlexNet模型，就用了三块GTX 580。
另外，AMD的软件能力也令人不安——AMD在今年6月发布了一份EPYC 7002 “Rome”服务器芯片指南，承认由于时钟倒计时器存在 BUG，导致第二代EPYC芯片运行1044天后，会出现内核卡死。如果有服务器使用这款芯片，需要每隔2.93年重新启动一次。
原因也不难理解，直到推出ROCm的2016年，AMD甚至还没摆脱亏损。在这期间，AMD只能把有限的资源都聚焦在CPU的研发上，无法为GPU部门投入太多资源，更不要说ROCm的软件团队了。
而当AMD在CPU市场收复失地，希望依靠AI卷土重来时，英伟达已经慢慢补齐了短板。
英伟达的反攻 2020年9月，英伟达宣布准备以400亿美元的价格准备收购移动CPU架构商Arm，其背后意图人尽皆知：一方面是整合移动端资源，另一方面则是入局CPU。
正如前文所说，尽管AI时代需要更多的GPU，但CPU仍不可或缺。当CPU与GPU共同在服务器中的工作时，实际场景更像是一个大学生（CPU）带领一群小学生（GPU）组队完成各种任务。这个时候，配合就显得尤为重要。
因此，英伟达之所以自己做CPU，并非完全因为英特尔或AMD，而是从自身产品需求出发，使CPU和GPU紧密耦合，以发挥最大性能。比如CPU和GPU中，需要用到尽可能相似技术的一致内存，以保证数据之间的无缝共享[8]。
虽然收购基本没有成功的可能性，但英伟达依然按部就班的招兵买马。2021年4月，黄仁勋在自家厨房里宣布，英伟达即将推出首款5nm制程工艺CPU Grace，基于Arm架构，面向超大型 AI 模型的和高性能计算。
紧接着就是具体工作的有序展开：英伟达首先选定了根据地以色列，那里有全球第三多的纳斯达克上市公司（仅次于美国和中国）；然后对外招聘600名硬件工程师、软件工程师和芯片设计师，搭建CPU研发团队[7]。
最后，英伟达挖来了英特尔在以色列的CPU架构专家Rafi Marom，后者曾参与10nm制程的Tiger Lake和Alder lake芯片开发工作。
在2022年3月的GTC大会上，英伟达对外宣布Grace CPU性能：拥有144个Arm内核和1TB/s的内存带宽，性能较当前最先进的DGX A100搭载的双CPU相比高1.5倍以上。
不过，原本预计在今年上半年可以开始供货的Grace芯片，目前已推迟至下半年。
APU Instinct MI300本质上是一颗“APU”，这是AMD早在2009年提出的一个概念——将CPU和GPU集成在一起，使得二者高速互联，实现1+1&gt;2的效果。
在2006年收购了GPU公司ATI后，AMD成为了当时唯一同时拥有CPU和GPU设计能力的芯片公司，而且在两个市场都是行业老二——但坏消息是，市场主流玩家也就两个。
在这种局面下，AMD希望借助APU打开市场局面。2011年，第一代APU推出后，AMD持续宣传APU是“x86架构三十年来的最大革命”，并向投资者强调，这款产品存在着“强劲且被压抑”的需求。
市场最初也对APU概念充满期待，结果2012年Q3财报出炉，AMD收入下滑25%，顺便减记了1亿美元的库存——APU需求量并不高，芯片根本卖不出去[1]。紧接着，公司股价跌到1.86美元的历史性低点，苏姿丰临危受命，开始掌舵风雨飘摇中的AMD。
APU的优势在于，由于CPU和GPU集成在了一起，数据传输效率得到了大幅度提高。苹果的M1 Ultra也采用了类似的“把几个小芯片拼成一块大芯片”的思路，换来了更强的数据吞吐能力。
但在2009年，APU的理念显得过于超前。
一方面，APU涉及芯片的先进封装技术，在当时既不成熟，成本也难以控制。另一方面，APU在需求高度多元化的消费市场很难行得通。
比如10种型号的CPU和GPU，理论上有100种组合方案，这就导致做10种方案无法满足市场需求，做100种方案难以收回生产成本。
因此在很长一段时间里，APU只能在PS4游戏机这类高度标准化的产品上才能找到市场。但深度学习的大爆发改变了这一点。
相比游戏和渲染，AI训练对算力和数据吞吐效率的需求成百上千倍的增加，目前针对AI市场推出的芯片产品，除了算力的堆砌，往往都采用3D堆叠和先进封装等方式，增加数据传输的效率，这与APU的优势不谋而合。
英特尔尚未正式发布的Falcon Shores，同样采用了将CPU、GPU、内存封装在一起的思路，只不过英特尔将其称为“XPU”。
但目前来看，最接近这个目标的反而是英伟达的Grace Hopper芯片。
英伟达的Grace Hopper将CPU和GPU集成在了一起
尾声 在2009年APU的概念被提出时，AMD正经历公司历史上的最低谷，APU多少有些毕功一役的憋大招成分。
但也正是因为处于低谷，导致AMD无法拿出足够的资金与技术支持，让APU的革命性理念真正落地，最终只变成了简单的CPU+GPU的组合。
从商业角度看，最适合在2009年搞点革命性产品的反而是富可敌国的英特尔，但英特尔当时在干什么呢——心安理得的挤牙膏，同时拒绝为iPhone设计芯片。
这似乎是高科技公司常常会出现的状况——在鼎盛年代忽视新的技术浪潮，在低谷期如梦方醒仓促憋大招。
事实上，英特尔还尝试过“联A抗N”——2017年，英特尔宣布将在自家CPU上集成AMD的GPU，合作推出新的芯片。
结果没过多久，英特尔就挖走了AMD的核心技术负责人之一：图形主管Raja Koduri，为英特尔开发高端独立GPU。
]]></content>
  </entry>
  
  <entry>
    <title>2029年前，NAND是否能够替代HDD</title>
    <url>/post/storage/could-nand-capacity-replace-hdds-by-2029.html</url>
    <categories><category>Storage</category>
    </categories>
    <tags>
      <tag>SSD</tag>
      <tag>NAND</tag>
      <tag>HDD</tag>
    </tags>
    <content type="html"><![CDATA[分析：Pure Storage表示，2028年后将不再出售新的硬盘驱动器（HDD）。Pure的这一大胆声明意味着，2029年本应销售的所有新HDD容量（不包括更换故障驱动器的容量）将被SSD容量取而代之。这将是大量额外的SSD出货量，并提出了一个问题：全球NAND制造能力是否能够生产足够的闪存？
我们试图从NAND制造的角度来判断Pure Storage的预测是否可能，暂时忽略了HDD和SSD的总拥有成本（TCO）的影响。这需要有所解释。
HDD和SSD价格趋势 硬盘驱动器的出货量在单位数量方面已经连续下降了五年以上，因为SSD在越来越多的驱动器类别中提供了一个引人注目的替代方案。SSD比HDD更快地响应I/O请求，因为它们不必等待读写头在磁盘表面上移动到正确的磁道，然后等待磁盘的旋转将正确的数据块带到头部。
Statista图表显示，从2010年开始，每年的磁盘驱动器出货量都在下降 SSD成本一直在稳步下降，但随着晶圆中闪存容量的增加，下降速度在减小。这是由以下三件事引起的：NAND晶圆组件缩小，3DNAND技术中添加了更多层单元，以及使用TLC（3位/单元）现在已成为主流的多位单元。数据还原技术（压缩和重复数据删除）意味着SSD可以比其原始物理容量存储更多的数据，将其每TB的成本与不使用数据还原的HDD的成本更接近。
Wells Fargo图表比较了2012年至2020年的PC HDD和SSD价格。这显示了SSD价格（每TB）比HDD价格下降得更快 整体而言，由于这个原因，硬盘驱动器（HDD）市场已经出现下滑。笔记本电脑几乎完全转向了SSD，台式电脑也在紧随其后，而快速（10000转每分钟的2.5英寸）的关键任务HDD也正在被SSD所取代。HDD业务市场正逐渐围绕着近线（7400转每分钟的3.5英寸）驱动器聚集。然而，QLC（每个单元4位）SSD技术进一步降低了每TB的成本，而持续的层次增加将维持这一下降趋势。
HDD制造商表示，他们的技术进步，如HAMR，将继续将他们的价格降低，并因此保持他们对SSD的成本优势。闪存驱动器的价格不会低于HDD价格，因为HDD的成本下降将保持磁介质的经济性。
Wells Fargo高级分析师Aaron Rakers在2019年预测，企业存储买家将在价格降至硬盘驱动器的5倍时开始更喜欢SSD。他注意到2017年企业SSD比大容量近线磁盘驱动器高出18倍。这在2019年降至9倍。2020年，Rakers表示，企业SSD的一般成本为185美元/TB，而近线HDD的成本约为19美元/TB，这意味着企业SSD的价格溢价为9.7倍。这在几个季度内保持不变，如下图所示：
HDD的客户通常希望更快的数据访问速度，如果SSD的成本与HDD相等，或者至少不比HDD高太多，并且SSD的使用寿命与HDD一样长，他们会使用SSD。正如分析师指出的，发生这种情况的障碍是，世界上没有足够的NAND制造能力来取代所有客户购买的HDD容量。
Pure已成为第一个明确表示闪存将取代硬盘驱动器的闪存存储供应商。它声称其QLC闪存系统可以取代近线存储驱动器阵列，因为其5年TCO低于近线驱动器阵列。闪存每TB成本的持续下降，来自更高密度驱动器，将加强这一优势。
总拥有成本（TCO）的计算从采购价格开始，然后考虑驱动器的使用寿命、功耗和冷却成本，以及寿命周期结束时的处置成本。由于这些成本是未来的估计值，而且磁盘驱动器制造商可以根据不同的假设提供不同的数据。
即使不考虑成本和价格问题，问题仍然是是否有足够的NAND制造能力来替代每年销售的数百万个硬盘驱动器。
NAND制造能力 Micron高级总监Colm Lysaght在2019年表示：“很明显，SSD的每GB价格会随着时间的推移接近HDD的每GB价格。但是，从近线HDD到SSD进行“批量切换”所需的EB数量对于NAND闪存行业来说太大了。生成所需EB所需的资本投资……是过于昂贵的。”
“SSD可能会蚕食（甚至可能吞噬）近线HDD市场，但两者将在未来许多年内共存。”
像Wikibon的David Floyer等分析师表示，NAND制造能力不会成为限制因素。他预测，2026年，NAND生产效率将导致SSD在每TB美元的基础上比HDD更便宜。这是一个大胆的说法。
现在是2023年，Pure基本上表示它认为闪存制造能力不会是一个限制因素——“生成所需EB所需的资本投资”不再是“过于昂贵”。这是真的吗？NAND制造能力能否应对硬盘驱动器容量替代的需求？
NAND制造建模 要计算NAND代工厂行业是否能够生产满足SSD数据存储需求增长和2029年取代新HDD所需的容量，我们必须估计存储数据增长率、当前出货的HDD和SSD容量以及NAND代工厂容量。代工厂的产能需要增加多少才能满足需求，增加的NAND层数和每单元位数会如何影响制造产量？
我们制定了一个初步的简化电子表格模型，并与一些分析师和供应商分享了该模型，他们的意见改进并完善了该模型。简而言之，它表明2029年将有5.7%的NAND代工厂产能缺口，即405EB。
模型假设 我们从2022年出货的约1320EB的HDD容量和277EB的SSD容量开始，这是通过检查供应商和研究机构的数字得出的。如果2022年出货的1320EB的HDD容量在2029年降至零，而数据增长继续，那么SSD行业将不得不增长以满足其自身的市场需求，并提供缺失的HDD贡献。
它将受到存储数据增长的间接影响。这将涉及相当简单的假设，但这在这里是可以接受的，因为我们只试图看看制造能力的基础是否已经到位。Objective Analysis的Jim Handy表示：“IDC的数据领域经常被引用为衡量数据生产增长速度的指标。他们说，2026年将生成221178EB的数据，并可以趋势到2029年的394127EB。”这提供了从2026年到2029年的21.2%的数据增长复合年增长率。
我们将其应用于2022年出货的HDD容量，得出2029年需要5071EB。接下来，我们问2022年有多少NAND制造能力存在？Handy告诉我们：“2021年的EB出货量为538EB，因此他们至少有能力在2022年出货这么多。”这就是我们的起点。
我们使用21.2%的复合年增长率将其趋势到2029年，达到2029年的2067EB。然后，我们加上HDD替换数量，5071EB，得到2029年需要的7137.8EB的NAND制造能力。
额外的3D层叠和QLC将带来多少产能？Handy告诉我们：“需要新的产能，它将以每年约30%的速度上线，以支持每年EB增长。闪存层大约每两年翻一番。这给了该行业约40%的增长率，因此他们不必以那么高的速度增加层数。我不知道层数翻倍是否会需要双倍的晶圆厂工具和地板空间来处理固定数量的晶圆。”
这是一个很大的变数
我们将每年增加的40%的层叠率总结为2029年额外的3375.9EB的制造能力。如果从TLC（3位/单元）切换到QLC（4位/单元）NAND，则会增加33.3%。这将我们带到5671.2EBx1.33=7542.8EB，比需要的7137.8EB少405EB。这意味着需要额外的NAND制造能力，这意味着必须建造新的NAND代工厂。
层叠率增长因子是一个关键数字。如果将其降低到每年30%，那么我们最终将在2029年面临2647.9EB的NAND制造缺口。
供应商观点 Dell发言人表示：“我们的看法是，我们的产品组合使Dell能够灵活地为客户提供非常广泛的驱动器选择。随着对高密度和低成本HDD的持续创新，我们预计现在还不能说客户需求不会在2028年之后存在。”
Seagate首席技术官JohnMorris告诉我们：“简单地说，我们认为用NAND取代HDD在任何情况下都是不可能的。这不仅因为需要大量资本支出来指数级增加NAND供应，而且因为NAND无法达到驱动摆脱HDD所需的每位成本。”
“更重要的是，SSD和HDD一直在相互协作，因为它们在企业世界中各自满足不同的工作负载需求。在大多数存储市场中，它们被部署在一起，以创建最具成本效益的解决方案。这种架构将在可预见的未来持续存在。我们对HDD价值的信念体现在我们对未来HDD创新的持续投资中。”
他还建议：“与其看总的HDD和SSDEB，不如只关注近线HDD和企业SSD EB（以及相关的复合年增长率）。根据2022年的实际数据，近线HDD EB约为企业SSD的7倍。”
Pure Storage的回应 Pure Storage发言人表示：“在看NAND市场供应时，我们鼓励您超越当前的SSD消费，而是看整个市场……TrendFocus的研究……将整个NAND市场认定为约2.5倍于今天使用的SSD。
“虽然SSD NAND不同于用于手机/平板电脑/USB闪存驱动器/汽车和家电的NAND，但这表明NAND制造和晶圆厂的可用池要大得多，可以从中驱动增长和平衡需求。最终，制造商将扩大生产并改装他们的晶圆厂，以满足最有利可图的市场，我们确实预计从HDD到SSD的转换将是该需求的主要因素。
“您的模型假设传统HDD将1:1转换为SSD。虽然这对于这种规模的模型是合适的，但我们确实相信，随着企业从HDD转向SSD，会出现额外的容量整合机会，原因是增加了数据减少、提高了性能（减少了复制文件的需求）、降低了故障率和改进了纠删码效率。”
总而言之，我们（非常简化的）建模演练表明，只要层数以每年40%的速度增加，SSD NAND制造产出缺口将低于6%。这还没有考虑到Pure的观点，即总的NAND制造能力“约为目前用于SSD的约2.5倍”。
5.7%的差异也在误差范围之内。基本上，我们认为在建造闪存以取代2029年的新HDD方面，似乎不存在NAND代工容量问题。
我们将会密切关注，看看未来会带来什么。
原文地址: 2029年前，NAND是否能够替代HDD  
]]></content>
  </entry>
  
  <entry>
    <title>RS422/485接口电路设计要点</title>
    <url>/post/hardware/design-points-of-rs422-485-circuit.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>UART</tag>
      <tag>RS422</tag>
      <tag>RS485</tag>
    </tags>
    <content type="html"><![CDATA[RS-422标准全称是“平衡电压数字接口电路的电气特性”，它定义了接口电路的特性。
实际上还有一根信号地线，共5根线。由于接收器采用高输入阻抗和发送驱动器比RS232更强的驱动能力，故允许在相同传输线上连接多个接收节点，最多可接10个节点。一个主设备（Master），其余为从设备（Slave），从设备之间不能通信，所以RS-422支持点对多的双向通信。接收器输入阻抗为4k，故发端最大负载能力是10&amp;TImes;4k+100Ω（终接电阻）。
 RS-422和RS-485电路  原理基本相同，都是以差动方式发送和接收，不需要数字地线。差动工作是同速率条件下传输距离远的根本原因，这正是二者与RS232的根本区别，因为RS232是单端输入输出，双工工作时至少需要数字地线。发送线和接收线三条线（异步传输），还可以加其它控制线完成同步等功能。
RS-422通过两对双绞线可以全双工工作收发互不影响，而RS485只能半双工工作，发收不能同时进行，但它只需要一对双绞线。RS422和RS485在19kpbs下能传输1200米。用新型收发器线路上可连接台设备。
典型的RS422接口电路 图 1 典型的RS422接口电路
典型的RS485接口电路 图 2 典型的RS485接口电路
图 3 全双工RS485接口电路拓扑
设计要点   接口保护用途的TVS管D1-8，通常选择最大反向工作电压VRWM为5.0V的双向TVS管，如Diodes SMBJ5.0CA。注：这里可以选择耐压更高的TVS元件。
  DI和RO引脚都使用10k电阻上拉，是为防止误触发，产生误动作，因为“UART以一个前导“0”触发一次接收动作”。
  图 1所示，差分接收器的端接电阻一般取值120 Ω，来源于通常RS422/485传输线所用的特征阻抗约为120 Ω。图 3所示的RS485多点应用中，若在SCH&amp;PCB设计时不清楚后期现场布线中哪两个设备距离最远，可在所有差分接收端都预留120 Ω端接电阻，以便后期现场应用时通过拨码开关选择性接入。
  由于RS422/485差分接收器的特性是，VIA - VIB的绝对值必须大于200 mV，否则无法正确识别高低电平。所以，图 1所示，当使用3.3V电源时，故障安全偏置电阻R5和R6最大取值为930 Ω；当使用5.0V电源时，R5和R6最大取值为1440 Ω。
  说明：故障安全偏置电阻，是为了解决“总线空闲、开路或短路”情况下，接收端状态不确定的问题。由于RS422只支持点对点应用，且故障安全偏置电阻只需要在接收端使用，所以图 1和图 2电路，R3-4不是必要的，R5-6和R12-15是必要的。注：如果R12/R13在发送端已经有，那么在接收端就不是必要的。
 图 1所示，在RS422点对点应用中，两端的差分接收器都需要120 Ω并联端接电阻。图 3所示，在RS485多点应用中，只需在最远的两点接收端使用120 Ω并联端接电阻，中间各支路不需要。
  图 2和图 3所示，各支路的A&amp;B引脚和Z&amp;Y引脚都串联0R电阻，当某路故障时将RS485总线拉低时，逐一断开电阻，方便排查故障。
  SCH&amp;PCB设计时，两个设备间的RS422/485通信线，除了两对差分线外，至少需要一根地线，防止共模电压超出规定的范围而导致通信故障。
  有选择的情况下，RS422/485通信电缆中，信号线不应与电源线并行或尽量远离电源线，若无法避免，信号线最好使用带屏蔽的双绞线。且现场布线，采用菊花链拓扑，不采用星形或环形拓扑，以免因反射等因素导致通信错误。
  原文地址： RS422/485接口电路设计要点  
]]></content>
  </entry>
  
  <entry>
    <title>三大网口类型：千兆网口、2.5G网口和5G网口</title>
    <url>/post/datacenter/three-typical-bandwidth-network-adapters.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>1G</tag>
      <tag>2.5G</tag>
      <tag>5G</tag>
    </tags>
    <content type="html"><![CDATA[当涉及到网络连接速度时，选择正确的网口类型非常重要。在现代网络中， 千兆网口  、2.5G网口和5G网口是常见的选项。本文将详细介绍这些网口类型以及如何选择适合你的需求的网口。
千兆网口 千兆网口，也称为Gigabit Ethernet，是最常见和广泛使用的网口类型之一。它提供的传输速度为1千兆位每秒（1 Gbps），也就是每秒可以传输1亿个位。千兆网口是许多家庭和办公室网络的标准连接方式。
千兆网口的主要优点是速度快、稳定性高和成本相对较低。它可以满足大多数家庭和小型办公室的网络需求，可以处理高清视频流、在线游戏和常见的网络任务。此外，千兆网口的设备和网络线材广泛可用，成本相对较低。
然而，千兆网口的主要限制是其传输速度有上限。对于需要更高速度的场景，千兆网口可能无法满足需求。例如，如果你需要同时传输大量数据或处理高带宽应用程序，千兆网口可能会成为瓶颈，限制了网络性能。
2.5G网口 2.5G网口是一种介于千兆网口和5G网口之间的新型网口类型。它提供的传输速度为2.5千兆位每秒（2.5 Gbps），比千兆网口快2.5倍，但比5G网口慢一半。2.5G网口的出现是为了填补千兆网口和5G网口之间的速度差距，提供更好的性能选择。
2.5G网口的优点是在提供更高速度的同时保持成本相对较低。它可以适应一些需要更高带宽的应用场景，如视频编辑、大规模数据传输和多设备同时使用网络等。2.5G网口还兼容千兆网口设备，因此可以逐步升级网络而无需更换所有设备。
然而，2.5G网口的主要限制是设备和线缆的可用性相对较少。相比之下，千兆网口设备和线缆更加普遍和便宜。此外，2.5G网口的速度虽然比千兆网口快，但对于某些高性能场景来说仍然不够。
5G网口 5G网口是一种高速网口类型，提供的传输速度为5千兆位每秒（5 Gbps），是目前可用的最高速度之一。它适用于需要处理大规模数据传输、实时视频流、虚拟现实和其他高带宽应用程序的场景。
5G网口的主要优点是其出色的传输速度和性能。它可以满足对高速网络连接有严格要求的专业用户和企业需求。对于需要同时处理多个高带宽任务的场景，5G网口是一个理想选择。
然而，5G网口的主要限制是其设备和线缆的成本相对较高，并且在市场上的可用性较为有限。除此之外，大多数家庭和小型办公室的常规网络需求可能不需要如此高的速度，因此选择5G网口可能会超出实际需求。
三者区别    网口类型 传输速度 主要优点 主要限制     千兆网口 1 Gbps 速度快、稳定性高、成本相对较低 速度有上限，可能成为网络性能瓶颈   2.5G网口 2.5 Gbps 提供更高速度、成本相对较低、兼容千兆网口 设备和线缆可用性相对较少，速度不足以满足某些高性能场景   5G网口 5 Gbps 出色的传输速度和性能 设备和线缆成本较高，市场上的可用性有限    这个表格清晰地列出了千兆网口、2.5G网口和5G网口之间的主要区别。千兆网口在速度、成本和稳定性方面具有优势，但速度有上限。2.5G网口提供更高的速度，兼容千兆网口设备，但设备和线缆可用性较少。5G网口具有出色的速度和性能，但设备和线缆成本较高且市场上的可用性有限。
如何选择 在选择千兆网口、2.5G网口和5G网口时，有几个关键因素需要考虑：
  需求：首先确定你的网络需求。如果你只是进行一般的上网浏览、电子邮件和常见的网络任务，千兆网口通常已经足够了。如果你需要处理大规模数据传输或需要更高速度的特定应用程序，可以考虑2.5G网口或5G网口。
  设备和线缆的可用性：检查市场上设备和线缆的可用性。千兆网口设备和线缆相对更普遍和便宜，而2.5G网口和5G网口的设备和线缆相对较少。确保你能够轻松获得所需的设备和线缆。
  成本：考虑你的预算。千兆网口通常是最经济实惠的选择，而2.5G网口和5G网口的设备和线缆成本较高。权衡你的需求和预算，选择最适合的选项。
  未来扩展性：考虑你的网络未来是否需要升级。如果你计划在未来增加更多设备或需要更高速度，可以选择具有更高性能的网口类型，如2.5G网口或5G网口，以便于网络的扩展和升级。
  笔者特地整理成表格，方便大家记忆：
   考虑因素 千兆网口 2.5G网口 5G网口     传输速度 1 Gbps 2.5 Gbps 5 Gbps   主要优点 速度快、稳定性高、成本相对较低 提供更高速度、成本相对较低、兼容千兆网口 出色的传输速度和性能   主要限制 速度有上限，可能成为网络性能瓶颈 设备和线缆可用性相对较少，速度不足以满足某些高性能场景 设备和线缆成本较高，市场上的可用性有限   适用场景 一般上网浏览、常见网络任务 需要更高带宽的应用场景、逐步升级网络 高速网络连接需求、高带宽应用程序   设备可用性 广泛可用、成本相对较低 相对较少、设备成本较高 有限可用性、设备成本较高   成本 相对较低 相对较低 较高   未来扩展性 有限 适中 适中    总结 综上所述，选择适合你的需求的网口类型是关键。千兆网口是最常见和经济实惠的选择，适用于大多数家庭和小型办公室的常规网络需求。如果你需要更高速度和性能，可以考虑2.5G网口或5G网口，但要注意设备和线缆的可用性以及成本因素。
]]></content>
  </entry>
  
  <entry>
    <title>VXLAN：数据中心网络的未来</title>
    <url>/post/datacenter/vxlan-the-future-of-data-center.html</url>
    <categories><category>DataCenter</category>
    </categories>
    <tags>
      <tag>VXLAN</tag>
    </tags>
    <content type="html"><![CDATA[随着云计算和虚拟化技术的快速发展，数据中心网络正面临着越来越大的挑战。
传统的网络架构在适应大规模数据中心的需求方面存在一些限制，如扩展性、隔离性和灵活性等方面。为了克服这些限制，并为数据中心网络提供更好的性能和可扩展性， VXLAN  （Virtual Extensible LAN）作为一种新兴的网络虚拟化技术应运而生。本文将详细介绍VXLAN的工作原理、优势以及在数据中心网络中的应用，探讨VXLAN作为数据中心网络的未来发展趋势。
VXLAN概述 VXLAN是一种网络虚拟化技术，旨在解决传统以太网的限制，并提供更好的可扩展性和隔离性。VXLAN通过在现有的IP网络上创建一个虚拟的二层网络，将传统的以太网帧封装在UDP报文中进行传输。这种封装使得VXLAN可以在现有的网络基础设施上运行，而无需对网络进行大规模改造。
VXLAN使用一个24位的VXLAN标识符（VNI）来标识虚拟网络，允许同时存在多个独立的虚拟网络。VXLAN报文的目的地MAC地址被替换为VXLAN网络中的虚拟机或物理主机的MAC地址，从而实现虚拟机之间的通信。VXLAN还支持多路径传输（MP-BGP EVPN）以及网络中的多租户隔离。
VXLAN工作原理 VXLAN的工作原理可以简单地分为封装和解封装两个过程。
  封装：当虚拟机（VM）发送一个以太网帧时，VXLAN模块将这个以太网帧封装在一个UDP报文中。报文的源IP地址是VM所在主机的IP地址，目的IP地址是VXLAN隧道的远程端点的IP地址。VXLAN头中的VNI字段标识了目标虚拟网络。随后，UDP报文被发送到底层网络中，到达目标主机。
  解封装：当接收到一个VXLAN报文时，VXLAN模块会解析UDP报文头，提取出封装的以太网帧。通过查找VNI字段，VXLAN模块可以确定目标虚拟网络，并将以太网帧发送到相应的虚拟机或物理主机。
  这种封装和解封装的过程使得VXLAN可以在底层网络上透明地传输以太网帧，同时提供了逻辑上隔离的虚拟网络。
VXLAN的优势 VXLAN作为一种新兴的网络虚拟化技术，在数据中心网络中具有以下优势：
  可扩展性：VXLAN使用24位的VNI标识符，可以支持高达16,777,216个虚拟网络，每个虚拟网络都可以拥有独立的二层命名空间。这种可扩展性使得VXLAN能够满足大规模数据中心的需求，并支持多租户隔离。
  跨子网通信：传统以太网在跨越不同子网时需要依赖于三层路由器进行转发。而VXLAN通过使用底层IP网络作为传输介质，可以实现虚拟网络的跨子网通信，使得虚拟机可以自由迁移而无需改变IP地址。
  灵活性：VXLAN可以在现有的网络基础设施上运行，无需进行大规模的网络改造。它可以与现有的网络设备和协议兼容，如交换机、路由器和BGP等。这种灵活性使得组建和管理虚拟网络变得更加简单和高效。
  多路径传输：VXLAN结合了多路径传输（MP-BGP EVPN）的特性，可以在数据中心网络中实现负载均衡和冗余。它可以根据网络负载和路径可用性来选择最佳的路径进行数据传输，提供更好的性能和可靠性。
  安全性：VXLAN支持隧道加密，可以在底层IP网络上保护数据的机密性和完整性。通过使用安全协议（如IPsec）或虚拟专用网络（VPN），VXLAN可以提供更高级别的数据传输安全。
  VXLAN在数据中心网络中的应用 VXLAN在数据中心网络中有广泛的应用场景，以下是其中一些典型的应用：
  虚拟机迁移：VXLAN使得虚拟机可以自由地在不同的物理主机之间迁移，而无需改变IP地址。这种灵活性和可扩展性对于实现数据中心的负载均衡、资源调度和容错性非常重要。
  多租户隔离：通过使用不同的VNI，VXLAN可以将数据中心划分为多个独立的虚拟网络，实现不同租户之间的隔离。这种隔离性保证了租户之间的数据安全和隐私，并且可以为每个租户提供独立的网络策略和服务质量保证。
  跨数据中心连接：VXLAN可以扩展到跨多个数据中心的网络环境中，使得不同数据中心之间可以建立虚拟网络连接。这种功能可以支持数据中心间的资源共享、业务扩展和灾备备份等需求。
  云服务提供商：VXLAN可以帮助云服务提供商构建高度可扩展的虚拟化网络基础设施。通过使用VXLAN，云服务提供商可以提供灵活的虚拟网络服务，并支持多租户环境下的资源隔离和安全性。
  虚拟网络功能（VNF）：VXLAN与网络功能虚拟化（NFV）相结合，可以实现虚拟网络功能的部署和管理。VXLAN可以作为底层网络虚拟化技术，为VNF提供灵活的网络连接和隔离，从而实现网络功能的快速部署和弹性扩展。
  结论 VXLAN作为一种新兴的数据中心网络虚拟化技术，具有强大的可扩展性、灵活性和隔离性，为数据中心网络的未来发展提供了新的方向和解决方案。通过使用VXLAN，数据中心可以实现虚拟机迁移、多租户隔离、跨数据中心连接以及云服务提供商的支持。VXLAN的工作原理和优势使其成为构建高性能、可靠和安全的数据中心网络的关键技术之一。
随着云计算和大数据应用的不断发展，数据中心网络将继续面临更多的挑战和需求。VXLAN作为数据中心网络的未来发展趋势之一，将继续演进和完善，以满足不断变化的业务需求，并推动数据中心网络的创新和发展。
]]></content>
  </entry>
  
  <entry>
    <title>苦战15年，谷歌云业务终于实现首次盈利</title>
    <url>/post/news/google-cloud-business-is-finally-profitable-for-the-first-time.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Cloud</tag>
      <tag>Google</tag>
    </tags>
    <content type="html"><![CDATA[谷歌云业务（Google Cloud）是谷歌母公司 Alphabet 旗下的一个部门，负责提供云计算、数据分析、机器学习等服务。
自从 2008 年推出以来，谷歌云业务一直处于亏损状态，直到今年第一季度才终于首次实现了盈利。
根据 Alphabet 公布的财报，截至 3 月 31 日的季度，谷歌云业务的收入为 74 亿美元，运营利润为 1.91 亿美元（约 13.24 亿元人民币），盈利率为 2.5%。虽然这是一个历史性的突破，但与其竞争对手相比，谷歌云业务仍然显得弱小。
亚马逊旗下的 AWS（Amazon Web Services）在 2022 年的收入达到了 800 亿美元，运营利润为 228 亿美元，盈利率为 28%。微软旗下的 Azure 也在不断增长，虽然没有公布具体的数字，但据估计其市场份额已经超过了谷歌云业务的两倍。
谷歌云业务为什么一直亏损？主要原因是其投入了大量的资金来扩大基础设施、开发新产品、招募新客户和人才。在过去的三年里，谷歌云业务累计亏损了 146 亿美元。
Alphabet 的 CEO Sundar Pichai 并不在意这些亏损，他认为这是为了未来的增长而做出的必要投资，并且谷歌本身有足够的利润来支撑这些投资。他对这次盈利表示满意，并称谷歌云业务已经成为全球最大的企业软件公司之一。
谷歌云业务能否在未来缩小与领先者之间的差距，还要看它能否继续保持创新和竞争力。目前，谷歌云业务正在开发一些新技术，如生成式人工智能（generative AI），以提升其搜索服务的质量和效率。Pichai 表示，他认为这些技术不会增加太多的基础设施成本，并且会给用户提供更多的选择。
同时，谷歌云业务也面临着一些挑战和压力。首先是市场环境的变化，由于新冠疫情和芯片短缺等因素，导致整个行业的需求和收入都有所下降。Alphabet 本季度的总收入只增长了 3%，达到 698 亿美元。其次是成本控制的问题，Alphabet 本季度承担了 26 亿美元的费用，主要包括裁员和取消闲置办公空间等项目。这些费用导致其运营收入和利润、利润率都有所下降。最后是战略调整的问题，Alphabet 的首席财务官 Ruth Porat 表示，该公司将致力于实现长期增长，并通过优化成本结构来为最有前景的领域创造投资空间。这意味着谷歌云业务可能会面临更多的内部竞争和审查，而不是无限制的支持和扶持。
 Google AI TPU  
]]></content>
  </entry>
  
  <entry>
    <title>Linux内核设计与实现—进程调度</title>
    <url>/post/linux/linux-kernel-design-and-implementation-process-schedule.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Linux</tag>
      <tag>Kernel</tag>
    </tags>
    <content type="html"><![CDATA[最近开始学习 Linux  内核，主要参考书籍为《Linux内核设计与实现》，所以本系列大章节和小章节会遵从原书结构，再辅以其他书籍或网上资料对其未理解部分进行补充。
概述 本系列定位为初级文档，不会详细阐述实现原理，只讲解概念和逻辑。
调度程序的定义 调度程序决定将哪个进程投入运行，何时运行以及运行多长时间。调度程序可看做在可运行态进程之间分配有限的处理器时间资源的内核子系统。
只要有可以执行的进程，那么就总会有进程正在执行。但是只要系统中可运行的进程的数目比处理器的个数多，就注定某一给定时刻会有一些进程不能执行。
在一组处于可运行状态的进程中选择一个来执行，是调度程序所需完成的基本工作。
调度策略的分类 策略决定调度程序在何时让什么进程运行。不同策略的调度器对系统呈现的风格不同。
###IO消耗型和处理器消耗型
进程可以被分为I/O 消耗型和处理器消耗型。
I/O消耗型指的是进程大部分时间用来提交I/O或者等待I/O请求。这样的进程进程处于可运行状态，一般都是运行一小会，更多时间是处于阻塞状态。常见的IO消耗型进程通常有图像界面，鼠标或键盘类。
处理器消耗型是把大多数时间用于执行代码，除非被抢占，否则他们一直不停的运行。常见的消耗型进程通常有算法，业务类。
进程优先级 调度算法中最基本的一类就是基于优先级的调度。通常做法是高优先级先运行，低优先级后运行，优先级一样则轮转运行。
Linux 采用了两种不同的优先级范围：
第一种是 Nice 值，Nice值的范围是-20~+19，拥有Nice值越大的进程的实际优先级越小（即Nice值为+19的进程优先级最小，为-20的进程优先级最大），默认的Nice值是0。
“Nice值”这个名称来自英文单词nice，意思为友好。Nice值越高，这个进程越“友好”，就会让给其他进程越多的时间。
第二种范围是实时优先级PRI值，其值是可配。默认情况下它的变化范围是从0 到99 （包括0 和99) 。与nice 值意义相反，越高的实时优先级数值意味着进程优先级越高。
最终的进程优先级为PRI(now) = PRI(start) + NI(now)。Linux操作系统中，我们是通过修改NI值的方式，来修改PRI优先级的大小。
在Linux中使用ps -efl来查看PRI和NI值。
mark@vxworks:~/github/2beanet$ ps -elf F S UID PID PPID C PRI NI ADDR SZ WCHAN STIME TTY TIME CMD 4 S root 1 0 0 80 0 - 42022 - Apr15 ? 00:00:40 /lib/systemd/systemd splash --system --deserialize 56 1 S root 2 0 0 80 0 - 0 - Apr15 ? 00:00:00 [kthreadd] 1 I root 3 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [rcu_gp] 1 I root 4 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [rcu_par_gp] 1 I root 5 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [slub_flushwq] 1 I root 6 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [netns] 1 I root 8 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [kworker/0:0H-events_highpri] 1 I root 10 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [mm_percpu_wq] 1 I root 11 2 0 80 0 - 0 - Apr15 ? 00:00:00 [rcu_tasks_kthread] 1 I root 12 2 0 80 0 - 0 - Apr15 ? 00:00:00 [rcu_tasks_rude_kthread] 1 I root 13 2 0 80 0 - 0 - Apr15 ? 00:00:00 [rcu_tasks_trace_kthread] 1 S root 14 2 0 80 0 - 0 - Apr15 ? 00:00:08 [ksoftirqd/0] 1 I root 15 2 0 80 0 - 0 - Apr15 ? 00:05:23 [rcu_preempt] 1 S root 16 2 0 -40 - - 0 - Apr15 ? 00:00:04 [migration/0] 1 S root 17 2 0 9 - - 0 - Apr15 ? 00:00:00 [idle_inject/0] 1 S root 19 2 0 80 0 - 0 - Apr15 ? 00:00:00 [cpuhp/0] 5 S root 20 2 0 80 0 - 0 - Apr15 ? 00:00:00 [cpuhp/1] 1 S root 21 2 0 9 - - 0 - Apr15 ? 00:00:00 [idle_inject/1] 1 S root 22 2 0 -40 - - 0 - Apr15 ? 00:00:04 [migration/1] 1 S root 23 2 0 80 0 - 0 - Apr15 ? 00:00:04 [ksoftirqd/1] 1 I root 25 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [kworker/1:0H-events_highpri] 5 S root 26 2 0 80 0 - 0 - Apr15 ? 00:00:00 [cpuhp/2] 1 S root 27 2 0 9 - - 0 - Apr15 ? 00:00:00 [idle_inject/2] 1 S root 28 2 0 -40 - - 0 - Apr15 ? 00:00:04 [migration/2] 1 S root 29 2 0 80 0 - 0 - Apr15 ? 00:01:44 [ksoftirqd/2] 1 I root 31 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [kworker/2:0H-events_highpri] 5 S root 32 2 0 80 0 - 0 - Apr15 ? 00:00:00 [cpuhp/3] 1 S root 33 2 0 9 - - 0 - Apr15 ? 00:00:00 [idle_inject/3] 1 S root 34 2 0 -40 - - 0 - Apr15 ? 00:00:04 [migration/3] 1 S root 35 2 0 80 0 - 0 - Apr15 ? 00:00:14 [ksoftirqd/3] 1 I root 37 2 0 60 -20 - 0 - Apr15 ? 00:00:00 [kworker/3:0H-events_highpri] 时间片 时间片是一个数值，它表明进程在被抢占前所能持续运行的时间。
调度策略必须规定一个默认的时间片，时间片过长会导致系统对交互的响应表现欠佳，让人觉得系统无法并发执行应用程序；
时间片太短会明显增大进程切换带来的处理器耗时，因为肯定会有相当一部分系统时间用在进程切换上，而这些进程能够用来运行的时间片却很短。
任何长时间片都将导致系统交互表现欠佳。很多操作系统中都特别重视这一点，所以默认的时间片很短，如10ms。
调度策略的活动 根据系统的进程自动分配CPU。
比如说现在两个可运行的进程，文字编辑程序和视频编码程序。文本编辑程序是I/O消耗型，大部分时间用于阻塞等待用户键盘输入，视频编码程序属于处理器消耗型，大量的时间用于视频解码运算。
用户希望按下按键能立马响应，而对视频编码没有严格的要求，用户分辨不出是立刻就运行还是延迟后在执行。
理想情况是调度器应该给予文本编辑程序相比视频编码程序更多的处理器时间，因为它属于交互式应用。
对文本编辑器而言，我们有两个目标：
 我们希望系统给它更多的处理器时间，这并非因为它需要更多的处理器时间（其实它不需要），是因为我们希望在它需要时总是能得到处理器； 我们希望文本编辑器能在其被唤醒时（也就是当用户打字时）抢占视频解码程序。这样才能确保文本编辑器具有很好的交互性能，以便能响应用户输入。  在多数操作系统中，上述目标的达成是要依靠系统分配给文本编辑器比视频解码程序更高的优先级和更多的时间片。
在Linux中，默认调度器为CFS（Completely Fair Scheduler，完全公平调度器），它是通过分配一个给定的处理器使用比来实现这个目的。假如文本编辑器和视频解码程序是仅有的两个运行进程，并且又具有同样的nice 值，那么处理器的使用比将都是50％——它们平分了处理器时间。
但因为文本编辑器将更多的时间用于等待用户输人，因此它肯定不会用到处理器的50% 。同时，视频解码程序无疑将能有机会用到超过50％的处理器时间，以便它能更快速地完成解码任务。
在上述场景中， 一旦文本编辑器被唤醒， CFS注意到给它的处理器使用比是50% ，但是其实它却用得少之又少。特别是， CFS 发现文本编辑器比视频解码器运行的时间短得多。
这种情况下，为了兑现让所有进程能公平分享处理器的承诺，它会立刻抢占视频解码程序，让文本编辑器投入运行。文本编辑器运行后，立即处理了用户的击键输入后，又一次进入睡眠等待用户下一次输入。因为文本编辑器井没有消费掉承诺给它的50％处理器使用比，因此情况依旧， CFS 总是会毫不犹豫地让文本编辑器在需要时被投入运行，而让视频处理程序只能在剩下的时刻运行。
Linux的调度策略 Linux内核支持的调度策略如下：
（1）限期进程使用限期调度策略（SCHED_DEADLINE）。
限期调度策略有3个参数：运行时间runtime、截止期限deadline和周期period。
每个周期运行一次，在截止期限之前执行完，一次运行的时间长度是runtime。
（2）实时进程支持两种调度策略：先进先出调度（SCHED_FIFO）和轮流调度（SCHED_RR）。
SCHED_FIFO 实现了一种简单的、先人先出的调度算法。
一但一个SCHED_FIFO 级进程处于可执行状态，就会一直执行，直到它自己受阻塞或显式地释放处理器为止。只有更高优先级的SCHED_FIFO 或者SCHED_RR任务才能抢占SCHED_FIFO 任务。如果有两个或者更多的同优先级的SCHED_FIFO 级进程，它们会轮流执行，但是依然只有在它们愿意让出处理器时才会退出。
SCHED_RR 与SCHED_FIFO 大体相同， 只是SCHED_RR 级的进程在耗尽事先分配给它的 时间后就不能再继续执行了。也就是说， SCHED_RR 是带有时间片的SCHED_FIFO。当SCHED_RR 任务耗尽它的时间片时，在同一优先级的其他实时进程被轮流调度。
（3）普通进程支持两种调度策略：标准轮流分时（SCHED_NORMAL，在POSIX中叫做SCHED_OTHER）和空闲（SCHED_IDLE）。
标准轮流分时策略使用完全公平调度算法，把处理器时间公平地分配给每个进程。
空闲调度策略用来执行优先级非常低的后台作业，优先级比使用标准轮流分时策略和相对优先级为19的普通进程还要低，进程的相对优先级对空闲调度策略没有影响。
Linux调度器类 这5种调度类的优先级从高到低依次为：停机调度类、限期调度类、实时调度类、公平调度类和空闲调度类。
 停机调度类  停机调度类是优先级最高的调度类，停机进程（stop-task）是优先级最高的进程，可以抢占所有其他进程，其他进程不可以抢占停机进程。
停机（stop是指stop machine）的意思是使处理器停下来，做更紧急的事情。
停机进程没有时间片，如果它不主动让出处理器，那么它将一直霸占处理器。
限期调度类  限期调度类使用最早期限优先算法，使用红黑树（一种平衡的二叉树）把进程按照绝对截止期限从小到大排序，每次调度时选择绝对截止期限最小的进程。如果限期进程用完了它的运行时间，它将让出处理器，并且把它从运行队列中删除。在下一个周期的开始，重新把它添加到运行队列中。
实时调度器  实时调度类为每个调度优先级维护一个队列，使用位图法用来快速查找第一个非空队列，然后用链表数组串联任务，跟FreeRTOS类似。
公平调度类  CFS（Completely Fair Scheduler）是 Linux 内置（也是目前默认）的一个内核调度器 ， 如名字所示，它实现了所谓的“完全公平”调度算法，将 CPU 资源均匀地分配给各进程（ 在内核代码中称为“任务”，task）。简单来说，如果一台机器有一个 CPU 多个（计算密集型）进程，那采用 CFS 调度器。
 两个进程：每个进程会各占 50% CPU 时间 四个进程：每个进程会各占 25% CPU 时间  空闲调度器  每个处理器上有一个空闲线程，即0号线程。空闲调度类的优先级最低，仅当没有其他进程可以调度的时候，才会调度空闲线程。
抢占与上下文切换 上下文切换， 也就是从一个可执行进程切换到另一个可执行进程。
上下文切换调用schedule() 函数，schedule() 在调用context_switch()进行处理。它完成了最基本的两项工作：
 调用声明在中的switch_mm(), 该函数负责把虚拟内存从上一个进程映射切换到新进程中。 调用声明在中的switch_to(), 该函数负责从上一个进程的处理器状态切换到新进程的处理器状态。这包括保存、恢复栈信息和寄存器信息，还有其他任何与体系结  构相关的状态信息，都必须以每个进程为对象进行管理和保存。
内核提供need_resched()函数用于检测TIF_NEED_RESCHED是否被置位，用于判断需要重新调度。
在system_tick中断中和主动调用schedule()切换线程时，会对TIF_NEED_RESCHED 置位。
当返回用户空间以及从中断返回的时候，内核也会检查TIF_NEED_RESCHED 标志。如果已被设置， 会重新选择进程执行。
用户抢占 内核在即将返回用户空间的时候，如果need_resched 标志被设置，会导致schedule() 被调用，此时就会发生用户抢占。内核无论是在中断处理程序还是在系统调用后返回，都会检查need_resched 标志，如果它被设置了， 那么，内核会选择一个其他（更合适的）进程运行。
简而言之，用户抢占在以下情况时产生：
 从系统调用返回用户空间时 从中断处理程序返回用户空间时  内核抢占 Linux 在2.6版本后通过config PREEMPT 配置为内核抢占。
在不支持内核抢占的内核中，内核代码可以一直执行，到它完成为止。在内核空间运行的进程不具备抢占性。内核代码一直要执行到完成（返回用户空间）或明显的阻塞为止。现在，只要重新调度是安全的（没有持有锁），内核就可以在任何时间抢占正在执行的任务。
内核抢占会发生在：
 中断处理程序正在执行，且返回内核空间之前 内核代码再一次具有可抢占性的时候 如果内核中的任务显式地调用schedule() 如果内核中的任务阻塞（ 这同样也会导致调用schedule() ）  简单理解就是被抢占之前是用户空间，就是用户抢占，被抢占之前是内核空间就是内核抢占。
我们在看一下代码就更明确这两个概念了。
我们知道当中断发生在用户空间，即 USR mode 时执行 __irq_usr，当中断发生在内核空间即 SVC mode 时执行 __irq_svc。
// 抢占之前是运行在用户空间，比如说调用文件系统接口(open, read, write)等 __irq_usr: usr_entry kuser_cmpxchg_check irq_handler get_thread_info tsk mov why, #0 b ret_to_user_from_irq UNWIND(.fnend ) ENDPROC(__irq_usr) // 抢占之前运行在内核空间 **__irq_svc**: svc_entry irq_handler **#ifdef CONFIG_PREEMPT // 判断是否支持内核抢占** ldr r8, [tsk, #TI_PREEMPT] @ get preempt count ldr r0, [tsk, #TI_FLAGS] @ get flags teq r8, #0 @ if preempt count != 0 movne r0, #0 @ force flags to 0 tst r0, #_TIF_NEED_RESCHED blne svc_preempt **#endif** 总结 这一章主要描述了调度器的基本概念，以及常见的调度策略和Linux支持的调度类，在最后讲解了一些进程切换相关的知识。如果对RTOS比较熟悉的同学，虽然RTOS简单很多，但毕竟概念相通，阅读起来会比较轻松。如果没有RTOS基础的同学，可以读一下我之前写过的FreeRTOS源码分析相关章节，相信一定大有裨益。
]]></content>
  </entry>
  
  <entry>
    <title>5G和边缘计算</title>
    <url>/post/hardware/5g-and-edge-computing.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>Edge</tag>
      <tag>Computing</tag>
    </tags>
    <content type="html"><![CDATA[第五代 (5G) 蜂窝网络和边缘计算是近年来出现的两项最具创新性的技术。两者都准备好彻底改变企业的运营方式，并且潜在的好处是非常巨大的。
5G 和边缘计算在企业中的优势 5G是一种无线电信技术，有望提供比其前身（4G）更快的速度、更广的覆盖范围和更低的延迟。这意味着可以更快地传输和接收数据，这对于为连接的设备和应用供电至关重要。5G还具有处理更多设备和数据流量的潜力，从而实现更大的可扩展性。
边缘计算是一种分布式计算模型，可使数据处理和分析更靠近源头。通过让计算能力更接近用户，边缘计算可以减少延迟，提高应用的速度，并提供更可靠和安全的数据处理。
5G和边缘计算的结合是一个强大的组合。通过实现更快的数据传输和处理，5G和边缘计算可以为企业提供改进的性能和更高的效率。5G和边缘计算可以结合使用来创建更高效的通信和协作系统，以及可以改进决策制定和简化流程的自治系统。此外，5G和边缘计算可以协同工作，提高数据和应用的安全性。
企业将从许多方面受益于5G和边缘计算。它可以降低成本、改善客户体验、提高流程效率并创造新的收入来源。随着5G和边缘计算变得越来越普遍，企业应该做好充分利用其潜力的准备。
5G和边缘计算的挑战是什么？ 5G和边缘计算的引入带来了一系列新挑战。5G是一种移动通信标准，可实现更高的数据传输速度和更低的延迟。边缘计算是一种分布式计算范式，可以在靠近数据源的地方进行数据处理，而不是依赖集中式云计算资源。5G和边缘计算都将改变我们使用技术的方式，但也存在许多与之相关的挑战。
5G和边缘计算的最大挑战之一是安全性。随着越来越多的数据被边缘计算设备处理，数据泄露的风险也在增加。为了确保数据的安全，需要有一个强大的系统来保护数据免受潜在威胁。此外，边缘计算设备需要正确配置，以确保数据不会落入坏人之手。
5G和边缘计算的另一个挑战是可扩展性。边缘计算设备通常在计算能力方面受到限制，这使得在需要处理更多数据时难以扩展操作。此外，随着 5G 的普及，有必要确保边缘计算网络能够在不影响性能的情况下处理更高的数据传输速度。
最后，5G和边缘计算的成本是另一个挑战。与任何新技术一样，实施成本可能很高。此外，还需要考虑维护和升级 5G和边缘计算网络的成本。
总体而言，5G和边缘计算带来了一系列新的可能性和机遇，但也带来了一系列新的挑战。安全性、可扩展性和成本都是需要解决的领域，以确保可以安全、可靠且经济高效地使用 5G 和边缘计算。
探索5G和边缘计算的不同部署选项 随着世界继续朝着更强大的连接和更先进的技术发展，5G和边缘计算的部署变得越来越重要。5G和边缘计算在数据速度、可靠性和可扩展性方面提供了一系列可能性，使其成为一系列应用的理想选择。然而，为了最大限度地发挥其潜力，探索可用的不同部署选项至关重要。
5G和边缘计算最常见的部署选项是通过蜂窝网络。这涉及定期安装基站，为5G网络提供覆盖和容量。此选项非常适合需要高水平连接的人口稠密地区，因为它提供可靠的覆盖范围和数据速度。
另一种选择是通过固定无线系统部署5G和边缘计算。这涉及使用一系列天线和接收器来提供覆盖范围和容量。此部署选项非常适合无法安装基站的地区，例如农村地区。它还具有能够快速有效地覆盖广阔区域的优势。
第三个部署选项是使用网状网络。这涉及创建一个相互无线连接的节点网络。该选项非常适合在固定无线系统不可行的地区提供覆盖和容量，因为它可以在很短的时间内部署。
最后，5G和边缘计算也可以通过卫星部署。这涉及使用一系列卫星为偏远地区提供覆盖和容量。此选项非常适合提供广域覆盖，以及需要低延迟和高数据速度的应用。
随着5G和边缘计算的应用越来越广泛，探索可用的不同部署选项非常重要。每个选项都有自己的优点和缺点，因此必须考虑哪个选项最适合特定应用。通过正确的部署选项，5G和边缘计算可以为一系列应用提供可靠、高速的数据连接。
在 5G 和边缘计算环境中保护数据 5G 技术和边缘计算正在彻底改变我们访问和存储数据的方式。随着这些新技术变得越来越普遍，保护数据变得越来越重要。公司必须确保他们存储的数据不受任何恶意行为者的影响，无论他们是黑客、政府还是竞争对手。
为确保 5G 和边缘计算环境中的数据安全，企业需要结合使用加密、虚拟专用网络 (VPN) 和访问控制机制等技术。加密是对数据进行加扰的过程，以便它只能由授权用户读取。VPN 允许用户通过不安全的公共网络（例如 Internet）访问安全网络。访问控制机制有助于规范谁可以访问特定系统以及他们在那里可以做什么。
除了这些技术之外，公司还应该考虑使用数据治理和数据隐私计划。数据治理是在组织内管理、保护和利用数据的过程。数据隐私计划旨在保护组织存储和访问其数据的个人的隐私。公司还应确保定期更新和修补其系统，以确保快速解决任何漏洞。
最后，公司需要确保其员工接受数据安全最佳实践方面的培训。员工应了解数据安全的重要性以及为确保数据安全而需要采取的步骤。这包括使用强密码和避免访问公共网络上的机密数据。
通过实施这些措施，公司可以确保其数据在 5G 和边缘计算环境中安全可靠。这将有助于保护客户的隐私并防止恶意行为者滥用他们的数据。
为企业释放5G和边缘计算的潜力 5G网络和边缘计算的出现使企业能够释放这些技术的潜力，最大限度地提高效率和生产力。5G和边缘计算的结合可以为企业提供更快的数据访问速度、更高的可扩展性和更高的安全性。
借助5G网络，企业将能够更快、更可靠地访问数据，从而快速响应客户需求。此外，企业可以利用更高的速度来支持更复杂的应用程序和流程。通过利用边缘，企业可以减少延迟并提高其应用的可扩展性。边缘计算还有助于降低数据成本，因为它无需将数据发送到云端。
5G和边缘计算的结合，通过利用 5G 网络的更高速度和更高的可靠性，企业可以更快、更可靠地访问数据。此外，边缘计算有助于减少延迟并提高可扩展性。这些进步可以使企业改善运营并在全球市场上更具竞争力。通过利用5G和边缘计算的强大功能，企业可以释放这些技术的潜力，最大限度地提高效率和生产力。
]]></content>
  </entry>
  
  <entry>
    <title>边缘计算的5类最佳应用场景</title>
    <url>/post/news/5-best-use-cases-for-edge-computing.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>Edge</tag>
      <tag>Computing</tag>
    </tags>
    <content type="html"><![CDATA[当你认可时间等同于金钱或安全时；当你面对数据合规性问题时，边缘计算就是你最好的选择。本文将带给你5个边缘计算的应用场景，从而帮助大家思考如何进行边缘化设计。
开篇 边缘计算是指将基础设施定位在靠近数据产生或消费的地方。与其将数据推送到公共或私有云进行存储和计算，不如在 “边缘”进行就地处理，处理数据的基础设施可以是简单的商品服务器，也可以是复杂的平台，如AWS for the Edge、Azure Stack Edge或Google Distributed Cloud。
边缘计算的第二层含义，包括性能、可靠性、安全性和操作的合规性。为了支持这些要求边缘计算会将计算、存储和带宽转移到边缘的基础设施上执行，因为这些功能在集中式云架构上是不能执行的。
Edgevana的首席执行官Mark Thiele说：“边缘计算为企业领导者提供了一个新的途径，可以与客户和合作伙伴发展更深的关系，并获得实时的洞察力”。
当开发团队开发规模还不大，并处于概念验证的早期时，可能很难认识到最佳基础设施。但是，随着团队规模的扩大以及项目进度的推进，大家会逐渐认识到对边缘基础设施的需求，这就会迫使团队重新架构甚至重构应用程序。从而增加开发成本，放慢开发进度，甚至阻碍企业的交付。
随着应用程序变得越来越现代化和集成化，企业应该在开发的早期考虑边缘技术和集成，以防止开发企业级应用程序时出现的性能和安全挑战。Devops团队应该在平台的基础设施要求被准确建模之前寻找响应的指标。以下是考虑边缘的五个理由。
提高效率与安全性 在制造业，当延迟可能导致工人受伤时，几秒钟的价值是什么？如果制造需要昂贵的材料，而提前几百毫秒发现缺陷可以节省大量资金，那又如何呢？
在制造业中，有效利用边缘计算可以减少浪费，提高效率，减少工伤，并提高设备的可用性。
架构师要考虑的一个关键因素是由于决策失败或延迟而导致的失败成本。如果存在重大风险或成本，如制造系统、手术平台或自动驾驶汽车，边缘计算可能为需要更大安全性的应用提供更高的性能和可靠性。
减少延时 亚秒级的响应时间是大多数金融交易平台的基本要求，现在许多应用都希望有这样的性能，缩短从感觉到发现问题的时间，缩短发现机会到做出行动的时间，总之在不断加速决定的周期。
咨询公司的高级副总裁Amit Patel说：“如果实时决策对你的业务很重要，那么提高速度或减少延迟就很关键，特别是在企业使用所有连接设备收集数据的情况下”。
当有成千上万的数据源和决策节点时，提供低延迟技术就显得尤为重要。这方面的例子包括连接数以千计的拖拉机和农场机器，并在边缘设备上部署机器学习（ML），或实现元数据或其他大规模企业对消费者的体验。
如果需要实时采取行动，就从边缘计算开始，“Akamai高级产品经理Pavel Despot说。”边缘基础设施适合于低延迟、高弹性和高吞吐量的应用场景，从而处理分布在不同地理位置用户的工作负载，这一技术涉及到流媒体、银行、电子商务、物联网设备等不同领域。
LaunchDarkly的开发者关系总监Cody De Arkland表示，在全球都分布有办公地点的企业或支持大规模的混合工作的企业就是一个典型的例子。边缘工作的价值在于，你能将工作分配到离你最近的人身上，这些人会对工作进行分担。如果应用程序对数据传输时间敏感的话，你应该考虑边缘基础设施，并考虑哪些工作应该在边缘运行。
提高应用程序的可靠性 Scale Computing的首席执行官Jeff Ready表示，我们看到制造业、零售业和运输业对边缘基础设施的兴趣很大，在这些行业中，设备根本不可能停机，数据的实时访问和利用数据的需求已经成为差异化竞争的要素。
因此，当停机成本高，维修时间长，以及集中式基础设施故障影响多个业务时，应考虑边缘基础设施。
Ready分享了两个例子。例如在海洋中间的一艘货船，它不能依靠断断续续的卫星连接来运行其船上系统，或者一家杂货店需要从店内收集数据来创造个性化的购物体验。如果一个集中式系统发生故障，可能会影响到多艘船和物流，而高度可靠的边缘基础设施可以减少停机的风险和造成的影响。
本地数据处理和法规支持 如果性能、延迟和可靠性不是主要的设计考虑因素，那么根据有关数据收集和消费地点的规定，可能仍然需要边缘基础设施的支持。
AWS物联网副总裁Yasser Alsaied认为，边缘基础设施对本地数据处理和数据驻留要求很重要。例如，它有利于那些远程操作工作负载的公司，这些公司由于连接性的原因而无法将数据上传到云端，该企业的特点是数据会驻留在某个特定的区域内，并对数据进行高度管制，或者拥有需要本地处理的大量数据。
开发团队应该回答的一个基本问题是，数据将在哪里被收集和消费？合规部门应提供关于数据限制的监管指南，并应就物理和地理限制咨询运营职能部门的领导。
对大数据集带宽的成本优化 带有视频监控、设施管理系统和能源跟踪系统的智能建筑，都会以每秒的速度捕获大量的数据。在建筑中本地处理这些数据比在云端集中处理数据要便捷得多。
ScaleFlux的营销副总裁JB Baker表示，所有行业都在经历数据的激增，要适应这种复杂性，需要一种完全不同的思维方式来利用巨大数据集的潜力。边缘计算是解决方案的一部分，因为它使计算和存储更接近数据的起源。
MinIO的首席执行官和联合创始人AB Periasamy提出了这样的建议：“随着数据在网络边缘的产生，在应用和基础设施架构方面产生了独特的挑战。将带宽作为模型中成本最高的项目，而资本和运营支出在边缘的运作方式有所不同。”
总之，当开发团队看到应用程序需要在性能、可靠性、延迟、安全、监管或规模方面的优势时，那么在开发的早期对边缘基础设施进行建模可以考虑更智能的架构。
]]></content>
  </entry>
  
  <entry>
    <title>自制STM32的下载器</title>
    <url>/post/hardware/smt32-download-fixture-how-to.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>SMT32, STLINK-V2</tag>
    </tags>
    <content type="html"><![CDATA[本文介绍制作一个STM32下载器的过程。
原理图 STLINK-V2下载器电路原理图如下。
上图中，H5接口是固件下载口。H4接口是STLINK-V2下载口（实现下载功能的接口 T_JTCK就是SWCLK, T_JTMS就是 SWDIO）。其他都是测试接口。
制作过程 首先我们焊接完的板子是不能用的，需要往里面下载固件。前提是你手头有一块好的ST-LinkV2下载器，不然就白搞了。
使用好的STLINK-V2下载器给我们自己制作的STLINK-V2板子下载固件，将STLINK-V2下载器连接角与板子上的H5固件下载接口对应连接，再将STLINK-V2连接到电脑，
安装ST的ST-LINK Utility软件，使用STM32 ST-LINK Utility 软件下载固件（这个软件官网可以下载）。先连接后下载，要是软件识别不了你的芯片，那你得仔细检查下你的板子啦！
连接成功后，然后开始下载固件。
图中框1打开固件STLinkV2.J16.S4.bin文件（这个最新固件官网可以下载的到）。
点击框2下载，弹出下载窗口。
点击框3开始下载。
下载完后，将自制的ST-Link插上电脑，然后，更新固件。点击框1，弹出窗口然后点击框2连接自制的ST-Link,识别出来后，点击框3开始更新固件。如果没有识别出来，可能是你同时插上了2个ST-Link，或者是你的板子有问题，得耐心排查。到这里，就算完成啦！接下来你可以体验你自制的ST-Link任意下载程序啦！这个电路我测试的是keil，IAR都支持，STM8也能下载。
]]></content>
  </entry>
  
  <entry>
    <title>PCB板上镀金与镀银有什么区别</title>
    <url>/post/hardware/difference-between-gold-plating-and-silver-plating-on-PCB.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>PCB, Gold Plating, Silver Plating</tag>
    </tags>
    <content type="html"><![CDATA[很多DIY玩家会发现，市场中各种各样的板卡产品所使用的PCB颜色五花八门，令人眼花缭乱。比较常见的PCB颜色有黑色、绿色、蓝色、黄色、紫色、红色、棕色。
除此之外，一些厂商还别出心裁地开发了白色、粉色等不同色彩的PCB。
在传统的印象中，黑色PCB似乎定位着高端，而红色、黄色等则是低端专用，那是不是这样呢？
没有涂覆阻焊漆的PCB铜层暴露在空气中极易氧化 我们知道PCB正反两面都是铜层，在PCB的生产中，铜层无论采用加成法还是减成法制造，都会得到光滑无保护的表面。
铜的化学性质虽然不如铝、铁、镁等活泼，但在有水的条件下，纯铜和氧气接触极易被氧化；因为空气中存在氧气和水蒸气，所以纯铜表面在和空气接触后很快会发生氧化反应。
由于PCB中铜层的厚度很薄，因此氧化后的铜将成为电的不良导体，会极大地损害整个PCB的电气性能。
为了阻止铜氧化，也为了在焊接时PCB的焊接部分和非焊接部分分开，还为了保护PCB表层，工程师们发明了一种特殊的涂料。这种涂料能够轻松涂刷在PCB表面，形成具有一定厚度的保护层，并阻断铜和空气的接触。这层涂层叫做阻焊层，使用的材料为阻焊漆。
既然叫漆，那肯定有不同的颜色。没错，原始的阻焊漆可以做成无色透明的，但PCB为了维修和制造方便，往往需要在板上面印制细小的文字。
透明阻焊漆只能露出PCB底色，这样无论是制造、维修还是销售，外观都不够好看。因此工程师们在阻焊漆中加入了各种各样的颜色，就形成了黑色或者红色、蓝色的PCB。
黑色的PCB难以看清走线，为维修带来了困难 从这一点来看，PCB的颜色和PCB的质量是没有任何关系的。黑色的PCB和蓝色PCB、黄色PCB等其他颜色PCB的差别在于刷上的阻焊漆颜色不同。
如果PCB设计、制造过程完全一样，颜色不会对性能产生任何影响，也不会对散热产生任何影响。
关于黑色的PCB，由于其表层走线几乎全部遮住，导致对后期的维修造成很大困难，所以是不太方便制造和使用的一种颜色。
因此近年来人们渐渐改革，放弃使用黑色阻焊漆，转而使用深绿色、深棕色、深蓝色等阻焊漆，目的就是为了方便制造和维修。
说到这里，大家已经基本清楚了PCB颜色的问题。关于之所以出现“颜色代表或低档”的说法，那是因为厂商喜爱使用黑色PCB来制造高端产品，用红色、蓝色、绿色、黄色等制造低端产品所导致。
总结一句话就是：产品赋予了颜色含义，而不是颜色赋予了产品含义。
金、银等贵金属用在PCB上有什么好处？ 颜色说清楚了，再来说说PCB上的贵重金属！一些厂商在宣传自己的产品时，会特别提到自己的产品采用了镀金、镀银等特殊工艺。那么这种工艺究竟有什么用处呢？
PCB表面需要焊接元件，就要求有一部分铜层暴露在外用于焊接。这些暴露在外的铜层被称为焊盘，焊盘一般都是长方形或者圆形，面积很小。
在上文中，我们知道PCB中使用的铜极易被氧化，因此刷上了阻焊漆后，暴露在空气中的就是焊盘上的铜了。
如果焊盘上的铜被氧化了，不仅难以焊接，而且电阻率大增，严重影响终产品性能。所以，工程师们才想出了各种各样的办法来保护焊盘。比如镀上惰性金属金，或在表面通过化学工艺覆盖一层银，或用一种特殊的化学薄膜覆盖铜层，阻止焊盘和空气的接触。
PCB上暴露出来的焊盘，铜层直接裸露在外。这部分需要保护，阻止它被氧化。
从这个角度来说，无论是金还是银，工艺本身的目的都是阻止被氧化、保护焊盘，使其在接下来的焊接工艺中确保良品率。
不过采用不同的金属，会对生产工厂使用的PCB的存放时间和存放条件提出要求。因此PCB厂一般会在PCB生产完成，交付客户使用前，利用真空塑封机器包装PCB，限度地确保PCB不发生氧化损害。
而在元件上机焊接之前，板卡生产厂商还要检测PCB的氧化程度，剔除氧化PCB，保证良品率。终消费者拿到的板卡，是已经过了各种检测，即使长时间使用后的氧化也几乎只会发生在插拔连接部位，且对焊盘和已经焊接好的元件，没有什么影响。
由于银和金的电阻更低，那么在采用了银和金等特殊金属后，会不会减少PCB使用时的发热量呢？
我们知道，影响发热量的因素是电阻；电阻又和导体本身材质、导体的横截面积、长度相关。焊盘表面金属材质厚度甚至远低于0.01毫米，如果采用OST（有机保护膜）方式处理的焊盘，根本不会有多余厚度产生。如此微小的厚度所表现出来的电阻几乎为0，甚至无法计算，当然不会影响到发热量了。
]]></content>
  </entry>
  
  <entry>
    <title>dBm-Vpp-W转换</title>
    <url>/post/hardware/dBm-Vpp-convert.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>dBm, Vpp</tag>
    </tags>
    <content type="html"><![CDATA[在不同的应用领域，大家对一个信号的表示各不相同，如RF领域喜欢使用dBm来描述一个信号的功率，而实际的调试或测试过程中，大家更喜欢使用示波器来测试一个信号的峰峰值
所以工程实践中经常需要对各个单位进行转换，如dBm转Vpp，dBm转W(功率瓦特)。本文就是介绍如何进行这些转换。  典型的测试测量系统 如下是一个典型的射频源和频谱仪/示波器组成的一个测量系统，射频源等效输出阻抗Rs为50欧姆，输出一个标准正弦波，频谱仪/示波器等效输入阻抗RL为50欧姆，则频谱仪测试到的正弦波功率为一个以dBm为单位的功率值，而示波器则测试到的是落在输入50欧姆电阻上的Vpp电压值：
频谱仪看到的功率值为：
$$ P_{dBm} = 10*lg \left(\frac{P_{w}}{1mW}\right) $$
示波器看到的电压值为：
$$ V_{pp} = 20* \sqrt{P_{w}} $$
dBm-Vpp-W转换表 下表是50欧姆系统中的正弦波信号参数的转换关系：
小程序实现 使用电路设计小程序可以很方便的实现dBm-Vpp-W的转换：
]]></content>
  </entry>
  
  <entry>
    <title>动图演示常用通信协议原理</title>
    <url>/post/hardware/animation-demonstrates-principles-of-common-communication-protocols.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>SPI</tag>
      <tag>UART</tag>
      <tag>I2C</tag>
      <tag>PWM</tag>
    </tags>
    <content type="html"><![CDATA[本文分享电子系统中信号波形的动图，有助于帮助我们理解传输的机理。
SPI传输 图1 SPI 数据传输
图1.2 SPI数据传输（2）
图1.3 SPI时序信号
I²C传输 图1.2.1 I2C总线以及寻址方式
UART传输 图1.3.1 PC 上通过UART来调试MCU
图1.3.2 RS-232通过电平转换芯片与MCU通讯
串口通信相关文章: VxWorks下的串口测试程序设计和源码  
红外控制 图1.4.1 红外控制信号也是一个串行通讯信号
图1.4.2 红外信号接收与放大整形电路
图1.4.3 一个使用红外接收光电管控制继电器进行鱼食投喂电路
串并转换电路 图1.5.1 串入、并出移位寄存器
图1.5.2 由八个D寄存器组成的移位寄存器
图1.5.3 串行传输示意图
其他波形动画 图1.6.1 PWM控制LED亮度
图1.6.2 PWM控制LED亮度
图1.6.3 调幅与调频信号
图1.6.4 相位调制信号
图1.6.5 方波边沿抖动波形
原文链接: 动图演示常用通信协议原理  
]]></content>
  </entry>
  
  <entry>
    <title>Linux 进程概念: 冯 • 诺依曼体系结构</title>
    <url>/post/linux/linux-process-concept-von-neumann-architecture.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Process</tag>
      <tag>Von Neumann Architecture</tag>
    </tags>
    <content type="html"><![CDATA[在 1945 年冯诺依曼和其他计算机科学家们提出了计算机具体实现的报告，其遵循了[图灵机]的设计，而且还提出用电子元件构造计算机，并约定了用二进制进行计算和存储。
冯诺依曼体系结构 冯诺依曼体系，最重要的是定义计算机基本结构为 5 个部分，分别是[运算器]、控制器、存储器、输入设备、输出设备，这 5 个部分也被称为冯诺依曼模型。
下图为冯 • 诺依曼体系结构流程图：
运算器、控制器是在中央处理器里的，存储器就是我们常见的内存，输入输出设备则是计算机外接的设备，比如键盘就是输入设备，显示器就是输出设备。 
我们常见的计算机，如笔记本。我们不常见的计算机，如服务器，大部分都遵守冯诺依曼体系，截至目前，我们所认识的计算机，都是有一个个的硬件组件组成。
存储单元和输入输出设备要与中央处理器打交道的话，离不开总线。所以，它们之间的关系如下图：
接下来，分别介绍内存、中央处理器、总线、输入输出设备。
输入、输出设备 输入设备向计算机输入数据，计算机经过计算后，把数据输出给输出设备。  常见的输入和输出设备有：
 输入设备：键盘，话筒，摄像头，磁盘，网卡等等… 输出设备：显示器，音响，磁盘，网卡，显卡等等…  注意：同种设备在不同场景下可能属于输入设备，也可能属于输入设备。
中央处理器 中央处理器也就是我们常说的 CPU，它是由运算器和控制器组成。
CPU 内部还有一些组件，常见的有寄存器、控制单元 和 逻辑运算单元 等。其中，控制单元负责控制 CPU 工作，逻辑运算单元负责计算，而寄存器可以分为多种类，每种寄存器的功能又不尽相同。
CPU 中的寄存器主要作用是存储计算时的数据，你可能好奇为什么有了内存还需要寄存器？原因很简单，因为内存离 CPU 太远了，而寄存器就在 CPU 里，还紧挨着控制单元和逻辑运算单元，自然计算时速度会很快。
常见的寄存器种类：
 通用寄存器，用来存放需要进行运算的数据，比如需要进行加和运算的两个数据。 程序计数器，用来存储 CPU 要执行下一条指令「所在的内存地址」，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令「的地址」。 指令寄存器，用来存放当前正在执行的指令，也就是指令本身，指令被执行完成之前，指令都存储在这里。  内存 内存，也就是所谓的存储器。
我们的程序和数据都是存储在内存，存储的区域是线性的。
在计算机数据存储中，存储数据的基本单位是字节（byte），1 字节等于 8 位（8 bit）。每一个字节都对应一个内存地址。
内存的地址是从 0 开始编号的，然后自增排列，最后一个地址为内存总字节数 - 1，这种结构好似我们程序里的数组，所以内存的读写任何一个数据的速度都是一样的。
思考一个问题： 当我们的体系结构中，有了输入、输出设备和 CPU 以后，就能正常工作了，那么为什么还需要内存呢？
从技术角度来说 CPU 的运算速度 &gt; &raquo; 寄存器的速度 &gt; &raquo; L1~L3Cache &gt; &raquo; 内存 &gt; &raquo; 外设（磁盘）&gt; &raquo; 光盘磁带
也就是说，输入设备和输出设备相对于 CPU 来说是非常慢的。
如果没有内存的话，那么当前这个体系整体呈现出来的就是：输入设备和输出设备很慢，而 CPU 很快。
相信大家知道木桶原理吧，那么最终整个体系结构所呈现出来的速度将会是很慢的。
所以，从数据角度出发，外设几乎不和 CPU 打交道，它是直接和内存打交道，CPU 也同样如此。
进言之，内存在我们看来，就是体系结构的一个大的缓存，用来适配外设和 CPU 速度不均的问题！
从成本角度来说 既然上面说了内存是用来适配外设和 CPU 速度不均的问题，那么为什么不直接在 CPU 里面开发一个类似于内存的东西呢？
这个想法可以，但是如果真要去实现的话，那么一台计算机的成本起码得 10W+，而计算机它是蔓延全世界的，也就是说人人都能用得起的！
寄存器的价格 &gt; &raquo; 内存 &gt; &raquo; 外设 (磁盘)
所以内存就是方便我们使用较低的成本，获得较高的性能。
总线 总线是用于 CPU 和内存以及其他设备之间的通信，总线可分为 3 种：   地址总线，用于指定 CPU 将要操作的内存地址； 数据总线，用于读写内存的数据； 控制总线，用于发送和接收信号，比如中断、设备复位等信号，CPU 收到信号后自然进行响应，这时也需要控制总线；  当 CPU 要读写内存数据的时候，一般需要通过下面这三个总线：
 首先要通过「地址总线」来指定内存的地址； 然后通过「控制总线」控制是读或写命令； 最后通过「数据总线」来传输数据；  局部性原理 我相信大家应该还有个疑惑：就是，先将输入设备的数据交给内存，再由内存将数据交给 CPU，这个过程真的比 CPU 直接从输入设备获取数据更快吗？
说明这个问题之前，我们首先需要知道：内存具有数据存储的能力。虽然内存的大小只有 4G/8G，但是既然内存有大小，那么它就有预装数据的能力，而这就是提高该体系结构效率的秘诀。
这里不得不说到的就是 局部性原理：根据统计学原理，当一个数据正在被访问时，那么下一次有很大可能会访问其周围的数据。所以当 CPU 需要获取某一行数据时，内存可以将该行数据之后的数据一同加载进来，而 CPU 处理数据和内存加载数据是可以同时进行的，这样下次 CPU 就可以直接从内存当中获取数据。
输出数据的时候也一样，CPU 处理完数据后直接将数据放到内存当中，当输出设备需要时再在内存当中获取即可，这也就有了我们平常所说的缓冲区的概念。
例如，缓冲区满了才将数据打印到屏幕上，使用 fflush 函数将缓冲区当中的数据直接输出之类的，都是将内存当中的数据直接拿到输出设备当中进行显示输出。
总结 冯 • 诺依曼体系结构核心原理为：用户输入的数据先放到内存当中，CPU 读取数据的时候就直接从内存当中读取，CPU 处理完数据后又写回内存当中，然后内存再将数据输出到输出设备当中，最后由输出设备进行输出显示。
我们可以知道，站在硬件角度或是数据层面上，CPU 和外设不能直接交互，而是通过内存，也就是说，所有设备都只能和内存打交道。
由此可以说明一个问题：为什么程序运行之前必须先加载到内存？
因为可执行程序（文件）是在硬盘（外设）上的，而 CPU 只能从内存当中获取数据，所以必须先将硬盘上的数据加载到内存，也就是必须先将程序加载到内存。
数据的流动过程 对冯诺依曼的理解，不能停留在概念上，要深入到对软件数据流理解上。
从你登录上 QQ 和某位朋友聊天开始，数据的流动过程是怎样的呢？从你打开窗口，开始给他发消息，到他的到消息之后的数据流动过程。
要使用 QQ，首先需要联网，而你和你的朋友的电脑都是冯诺依曼体系结构，在你向朋友发送消息这个过程中，你的电脑当中的键盘充当输入设备，显示器和网卡充当输出设备，你朋友的电脑当中的网卡充当输入设备，显示器充当输出设备。
刚开始你在键盘当中输入消息，键盘将消息加载到内存，此时你的显示器就可以从内存获取消息进而显示在你自己的显示器上，此时你就能在你自己的电脑上看到你所发的消息了。
在键盘将消息加载到内存后，CPU 从内存获取到消息后对消息进行各种封装，然后再将其写回内存，此时你的网卡就可以从内存获取已经封装好的消息，然后在网络当中经过一系列处理（这里忽略网络处理细节）。
之后你朋友的网卡从网络当中获取到你所发的消息后，将该消息加载到内存当中，你朋友的 CPU 再从内存当中获取消息并对消息进行解包操作，然后将解包好的消息写回内存，最后你朋友的显示器从内存当中获取消息并显示在他的电脑上。
那么如果是在 QQ 上发送文件呢？
首先你的文件最开始是在你本地的磁盘上的，先从磁盘上把文件读到内存中，文件里面的东西其实还是数据，把数据再经过 CPU 封装成报文，然后刷新到我们的内存中，定期再经过网卡，把数据刷新到网卡上，然后再发出去。
传文件的本质就是：两端的磁盘进行通信。 ]]></content>
  </entry>
  
  <entry>
    <title>40个简单但有效的Linux Shell脚本示例</title>
    <url>/post/linux/40-simple-and-useful-linux-shell-script.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>shell script</tag>
    </tags>
    <content type="html"><![CDATA[历史上，shell一直是类Unix系统的本地命令行解释器。它已被证明是Unix的主要功能之一，并发展成为一个全新的主题。Linux提供了各种功能强大的shell，包括Bash、Zsh、Tcsh和Ksh。这些外壳最令人惊讶的特性之一是其可编程性。创建简单而有效的Linux shell脚本来处理日常工作非常容易。
Hello World 程序员经常通过学习hello world程序来学习新语言。这是一个简单的程序，将字符串“HelloWorld”打印到标准输出中。然后，使用vim或nano等编辑器创建hello-world.sh文件，并将以下行复制到其中。
#!/bin/bash echo &#34;Hello World&#34; 保存并退出文件。接下来，您需要使用以下命令使该文件可执行。
$ chmod a+x hello-world.sh 可以使用以下两个命令中的任何一个来运行此命令。
$ bash hello-world.sh $ ./hello-world.sh 它将打印出传递给脚本内部回显的字符串。
使用echo打印 echo命令用于在bash中打印信息。它类似于C函数“printf”，并提供了许多常见选项，包括转义序列和重定向。将以下行复制到名为echo.sh的文件中，并使其可执行，如上所述。
#!/bin/bash echo &#34;Printing text&#34; echo -n &#34;Printing text without newline&#34; echo -e &#34;\nRemoving \t special \t characters\n&#34; 运行脚本以查看其功能。-e选项用于告诉echo传递给它的字符串包含特殊字符，需要扩展功能。
使用注释 注释对文档很有用，是高质量代码库的要求。将注释放在处理关键逻辑的代码中是一种常见的做法。要注释掉一行，只需在其前面使用#（hash）字符。例如，请查看下面的bash脚本示例。
#!/bin/bash  # Adding two values ((sum=25+35)) #Print the result echo $sum 此脚本将输出数字60。首先，在某些行之前使用#检查注释的使用方式。不过，第一行是一个例外。它被称为shebang，让系统知道在运行这个脚本时要使用哪个解释器。
多行注释 许多人使用多行注释来记录他们的shell脚本。在下一个名为comment.sh的脚本中检查这是如何完成的。
#!/bin/bash : &#39; This script calculates the square of 5. &#39; ((area=5*5)) echo $area 注意多行注释是如何放置在内部的：“和”字符。
While循环 while循环构造用于多次运行某些指令。查看以下名为while.sh的脚本，以更好地理解此概念。
#!/bin/bash #!/bin/bash i=0 while [ $i -le 2 ] do echo Number: $i ((i++)) done 因此，while循环采用以下形式。
#!/bin/bash while [ condition ] do commands 1 commands n done 方括号周围的空格是必填的。
For循环 for循环是另一种广泛使用的bashshell构造，它允许用户高效地迭代代码。下面演示了一个简单的示例。
#!/bin/bash  for (( counter=1; counter&lt;=10; counter++ )) do echo -n &#34;$counter&#34; done printf &#34;\n&#34; 接收用户输入 #!/bin/bash  echo -n &#34;Enter Something:&#34; read something echo &#34;You Entered: $something&#34; If语句 if CONDITION then STATEMENTS fi 只有当条件为真时，才会执行这些语句。fi关键字用于标记if语句的结尾。下面显示了一个快速示例。
#!/bin/bash  echo -n &#34;Enter a number: &#34; read num if [[ $num -gt 10 ]] then echo &#34;Number is greater than 10.&#34; fi 如果通过输入提供的数字大于10，上述程序将仅显示输出。-gt表示大于；类似地-lt表示小于-le表示小于等于；且-ge表示大于等于。此外，还需要[[]]。
使用If Else进行更多控制 将else构造与if结合起来，可以更好地控制脚本的逻辑。下面显示了一个简单的示例。
#!/bin/bash  read n if [ $n -lt 10 ]; then echo &#34;It is a one digit number&#34; else echo &#34;It is a two digit number&#34; fi 其他部分需要放在if的动作部分之后和fi之前。
使用AND运算符 AND运算符允许我们的程序检查是否同时满足多个条件。由AND运算符分隔的所有部分必须为true。否则，包含AND的语句将返回false。查看下面的bash脚本示例，以更好地了解AND的工作原理。
#!/bin/bash  echo -n &#34;Enter Number:&#34; read num if [[ ( $num -lt 10 ) &amp;&amp; ( $num%2 -eq 0 ) ]]; then echo &#34;Even Number&#34; else echo &#34;Odd Number&#34; fi AND运算符由&amp;&amp;符号表示。
使用OR运算符 OR运算符是另一个关键的构造，它允许我们在脚本中实现复杂、健壮的编程逻辑。与AND相反，当OR运算符的任一操作数为真时，由OR运算符组成的语句返回真。仅当由OR分隔的每个操作数为假时，它才返回假。
#!/bin/bash  echo -n &#34;Enter any number:&#34; read n if [[ ( $n -eq 15 || $n -eq 45 ) ]] then echo &#34;You won&#34; else echo &#34;You lost!&#34; fi 这个简单的示例演示了OR运算符如何在Linuxshell脚本中工作。只有当用户输入数字15或45时，它才会宣布用户为获胜者。||符号表示OR运算符。
使用El if elif语句代表else if，并为实现链逻辑提供了一种方便的方法。通过评估以下示例，了解elif的工作原理。
#!/bin/bash  echo -n &#34;Enter a number: &#34; read num if [[ $num -gt 10 ]] then echo &#34;Number is greater than 10.&#34; elif [[ $num -eq 10 ]] then echo &#34;Number is equal to 10.&#34; else echo &#34;Number is less than 10.&#34; fi 上面的程序是不言自明的，所以我们不会逐行剖析它。相反，更改脚本中的变量名称和值等部分，以检查它们如何一起工作。
case 条件 switch构造是Linux bash脚本提供的另一个强大功能。它可以用于需要嵌套条件的地方，但不希望使用复杂的if-else elif链。看看下一个例子。
#!/bin/bash  echo -n &#34;Enter a number: &#34; read num case $num in 100) echo &#34;Hundred!!&#34; ;; 200) echo &#34;Double Hundred!!&#34; ;; *) echo &#34;Neither 100 nor 200&#34; ;; esac 条件写在case和esac关键字之间。*）用于匹配除100和200以外的所有输入。
命令行参数 在许多情况下，直接从命令shell获取参数是有益的。下面的示例演示了如何在bash中执行此操作。
#!/bin/bash echo &#34;Total arguments : $#&#34; echo &#34;First Argument = $1&#34; echo &#34;Second Argument = $2&#34; 运行此脚本时，在其名称后添加两个附加参数。我将其命名为test.sh，调用过程概述如下。
$ ./test.sh Hey Howdy 因此，$1用于访问第一个参数，$2用于访问第二个参数，依此类推。最后，$#用于获取参数总数。
使用名称获取参数 下面的示例显示了如何获取带有名称的命令行参数。
#!/bin/bash  for arg in &#34;$@&#34; do index=$(echo $arg | cut -f1 -d=) val=$(echo $arg | cut -f2 -d=) case $index in X) x=$val;; Y) y=$val;; *) esac done ((result=x+y)) echo &#34;X+Y=$result&#34; 将此脚本命名为test.sh，并按如下所示调用它。
$ ./test.sh X=44 Y=100 它应该返回X+Y=144。这里的参数存储在“$@”中，脚本使用Linuxcut命令获取它们。
连接字符串 字符串处理对于广泛的现代bash脚本来说非常重要。值得庆幸的是，它在bash中更加舒适，并允许以更精确、简洁的方式实现这一点。请参见下面的示例，了解bash字符串连接。
#!/bin/bash  string1=&#34;Ubuntu&#34; string2=&#34;Pit&#34; string=$string1$string2 echo &#34;$stringis a great resource for Linux beginners.&#34; 字符串截取 与许多编程语言不同，bash不提供任何用于剪切字符串部分的内置函数。然而，下面的示例演示了如何使用参数展开来实现这一点。
#!/bin/bash Str=&#34;Learn Bash Commands from UbuntuPit&#34; subStr=${Str:0:20} echo $subStr 该脚本应打印出“学习Bash命令”作为其输出。参数展开形式为${VAR_NAME:S:L}。这里，S表示起始位置，L表示长度。
使用cut 做截取 可以在脚本中使用Linux cut命令来截取字符串的一部分，也就是子字符串。下一个示例显示了如何做到这一点。
#!/bin/bash Str=&#34;Learn Bash Commands from UbuntuPit&#34; #subStr=${Str:0:20} subStr=$(echo $Str| cut -d &#39; &#39; -f 1-3) echo $subStr 添加两个值 在Linux shell脚本中执行算术运算非常容易。下面的示例演示了如何从用户接收两个数字作为输入并将它们相加。
#!/bin/bash echo -n &#34;Enter first number:&#34; read x echo -n &#34;Enter second number:&#34; read y (( sum=x+y )) echo &#34;The result of addition=$sum&#34; 如您所见，在bash中添加数字相当简单。
添加多个值 您可以使用循环获取多个用户输入并将其添加到脚本中。以下示例显示了这一点。
#!/bin/bash sum=0 for (( counter=1; counter&lt;5; counter++ )) do echo -n &#34;Enter Your Number:&#34; read n (( sum+=n )) #echo -n &#34;$counter &#34; done printf &#34;\n&#34; echo &#34;Result is: $sum&#34; 但是，省略(())将导致字符串串联而不是相加。所以，在你的程序中检查类似的情况。
Bash中的函数 与任何编程方言一样，函数在Linux shell脚本中扮演着重要角色。它们允许管理员创建自定义代码块以供频繁使用。下面的演示将概述函数如何在Linux bash脚本中工作。
#!/bin/bash function Add() { echo -n &#34;Enter a Number: &#34; read x echo -n &#34;Enter another Number: &#34; read y echo &#34;Adiition is: $(( x+y ))&#34; } Add 这里我们像以前一样添加了两个数字。但在这里，我们使用了一个名为Add的函数来完成这项工作。因此，每当您需要再次添加时，只需调用此函数，而不必再次编写该部分。
具有返回值的函数 最神奇的功能之一是允许数据从一个函数传递到另一个函数。它在各种场景中都很有用。查看下一个示例。
#!/bin/bash  function Greet() { str=&#34;Hello $name, what brings you to UbuntuPit.com?&#34; echo $str } echo &#34;-&gt; what&#39;s your name?&#34; read name val=$(Greet) echo -e &#34;-&gt; $val&#34; 这里，输出包含从Greet（）函数接收的数据。
从Bash脚本创建目录 使用shell脚本运行系统命令的能力使开发人员的工作效率大大提高。下面的简单示例将向您展示如何在shell脚本中创建目录。
#!/bin/bash echo -n &#34;Enter directory name -&gt;&#34; read newdir cmd=&#34;mkdir $newdir&#34; eval $cmd 该脚本只需调用标准shell命令mkdir，并在仔细查看时将目录名传递给它。这个程序应该在文件系统中创建一个目录。您还可以传递命令以在backticks（“）内部执行，如下所示。
`mkdir $newdir` 确认存在后创建目录 如果当前工作目录中已包含同名文件夹，则上述程序将无法运行。例如，下面的程序将检查是否存在名为$dir的文件夹，如果找不到，则只创建一个。
#!/bin/bash echo -n &#34;Enter directory name -&gt;&#34; read dir if [ -d &#34;$dir&#34; ] then echo &#34;Directory exists&#34; else `mkdir $dir` echo &#34;Directory created&#34; fi 使用eval编写此程序以提高bash脚本编写技能。 读取文件 Bash脚本允许用户非常有效地读取文件。下面的示例将展示如何使用shell脚本读取文件。首先，创建一个名为editors.txt的文件，其中包含以下内容。
1. Vim 2. Emacs 3. ed 4. nano 5. Code 此脚本将输出上述5行中的每一行。
#!/bin/bash file=&#39;editors.txt&#39; while read line; do echo $line done &lt; $file 删除文件 以下程序将演示如何在Linux shell脚本中删除文件。程序将首先要求用户提供文件名作为输入，如果文件名存在，则将其删除。Linux rm命令在此处执行删除操作。
#!/bin/bash echo -n &#34;Enter filename -&gt;&#34; read name rm -i $name 让我们输入editors.txt作为文件名，并在要求确认时按y。它应该删除该文件。
附加到文件 下面的shell脚本示例将向您展示如何使用bash脚本将数据附加到文件系统上的文件。它向早期的editors.txt文件添加了一行。
#!/bin/bash echo &#34;Before appending the file&#34; cat editors.txt echo &#34;6. NotePad++&#34; &gt;&gt; editors.txt echo &#34;After appending the file&#34; cat editors.txt 现在您应该注意到，我们直接从Linux bash脚本使用日常终端命令。
测试文件存在 下一个shell脚本示例显示如何检查bash程序中文件的存在。
#!/bin/bash filename=$1 if [ -f &#34;$filename&#34; ]; then echo &#34;File exists&#34; else echo &#34;File does not exist&#34; fi 我们直接从命令行传递文件名作为参数。
从Shell脚本发送邮件 从bash脚本发送电子邮件非常简单。下面的简单示例将演示一种从bash应用程序执行此操作的方法。
#!/bin/bash recipient=”admin@example.com” subject=”Greetings” message=”Welcome to UbuntuPit” `mail -s $subject $recipient &lt;&lt;&lt; $message` 它将向收件人发送包含给定主题和消息的电子邮件。
解析日期和时间 下一个bash脚本示例将向您展示如何使用脚本处理日期和时间。同样，Linuxdate命令用于获取必要的信息，我们的程序执行解析。
#!/bin/bash year=`date +%Y` month=`date +%m` day=`date +%d` hour=`date +%H` minute=`date +%M` second=`date +%S` echo `date` echo &#34;Current Date is: $day-$month-$year&#34; echo &#34;Current Time is: $hour:$minute:$second&#34; 运行此程序以了解其工作原理。此外，尝试从终端运行date命令。
sleep命令 sleep命令允许shell脚本在指令之间暂停。它在许多场景中都很有用，例如执行系统级作业。下一个示例显示了shell脚本中的sleep命令。
#!/bin/bash echo &#34;How long to wait?&#34; read time sleep $time echo &#34;Waited for $timeseconds!&#34; 该程序暂停最后一条指令的执行，直到$time秒，在本例中，用户提供了这一点。
wait命令 wait命令用于暂停Linux bash脚本中的系统进程。查看下面的示例，详细了解这在bash中的工作方式。
#!/bin/bash echo &#34;Testing wait command&#34; sleep 5 &amp; pid=$! kill $pid wait $pid echo $pid was terminated. 显示上次更新的文件 有时，您可能需要为某些操作查找最后更新的文件。下面的简单程序向我们展示了如何在bash中使用awk命令执行此操作。它将列出当前工作目录中最近更新或创建的文件。
#!/bin/bash  ls -lrt | grep ^- | awk &#39;END{print $NF}&#39; 为了简单起见，我们将避免在本示例中描述awk的功能。相反，您可以简单地复制此代码来完成任务。
添加批处理扩展 下面的示例将对目录中的所有文件应用自定义扩展名。创建一个新目录，并将一些文件放在其中以供演示。我的文件夹共有五个文件，每个文件名为test，后跟（0-4）。我已将此脚本编程为在文件末尾添加（.UP）。您可以添加所需的任何扩展名。
#!/bin/bash dir=$1 for file in `ls $1/*` do mv $file $file.UP done 首先，不要从任何常规目录尝试此脚本；相反，请从测试目录运行此命令。此外，您需要提供文件的目录名作为命令行参数。对当前工作目录使用句点（.）。
打印文件或目录的数量 下面的Linuxbash脚本查找给定目录中存在的文件或文件夹的数量。它使用Linux find命令来执行此操作。首先，需要传递目录名以从命令行搜索文件。
#!/bin/bash  if [ -d &#34;$@&#34; ]; then echo &#34;Files found: $(find &#34;$@&#34; -type f | wc -l)&#34; echo &#34;Folders found: $(find &#34;$@&#34; -type d | wc -l)&#34; else echo &#34;[ERROR] Please retry with another folder.&#34; exit 1 fi 如果指定的目录不可用或存在权限问题，程序将要求用户重试。
清理日志文件 下一个简单的示例演示了在现实生活中使用shell脚本的简便方法。该程序只需删除/var/log目录中的所有日志文件。您可以更改保存此目录的变量以清理其他日志。
#!/bin/bash LOG_DIR=/var/log cd $LOG_DIR cat /dev/null &gt; messages cat /dev/null &gt; wtmp echo &#34;Logs cleaned up.&#34; 请记住以root身份运行此Linuxshell脚本。
使用Bash备份脚本 Shell脚本提供了一种强大的方法来备份文件和目录。以下示例将备份过去24小时内修改的每个文件或目录。该程序使用find命令执行此操作。
#!/bin/bash  BACKUPFILE=backup-$(date +%m-%d-%Y) archive=${1:-$BACKUPFILE} find . -mtime -1 -type f -print0 | xargs -0 tar rvf &#34;$archive.tar&#34; echo &#34;Directory $PWDbacked up in archive file \&#34;$archive.tar.gz\&#34;.&#34; exit 0 备份过程成功后，它将打印文件和目录的名称。
检查你是否是root用户 下面的示例演示了通过Linux bash脚本快速确定用户是否为root用户的方法。
#!/bin/bash ROOT_UID=0 if [ &#34;$UID&#34; -eq &#34;$ROOT_UID&#34; ] then echo &#34;You are root.&#34; else echo &#34;You are not root&#34; fi exit 0 此脚本的输出取决于运行它的用户。它将根据$UID匹配根用户。
从文件中删除重复行 文件处理需要相当长的时间，并在许多方面阻碍了管理员的工作效率。例如，在文件中搜索重复项可能会成为一项艰巨的任务。幸运的是，您可以使用一个简短的shell脚本来完成此操作。
#! /bin/sh  echo -n &#34;Enter Filename-&gt; &#34; read filename if [ -f &#34;$filename&#34; ]; then sort $filename | uniq | tee sorted.txt else echo &#34;No $filenamein $pwd...try again&#34; fi exit 0 上面的脚本逐行遍历文件并删除所有重复的行。然后，它将新内容放入新文件，并保持原始文件的完整性。
系统维护 我经常使用一个小的Linuxshell脚本来升级我的系统，而不是手动升级。下面的简单shell脚本将向您展示如何做到这一点。
#!/bin/bash  echo -e &#34;\n$(date &#34;+%d-%m-%Y --- %T&#34;)--- Starting work\n&#34; apt-get update apt-get -y upgrade apt-get -y autoremove apt-get autoclean echo -e &#34;\n$(date &#34;+%T&#34;)\t Script Terminated&#34; 该脚本还处理不再需要的旧包。您需要使用sudo运行此脚本，否则它将无法正常工作。
]]></content>
  </entry>
  
  <entry>
    <title>Linux中touch命令的8个实际例子</title>
    <url>/post/linux/8-examples-of-touch-cmd-in-linux.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>touch</tag>
    </tags>
    <content type="html"><![CDATA[在本文中，我们将介绍一些有用的 Linux   实际示例touch command。这touch command是一个标准程序Unix/Linux操作系统，用于创建、更改和修改文件的时间戳。在开始接触命令示例之前，请查看以下选项。
touch命令选项   -a, 只更改访问时间 -c, 如果文件不存在，不创建 -d, 更新访问和修改时间 -m, 只更改修改时间 -r, 使用文件的访问和修改次数 -t, 使用指定时间创建文件  如何创建一个空文件  以下 touch 命令创建一个名为的空（零字节）新文件sheena.
 # touch sheena 如何创建多个文件  通过使用 touch 命令，您还可以创建多个文件。例如，以下命令将创建 3 个名为的文件，sheena,meena和temp.
 # touch sheena meena temp 如何更改文件访问和修改时间 更改或更新名为的文件的上次访问和修改时间temp， 使用-a选项如下。以下命令设置文件的当前时间和日期。如果temp文件不存在，它将创建具有名称的新空文件。  # touch -a temp  find 命令使用时间戳来列出和查找文件。
 如何避免创建新文件 使用-c带有 touch 命令的选项可避免创建新文件。例如，以下命令不会创建名为temp如果它不存在。  # touch -c temp ##如何更改文件修改时间
如果您想更改名为的文件的唯一修改时间temp，然后使用-m带有触摸命令的选项。请注意，它只会更新文件的最后修改时间（而不是访问时间）。  # touch -m temp ##明确设置访问和修改时间
 您可以使用显式设置时间-c和-t带有触摸命令的选项。格式如下。
 # touch -c -t YYDDHHMM temp  例如，以下命令设置文件的访问和修改日期和时间temp作为17:30(17:30 p.m.)August 10当年（2021）。
 # touch -c -t 12101730 temp  接下来验证文件的访问和修改时间temp， 和ls -l命令。
 # ls -l total 2 -rw-r--r--. 1 root root 0 Dec 10 17:30 temp 如何使用另一个文件的时间戳  以下触摸命令与-r选项，将更新文件的时间戳meena带有时间戳temp文件。因此，两个文件都拥有相同的时间戳。
 # touch -r temp meena 使用指定时间创建文件  如果你想创建一个指定时间而不是当前时间的文件，那么格式应该是。
 # touch -t YYMMDDHHMM.SS rumenz  例如下面的命令 touch 命令-t选项将给出rumenz归档时间戳18:30:55 p.m.在August 5,2021.
 # touch -t 202108051830.55 rumenz ]]></content>
  </entry>
  
  <entry>
    <title>C语言回调函数，提升C技巧必备</title>
    <url>/post/linux/c-programming-callback-function.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>c</tag>
      <tag>callback</tag>
    </tags>
    <content type="html"><![CDATA[本文简要介绍了C语言编程中的回调函数
函数指针 在讲回调函数之前，我们需要了解函数指针。
我们都知道，C语言的灵魂是指针，我们经常使用整型指针，字符串指针，结构体指针等。
int *p1; char *p2; STRUCT *p3; // STRUCT为我们定义的结构体 但是好像我们一般很少使用函数指针，我们一般使用函数都是直接使用函数调用。
下面我们来了解一下函数指针的概念和使用方法。
概念 函数指针是指向函数的指针变量。
通常我们说的指针变量是指向一个整型、字符型或数组等变量，而函数指针是指向函数。
函数指针可以像一般函数一样，用于调用函数、传递参数。
函数指针的定义方式为：
函数返回值类型 (* 指针变量名) (函数参数列表);  “函数返回值类型”表示该指针变量可以指向具有什么返回值类型的函数；“函数参数列表”表示该指针变量可以指向具有什么参数列表的函数。这个参数列表中只需要写函数的参数类型即可。
我们看到，函数指针的定义就是将“函数声明”中的“函数名”改成“（指针变量名）”。但是这里需要注意的是：“（指针变量名）”两端的括号不能省略，括号改变了运算符的优先级。如果省略了括号，就不是定义函数指针而是一个函数声明了，即声明了一个返回值类型为指针型的函数。
那么怎么判断一个指针变量是指向变量的指针变量还是指向函数的指针变量呢？首先看变量名前面有没有“”，如果有“”说明是指针变量；其次看变量名的后面有没有带有形参类型的圆括号，如果有就是指向函数的指针变量，即函数指针，如果没有就是指向变量的指针变量。
最后需要注意的是，指向函数的指针变量没有 ++ 和 – 运算。
一般为了方便使用，我们会选择：
typedef 函数返回值类型 (* 指针变量名) (函数参数列表);  比如：
typedef int (*Fun1)(int); //声明也可写成int (*Fun1)(int x)，但习惯上一般不这样。 typedef int (*Fun2)(int, int); //参数为两个整型，返回值为整型 typedef void (*Fun3)(void); //无参数和返回值 typedef void* (*Fun4)(void*); //参数和返回值都为void*指针 如何用函数指针调用函数 给大家举一个例子：
int Func(int x); /*声明一个函数*/ int (*p) (int x); /*定义一个函数指针*/ p = Func; /*将Func函数的首地址赋给指针变量p*/ p = &amp;Func; /*将Func函数的首地址赋给指针变量p*/ 赋值时函数 Func 不带括号，也不带参数。由于函数名 Func 代表函数的首地址，因此经过赋值以后，指针变量 p 就指向函数 Func() 代码的首地址了。
下面来写一个程序，看了这个程序你们就明白函数指针怎么使用了：
#include &lt;stdio.h&gt;int Max(int, int); //函数声明 int main(void) { int(*p)(int, int); //定义一个函数指针  int a, b, c; p = Max; //把函数Max赋给指针变量p, 使p指向Max函数  printf(&#34;please enter a and b:&#34;); scanf(&#34;%d%d&#34;, &amp;a, &amp;b); c = (*p)(a, b); //通过函数指针调用Max函数  printf(&#34;a = %d\nb = %d\nmax = %d\n&#34;, a, b, c); return 0; } int Max(int x, int y) //定义Max函数 { int z; if (x &gt; y) { z = x; } else { z = y; } return z; } 特别注意的是，因为函数名本身就可以表示该函数地址（指针），因此在获取函数指针时，可以直接用函数名，也可以取函数的地址。
p = Max可以改成 p = &amp;Max c = (*p)(a, b) 可以改成 c = p(a, b)  函数指针作为某个函数的参数 既然函数指针变量是一个变量，当然也可以作为某个函数的参数来使用的。示例：
#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt; typedef void(*FunType)(int); //前加一个typedef关键字，这样就定义一个名为FunType函数指针类型，而不是一个FunType变量。 //形式同 typedef int* PINT; void myFun(int x); void hisFun(int x); void herFun(int x); void callFun(FunType fp,int x); int main() { callFun(myFun,100);//传入函数指针常量，作为回调函数  callFun(hisFun,200); callFun(herFun,300); return 0; } void callFun(FunType fp,int x) { fp(x);//通过fp的指针执行传递进来的函数，注意fp所指的函数有一个参数 } void myFun(int x) { printf(&#34;myFun: %d\n&#34;,x); } void hisFun(int x) { printf(&#34;hisFun: %d\n&#34;,x); } void herFun(int x) { printf(&#34;herFun: %d\n&#34;,x); } 输出：
myFun: 100 hisFun: 200 herFun: 300 函数指针作为函数返回类型 有了上面的基础，要写出返回类型为函数指针的函数应该不难了，下面这个例子就是返回类型为函数指针的函数：
void (* func5(int, int, float ))(int, int) { ... } 在这里， func5 以 (int, int, float) 为参数，其返回类型为 void (\*)(int, int) 。在C语言中，变量或者函数的声明也是一个大学问，想要了解更多关于声明的话题，可以参考我之前的文章 - C专家编程》读书笔记(1-3章)。这本书的第三章花了整整一章的内容来讲解如何读懂C语言的声明。
函数指针数组 在开始讲解回调函数前，最后介绍一下函数指针数组。既然函数指针也是指针，那我们就可以用数组来存放函数指针。下面我们看一个函数指针数组的例子：
/* 方法1 */ void (*func_array_1[5])(int, int, float); /* 方法2 */ typedef void (*p_func_array)(int, int, float); p_func_array func_array_2[5]; 上面两种方法都可以用来定义函数指针数组，它们定义了一个元素个数为5，类型是 * void (\*)(int, int, float) * 的函数指针数组。
函数指针总结 函数指针常量 ：Max；函数指针变量：p；
数名调用如果都得如(*myFun)(10)这样，那书写与读起来都是不方便和不习惯的。所以C语言的设计者们才会设计成又可允许myFun(10)这种形式地调用（这样方便多了，并与数学中的函数形式一样）。
在函数指针变量也可以存入一个数组内。数组的声明方法：int (*fArray[10]) ( int );
 回调函数 什么是回调函数 我们先来看看百度百科是如何定义回调函数的：
回调函数就是一个通过函数指针调用的函数。如果你把函数的指针（地址）作为参数传递给另一个函数，当这个指针被用来调用其所指向的函数时，我们就说这是回调函数。回调函数不是由该函数的实现方直接调用，而是在特定的事件或条件发生时由另外的一方调用的，用于对该事件或条件进行响应。  这段话比较长，也比较绕口。下面我通过一幅图来说明什么是回调：
假设我们要使用一个排序函数来对数组进行排序，那么在主程序(Main program)中，我们先通过库，选择一个库排序函数(Library function)。但排序算法有很多，有冒泡排序，选择排序，快速排序，归并排序。同时，我们也可能需要对特殊的对象进行排序，比如特定的结构体等。库函数会根据我们的需要选择一种排序算法，然后调用实现该算法的函数来完成排序工作。这个被调用的排序函数就是回调函数(Callback function)。
结合这幅图和上面对回调函数的解释，我们可以发现，要实现回调函数，最关键的一点就是要将函数的指针传递给一个函数(上图中是库函数)，然后这个函数就可以通过这个指针来调用回调函数了。注意，回调函数并不是C语言特有的，几乎任何语言都有回调函数。在C语言中，我们通过使用函数指针来实现回调函数。
我的理解是：把一段可执行的代码像参数传递那样传给其他代码，而这段代码会在某个时刻被调用执行，这就叫做回调。
如果代码立即被执行就称为同步回调，如果过后再执行，则称之为异步回调。
回调函数就是一个通过函数指针调用的函数。如果你把函数的指针（地址）作为参数传递给另一个函数，当这个指针被用来调用其所指向的函数时，我们就说这是回调函数。
回调函数不是由该函数的实现方直接调用，而是在特定的事件或条件发生时由另外的一方调用的，用于对该事件或条件进行响应。
为什么要用回调函数？ 因为可以把调用者与被调用者分开，所以调用者不关心谁是被调用者。它只需知道存在一个具有特定原型和限制条件的被调用函数。
简而言之，回调函数就是允许用户把需要调用的方法的指针作为参数传递给一个函数，以便该函数在处理相似事件的时候可以灵活的使用不同的方法。
int Callback() // /&lt; 回调函数 { // TODO  return 0; } int main() // /&lt; 主函数 { // TODO  Library(Callback); // /&lt; 库函数通过函数指针进行回调  // TODO  return 0; } 回调似乎只是函数间的调用，和普通函数调用没啥区别。
但仔细看，可以发现两者之间的一个关键的不同：在回调中，主程序把回调函数像参数一样传入库函数。
这样一来，只要我们改变传进库函数的参数，就可以实现不同的功能，这样有没有觉得很灵活？并且当库函数很复杂或者不可见的时候利用回调函数就显得十分优秀。
怎么使用回调函数？ int Callback_1(int a) // /&lt; 回调函数1 { printf(&#34;Hello, this is Callback_1: a = %d &#34;, a); return 0; } int Callback_2(int b) // /&lt; 回调函数2 { printf(&#34;Hello, this is Callback_2: b = %d &#34;, b); return 0; } int Callback_3(int c) // /&lt; 回调函数3 { printf(&#34;Hello, this is Callback_3: c = %d &#34;, c); return 0; } int Handle(int x, int (*Callback)(int)) // /&lt; 注意这里用到的函数指针定义 { Callback(x); } int main() { Handle(4, Callback_1); Handle(5, Callback_2); Handle(6, Callback_3); return 0; } 如上述代码：可以看到，Handle() 函数里面的参数是一个指针，在 main() 函数里调用 Handle() 函数的时候，给它传入了函数 Callback_1()/Callback_2()/Callback_3() 的函数名，这时候的函数名就是对应函数的指针，也就是说，回调函数其实就是函数指针的一种用法。
下面是一个四则运算的简单回调函数例子： #include &lt;stdio.h&gt;#include &lt;stdlib.h&gt; /**************************************** * 函数指针结构体 ***************************************/ typedef struct _OP { float (*p_add)(float, float); float (*p_sub)(float, float); float (*p_mul)(float, float); float (*p_div)(float, float); } OP; /**************************************** * 加减乘除函数 ***************************************/ float ADD(float a, float b) { return a + b; } float SUB(float a, float b) { return a - b; } float MUL(float a, float b) { return a * b; } float DIV(float a, float b) { return a / b; } /**************************************** * 初始化函数指针 ***************************************/ void init_op(OP *op) { op-&gt;p_add = ADD; op-&gt;p_sub = SUB; op-&gt;p_mul = &amp;MUL; op-&gt;p_div = &amp;DIV; } /**************************************** * 库函数 ***************************************/ float add_sub_mul_div(float a, float b, float (*op_func)(float, float)) { return (*op_func)(a, b); } int main(int argc, char *argv[]) { OP *op = (OP *)malloc(sizeof(OP)); init_op(op); /* 直接使用函数指针调用函数 */ printf(&#34;ADD = %f, SUB = %f, MUL = %f, DIV = %f\n&#34;, (op-&gt;p_add)(1.3, 2.2), (*op-&gt;p_sub)(1.3, 2.2), (op-&gt;p_mul)(1.3, 2.2), (*op-&gt;p_div)(1.3, 2.2)); /* 调用回调函数 */ printf(&#34;ADD = %f, SUB = %f, MUL = %f, DIV = %f\n&#34;, add_sub_mul_div(1.3, 2.2, ADD), add_sub_mul_div(1.3, 2.2, SUB), add_sub_mul_div(1.3, 2.2, MUL), add_sub_mul_div(1.3, 2.2, DIV)); return 0; } 回调函数实例（很有用） 一个 GPRS 模块联网的小项目，使用过的同学大概知道 2G、4G、NB 等模块要想实现无线联网功能都需要经历模块上电初始化、注册网络、查询网络信息质量、连接服务器等步骤，这里的的例子就是，利用一个状态机函数（根据不同状态依次调用不同实现方法的函数），通过回调函数的方式依次调用不同的函数，实现模块联网功能，如下：
/********* 工作状态处理 *********/ typedef struct { uint8_t mStatus; uint8_t (* Funtion)(void); //函数指针的形式 } M26_WorkStatus_TypeDef; //M26的工作状态集合调用函数  /********************************************** ** &gt;M26工作状态集合函数 ***********************************************/ M26_WorkStatus_TypeDef M26_WorkStatus_Tab[] = { {GPRS_NETWORK_CLOSE, M26_PWRKEY_Off }, //模块关机  {GPRS_NETWORK_OPEN, M26_PWRKEY_On }, //模块开机  {GPRS_NETWORK_Start, M26_Work_Init }, //管脚初始化  {GPRS_NETWORK_CONF, M26_NET_Config }, //AT指令配置  {GPRS_NETWORK_LINK_CTC, M26_LINK_CTC }, //连接调度中心  {GPRS_NETWORK_WAIT_CTC, M26_WAIT_CTC }, //等待调度中心回复  {GPRS_NETWORK_LINK_FEM, M26_LINK_FEM }, //连接前置机  {GPRS_NETWORK_WAIT_FEM, M26_WAIT_FEM }, //等待前置机回复  {GPRS_NETWORK_COMM, M26_COMM }, //正常工作  {GPRS_NETWORK_WAIT_Sig, M26_WAIT_Sig }, //等待信号回复  {GPRS_NETWORK_GetSignal, M26_GetSignal }, //获取信号值  {GPRS_NETWORK_RESTART, M26_RESET }, //模块重启 } /********************************************** ** &gt;M26模块工作状态机，依次调用里面的12个函数 ***********************************************/ uint8_t M26_WorkStatus_Call(uint8_t Start) { uint8_t i = 0; for(i = 0; i &lt; 12; i++) { if(Start == M26_WorkStatus_Tab[i].mStatus) { return M26_WorkStatus_Tab[i].Funtion(); } } return 0; } 所以，如果有人想做个 NB 模块联网项目，可以 copy 上面的框架，只需要修改回调函数内部的具体实现，或者增加、减少回调函数，就可以很简洁快速的实现模块联网。
]]></content>
  </entry>
  
  <entry>
    <title>DDR、DDR2、DDR3、DDR4、LPDDR区别</title>
    <url>/post/hardware/difference-between-different-generation-DDRs.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>DDR</tag>
    </tags>
    <content type="html"><![CDATA[今天我们和大家介绍一下各代DDR的区别
什么是DDR DDR是Double Data Rate的缩写，即“双比特翻转”。DDR是一种技术，中国大陆工程师习惯用DDR称呼用了DDR技术的SDRAM，而在中国台湾以及欧美，工程师习惯用DRAM来称呼。
DDR的核心要义是在一个时钟周期内，上升沿和下降沿都做一次数据采样，这样400MHz的主频可以实现800Mbps的数据传输速率。
每一代DDR的基本区别 关键技术解释 VTT VTT为DDR的地址线，控制线等信号提供上拉电源，上拉电阻是50Ω左右。VTT=1/2VDDQ，并且VTT要跟随VDDQ，因此需要专用的电源同时提供VDDQ和VTT。例如芯片TPS51206DSQT，LP2996。用专门的电源芯片，还有一个重要的原因，在Fly-by的拓扑中，VTT提供电流，增强DDR信号线的驱动能力。
DDR的接收器是一个比较器，其中一端是VREF，另一端是信号，例如地址线A2在有VTT上拉的时候，A2的信号在0和1.8V间跳动，当A2电压高于VTT时，电流流向VTT。当A2低于VTT时，VTT流向DDR。因此VTT需要有提供电流和吸收电流的能力，一般的开关电源不能作为VTT的提供者。此外，VTT电源相当于DDR接收器信号输入端的直流偏执，且这个偏执等于VREF，因此VTT的噪声要越小越好，否则当A2的状态为高阻态时，DDR接收器的比较器容易产生误触发。
上文说过，VTT相当于DDR接收器的直流偏执，其实如果没有VTT，这个直流偏执也存在，它在芯片的内部，提供电流的能力很弱。如果只有1个或2个DDR芯片，走Fly-by拓扑，那么不需要外部的VTT上拉。如果有2个以上的DDR芯片，则一定需要VTT上拉。
Prefetch Prefetch字面意思就是预存取，每一代的DDR预存取大小不同，详见第2章中表格。以DDR3为例，它的Prefetch=8n，相当于DDR的每一个IO都有一个宽度为8的buffer，从IO进来8个数据后，在第8个数据进来后，才把这8个数据一次性的写入DDR内部的存储单元。下图是一个形象的解释，同时我们关注一下几个速率。DDR3的时钟是800MHz，Data Rate是1600Mbps，由于这个Buffer的存在，DDR内部的时钟只需要200MHz就可以了（注意DDR内部不是双比特翻转采样）。
我们来做一个频率对照表，如下：
DDR内部的最小存储单元（1bit）是一个晶体管+一个电容，电容会放电，需要不断的“刷新”（充电）才能保持正常的工作状态，由于电容充放电需要时间，DDR内部的频率受限于此，很难提高，目前技术一般在100~200MHz。因此需要用Prefetch技术来提内部数据高吞吐率（其实就是串并转换原理）。Prefetch位宽的提高，是DDR2,3,4非常显著的变化。
第一段提到，对于DDR3，在第8个数据进来后，FIFO满了，然后才把这8个数据一次性的写入DDR内部的存储单元，那么必须要求DDR的内部时钟和外部时钟有一定的约束关系，FIFO满的时候一定是以DQS下降沿采样结束的，数据手册中对DQS的下降沿与clk有一个建立时间和保持时间的约束要求的目的原来是这样。
SSTL SSTL（Stub Series Terminated Logic）接口标准也是JEDEC所认可的标准之一。该标准专门针对高速内存(特别是SDRAM)接口。SSTL规定了开关特点和特殊的端接方案。
SSTL标准规定了IC供电，IO的DC和AC输入输出门限，差分信号门限，Vref电压等。SSTL_3是3.3V标准，SSTL_2是2.5V标准，SSTL_18是1.8V标准，SSTL_15是1.5V。
SSTL最大的特点是需要终端匹配电阻，也叫终端终结电阻，上拉到VTT（1/2VDDQ）。这个短接电阻最大的作用是为了信号完整性，特别是在1拖多的Fly-by走线拓扑下，还能增强驱动能力。
Bank 以下图为例，一个Bank中包含若干个Array，Array相当于一个表单，选中“行地址”和“列地址”后，表单中的一个单元格就被选中，这个单元格就是一个bit。Bank中的所有Array的行地址是连在一起的，列地址也是。那么选中“行地址”和“列地址”后，将一起选中所有Array的bit。有多少个array，就有多少个bit被选中。以DDR3为例，Data线宽度是32，prefetch是8，那么Array就有32x8=256.内部一次操作会选中256bit的数据。
Bank数量越多，需要的Bank选择线越多，DDR3有8个bank，需要3个BA信号BA0~2。BA，行地址，列地址共同组成了存储单元的访问地址，缺一不可。
DDR的容量计算 下图是DDR3 1Gb的寻址配置，以其中128Mbx8为例说明，其中x8表示IO数据（DQ）位宽度。
 DDD容量=2Bank Addressx2Row Addressx2Col Addressx位宽=23x214x210x8=1Gb Page Site=2Col Addressx位宽➗8（Byte)  我的理解是，这个page size更像是逻辑上的一个页，并不是一个bank中，一行的所有bit，因为一行的所有bit要考虑prefetch宽度。
上表是JESD-3D中的表格，Row Address和Column Address都是真实需要寻址的地址，其他用途的地址比如A10，A12或者A11等并没有计算在内。在计算时，不要因为有A13，就认为Column Address就是A0~A13。
Burst Burst字面意思是突发，DDR的访问都是以突发的方式连续访问同一行的相邻几个单元。进行Brust时，需要有几个参数：
 Burst Length：一次突发访问几个列地址。 Read/Write: 是读还是写 Starting Column：从哪一列开始Burst Burst：突发的顺序。  下图是DDR3中突发类型和顺序，Burst是通过A12/BC#选择的。但对于DDR，DDR2和DDR4，不一定就是通过A12/BC#，详见PIN定义章节。
DDR的tRDC，CL，tAC 在实际工作中，Bank地址与相应的行地址是同时发出的，此时这个命令称之为“行激活”（Row Active）。在此之后，将发送列地址寻址命令与具体的操作命令（是读还是写），这两个命令也是同时发出的，所以一般都会以“读/写命令”来表示列寻址。根据相关的标准，从行有效到读/写命令发出之间的间隔被定义为tRCD，即RAS to CAS Delay（RAS至CAS延迟，RAS就是行地址选通脉冲，CAS就是列地址选通脉冲），我们可以理解为行选通周期。tRCD是DDR的一个重要时序参数，广义的tRCD以时钟周期（tCK，Clock Time）数为单位，比如tRCD=3，就代表延迟周期为两个时钟周期，具体到确切的时间，则要根据时钟频率而定，DDR3-800，tRCD=3，代表30ns的延迟。
接下来，相关的列地址被选中之后，将会触发数据传输，但从存储单元中输出到真正出现在内存芯片的 I/O 接口之间还需要一定的时间（数据触发本身就有延迟，而且还需要进行信号放大），这段时间就是非常著名的 CL（CAS Latency，列地址脉冲选通潜伏期）。CL 的数值与 tRCD 一样，以时钟周期数表示。如 DDR3-800，时钟频率为 100MHz，时钟周期为 10ns，如果 CL=2 就意味着 20ns 的潜伏期。不过CL只是针对读取操作。
由于芯片体积的原因，存储单元中的电容容量很小，所以信号要经过放大来保证其有效的识别性，这个放大/驱动工作由S-AMP负责，一个存储体对应一个S- AMP通道。但它要有一个准备时间才能保证信号的发送强度（事前还要进行电压比较以进行逻辑电平的判断），因此从数据I/O总线上有数据输出之前的一个时钟上升沿开始，数据即已传向S-AMP，也就是说此时数据已经被触发，经过一定的驱动时间最终传向数据I/O总线进行输出，这段时间我们称之为 tAC（Access Time from CLK，时钟触发后的访问时间）。
目前内存的读写基本都是连续的，因为与CPU交换的数据量以一个Cache Line（即CPU内Cache的存储单位）的容量为准，一般为64字节。而现有的Rank位宽为8字节（64bit），那么就要一次连续传输8次，这就涉及到我们也经常能遇到的突发传输的概念。突发（Burst）是指在同一行中相邻的存储单元连续进行数据传输的方式，连续传输的周期数就是突发长度（Burst Lengths，简称BL）。
在进行突发传输时，只要指定起始列地址与突发长度，内存就会依次地自动对后面相应数量的存储单元进行读/写操作而不再需要控制器连续地提供列地址。这样，除了第一笔数据的传输需要若干个周期（主要是之前的延迟，一般的是tRCD+CL）外，其后每个数据只需一个周期的即可获得。
突发连续读取模式：只要指定起始列地址与突发长度，后续的寻址与数据的读取自动进行，而只要控制好两段突发读取命令的间隔周期（与BL相同）即可做到连续的突发传输。
谈到了突发长度时。如果BL=4，那么也就是说一次就传送4×64bit的数据。但是，如果其中的第二笔数据是不需要的，怎么办？还都传输吗？为了屏蔽不需要的数据，人们采用了数据掩码（Data I/O Mask，简称DQM）技术。通过DQM，内存可以控制I/O端口取消哪些输出或输入的数据。这里需要强调的是，在读取时，被屏蔽的数据仍然会从存储体传出，只是在“掩码逻辑单元”处被屏蔽。DQM由北桥控制，为了精确屏蔽一个P-Bank位宽中的每个字节，每个DIMM有8个DQM 信号线，每个信号针对一个字节。这样，对于4bit位宽芯片，两个芯片共用一个DQM信号线，对于8bit位宽芯片，一个芯片占用一个DQM信号，而对于 16bit位宽芯片，则需要两个DQM引脚。
在数据读取完之后，为了腾出读出放大器以供同一Bank内其他行的寻址并传输数据，内存芯片将进行预充电的操作来关闭当前工作行。还是以上面那个Bank示意图为例。当前寻址的存储单元是B1、R2、C6。如果接下来的寻址命令是B1、R2、C4，则不用预充电，因为读出放大器正在为这一行服务。但如果地址命令是B1、R4、C4，由于是同一Bank的不同行，那么就必须要先把R2关闭，才能对R4寻址。从开始关闭现有的工作行，到可以打开新的工作行之间的间隔就是tRP（Row Precharge command Period，行预充电有效周期），单位也是时钟周期数。
ODT ODT是内建核心的终结电阻，它的功能是让一些信号在终结电阻处消耗完，防止这些信号在电路上形成反射。换句话说就是在片内设置合适的上下拉电阻，以获得更好的信号完整性。被ODT校准的信号包括：
 DQ, DQS, DQS# and DM for x4 configuration DQ, DQS, DQS#, DM, TDQS and TDQS# for X8 configuration DQU, DQL, DQSU, DQSU#, DQSL, DQSL#, DMU and DML for X16 configuration  当一个CPU挂了很多个DDR芯片的时候，他们是共用控制线，地址线的，走线肯定要分叉，如果没有中端匹配电阻，肯定会产生信号完整性问题。那么如果只有一个DDR芯片的时候，需不需要呢？正常情况下，走线很短，有符合规则，是不需要的。
下图是DDR中的IO上下拉电阻，RON是DDR的输出结构的上下拉电阻，RTT是DDR输入结构的上下拉电阻。这两个电阻的阻值都是可调的。
下图是RON的调节，注意这不是ODT的任务，调节是通过寄存器实现。
下图是RTT的调节，是ODT要做的事情，而且RTT的档位要多，也是通过寄存器调节的。
注意，DDR3的PIN定义上有一个引脚是ODT，如果ODT=0，DRAM Termination State功能关闭；ODT=1，DRAM Termination State的功能参考寄存器设置。如下是一个真值表。因为DRAM Termination State非常耗电，所以不用的时候最好不要打开。
DDR3的ZQ ZQ信号在DDR3时代开始引入，要求在ZQ引脚放置一个240Ω±1%的高精度电阻到地，注意必须是高精度。而且这个电阻是必须的，不能省略的。进行ODT时，是以这个引脚上的阻值为参考来进行校准的。
校准需要调整内部电阻，以获得更好的信号完整性，但是内部电阻随着温度会有些细微的变化，为了将这个变化纠正回来，就需要一个外部的精确电阻作为参考。详细来讲，就是为RTT和RON提供参考电阻。
OCD OCD 是在 DDR-II 开始加入的新功能，而且这个功能是可选的，有的资料上面又叫离线驱动调整。OCD的主要作用在于调整 I/O 接口端的电压，来补偿上拉与下拉电阻值， 从而调整DQS 与 DQ 之间的同步确保信号的完整与可靠性。调校期间，分别测试 DQS 高电平和 DQ高电平，以及 DQS 低电平和 DQ 高电平的同步情况。如果不满足要求，则通过设定突发长度的地址线来传送上拉 / 下拉电阻等级（加一档或减一档），直到测试合格才退出 OCD 操作，通过 OCD 操作来减少 DQ 、 DQS的倾斜从而提高信号的完整性及控制电压来提高信号品质。由于在一般情况下对应用环境稳定程度要求并不太高，只要存在差分 DQS时就基本可以保证同步的准确性， 而且 OCD 的调整对其他操作也有一定影响， 因此 OCD 功能在普通台式机上并没有什么作用，其优点主要体现在对数据完整性非常敏感的服务器等高端产品领域。
DDR3的PIN定义 下面是三星K4B4G0446Q/K4B4G0846Q的PIN定义，每一个都有很详细的解释。
以x8的配置为例，如下是其Ball Map。
  一对时钟线CK和CKn
  数据线DQ0~DQ7共8位。
  一对差分对DQS和DQSn
  地址线A0~A15，其中，A10和A12有特殊用途。
  行选中信号RASn
  列选中信号CASn
  写使能Wen
  片选CSn
  Bank选择BA0~2
  一个Reset信号，是DDR3新增的一项重要功能，并为此专门准备了一个引脚。这一引脚将使DDR3的初始化处理变得简单。当Reset命令有效时，DDR3 内存将停止所有的操作，并切换至最少量活动的状态，以节约电力。在Reset期间，DDR3内存将关闭内在的大部分功能，所有数据接收与发送器都将关闭，且所有内部的程序装置将复位，DLL（延迟锁相环路）与时钟电路将停止工作，甚至不理睬数据总线上的任何动静。这样一来，该功能将使DDR3达到最节省电力的目的。
  ZQ和ODT PIN上文已经说明。
  DDR的走线规则 DDR的信号线需要分组：
数据线一组（DQ,DQS,DQM），误差控制在20mil以内；
控制线一组（Address，控制线，时钟），以时钟为中心，误差控制在100mil以内。
]]></content>
  </entry>
  
  <entry>
    <title>什么是DSP</title>
    <url>/post/dsp/what-is-dsp.html</url>
    <categories><category>DSP</category>
    </categories>
    <tags>
      <tag>DSP</tag>
    </tags>
    <content type="html"><![CDATA[嵌入式工程师都知道什么是CPU、MCU，还有一位成员——DSP，DSP到底是什么？
DSP概述 DSP（digital signal processor）是一种独特的微处理器，有自己的完整指令系统，是以数字信号来处理大量信息的器件。其最大特点是内部有专用的硬件乘法器和哈佛总线结构对大量的数字信号处理的速度快。一个数字信号处理器在一块不大的芯片内包括有控制单元、运算单元、各种寄存器以及一定数量的存储单元等等，在其外围还可以连接若干存储器，并可以与一定数量的外部设备互相通信，有软、硬件的全面功能，本身就是一个微型计算机。
DSP采用的是哈佛设计，即数据总线和地址总线分开，使程序和数据分别存储在两个分开的空间，允许取指令和执行指令完全重叠。也就是说在执行上一条指令的同时就可取出下一条指令，并进行译码，这大大的提高了微处理器的速度。另外还允许在程序空间和数据空间之间进行传输，因为增加了器件的灵活性。
当今的数字化时代背景下，DSP己成为通信、计算机、消费类电子产品等领域的基础器件。
根据数字信号处理的要求，DSP芯片一般具有如下的一些主要特点：
 在一个指令周期内可完成一次乘法和一次加法。 程序和数据空间分开，可以同时访问指令和数据。 片内具有快速RAM，通常可通过独立的数据总线在两块中同时访问。 具有低开销或无开销循环及跳转的硬件支持。 快速的中断处理和硬件I/O支持。 具有在单周期内操作的多个硬件地址产生器。 可以并行执行多个操作。 支持流水线操作，使取指、译码和执行等操作可以重叠执行。  与通用微处理器相比，DSP芯片的其他通用功能相对较弱些。
DSP 芯片的诞生过程 DSP 芯片的诞生是时代所需。20世纪60年代以来，随着计算机和信息技术的飞速发展，数字信号处理技术应运而生并得到迅速的发展。在 DSP 芯片出现之前数字信号处理只能依靠微处理器来完成。但由于微处理器较低的处理速度不快，根本就无法满足越来越大的信息量的高速实时要求。
上世纪 70 年代，DSP芯片的理论和算法基础已成熟。但那时的DSP仅仅停留在教科书上，即使是研制出来的 DSP 系统也是由分立元件组成的，其应用领域仅局限于军事、航空航天部门。
 1978 年， AMI 公司发布世界上第一个单片 DSP 芯片 S2811，但没有现代 DSP芯片所必须有的硬件乘法器； 1979 年， 美国 Intel 公司发布的商用可编程器件 2920 是 DSP 芯片的一个主要里程碑，但其依然没有硬件乘法器； 1980 年，日本 NEC 公司推出的 MPD7720 是第一个具有硬件乘法器的商用 DSP芯片，从而被认为是第一块单片 DSP 器件； 1982 年世界上诞生了第一代 DSP 芯片 TMS32010 及其系列产品。这种 DSP 器件采用微米工艺 NMOS 技术制作，虽功耗和尺寸稍大，但运算速度却比微处理器快了几十倍。  DSP 芯片的问世是个里程碑，它标志着 DSP 应用系统由大型系统向小型化迈进了一大步。至 80 年代中期，随着 CMOS 工艺的 DSP 芯片应运而生，其存储容量和运算速度都得到成倍提高，成为语音处理、图像硬件处理技术的基础。
80 年代后期，第三代 DSP 芯片问世，运算速度进一步提高，其应用范围逐步扩大到通信、计算机领域；
90 年代 DSP 发展最快，相继出现了第四代和第五代 DSP 芯片。第五代与第四代相比系统集成度更高，将 DSP 芯核及外围元件综合集成在单一芯片上。
进入 21 世纪后，第六代 DSP 芯片横空出世。第六代芯片在性能上全面碾压第五代芯片，同时基于商业目的的不同发展出了诸多个性化的分支，并开始逐渐拓展新的领域。
DSP 芯片的应用领域 如今，各种各样的DSP器件已相当丰富。大大小小封装形式的DSP器件，已广泛应用于各种产品的生产领域，而且DSP的应用领域仍在不断地扩大，发展迅速异常。
DSP芯片强调数字信号处理的实时性。DSP作为数字信号处理器将模拟信号转换成数字信号，用于专用处理器的高速实时处理。它具有高速，灵活，可编程，低功耗的界面功能，在图形图像处理，语音处理，信号处理等通信领域起到越来越重要的作用。
根据美国的权威资讯公司统计，目前 DSP 芯片在市场上应用最多的是通信领域，其次是计算机领域。
DSP芯片的应用领域 1）DSP芯片在多媒体通信领域的应用。 媒体数据传输产生的信息量是巨大的，多媒体网络终端在整个过程中需要对获取的信息量进行快速分析和处理，因此 DSP 被运用在语音编码，图像压缩和减少语音通信上。如今 DSP 对于语音解码计算产生实时效果，设计协议要求已经成为最基本的一条国际标准。
2）DSP芯片在工业控制领域的应用。 在工业控制领域， 工业机器人被广泛应用，对机器人控制系统的性能要求也越来越高。机器人控制系统重中之重就是实时性，在完成一个动作的同时会产生较多的数据和计算处理，这里可以采用高性能的 DSP。DSP通过应用到机器人的控制系统后，充分利用自身的实时计算速度特性，使得机器人系统可以快速处理问题，随着不断提高 DSP 数字信号芯片速度，在系统中容易构成并行处理网络，大大提高控制系统的性能，使得机器人系统得到更为广泛的发展。
3）DSP芯片在仪器仪表领域的应用。 DSP 丰富的片内资源可以大大简化仪器仪表的硬件电路，实现仪器仪表的 SOC 设计。器仪表的测量精度和速度是一项重要的指标，使用 DSP 芯片开发产品可使这两项指标大大提高。例如 TI 公司的 TMS320F2810 具有高效的 32 位 CPU 内核，12 位 A/D 转换器，丰富的片上存储器和灵活的指挥系统，为高精密仪器搭建了广阔的平台。高精密仪器现在已经发展成为 DSP 的一个重要应用，正处于快速传播时期，将推动产业的技术创新。
4）DSP芯片在汽车安全与无人驾驶领域的应用。 汽车电子系统日益兴旺发达起来，诸如装设红外线和毫米波雷达，将需用 DSP 进行分析。如今，汽车愈来愈多，防冲撞系统已成为研究热点。而且，利用摄像机拍摄的图像数据需要经过 DSP 处理，才能在驾驶系统里显示出来，供驾驶人员参考。
5）DSP芯片在军事领域的应用。 DSP 的功耗低、体积小、实时性反应速度都是武器装备中特别需要的。如机载空空导弹，在有限的体积内装有红外探测仪和相应的 DSP信号处理器等部分，完成目标的自动锁定与跟踪。先进战斗机上装备的目视瞄准器和步兵个人携带的头盔式微光仪，需用 DSP 技术完成图像的滤波与增强，智能化目标搜索捕获。DSP 技术还用于自动火炮控制、巡航导弹、预警飞机、相控阵天线等雷达数字信号处理中。
未来DSP技术将向以下几个方向继续发展： 1）DSP芯核集成度越来越高。 缩小 DSP 芯片尺寸一直是 DSP 技术的发展趋势，当前使用较多的是基于 RISC 结构，随着新工艺技术的引入，越来越多的制造商开始改进DSP 芯核，并且把多个 DSP 芯核、 MPU 芯核以及外围的电路单元集成在一个芯片上，实现了 DSP 系统级的集成电路。
2）可编程DSP芯片将是未来主导产品。 随着个性化发展的需要， DSP 的可编程化为生产厂商提供了更多灵活性，满足厂家在同一个 DSP 芯片上开发出更多不同型号特征的系列产品，也使得广大用户对于 DSP 的升级换代。例如冰箱、洗衣机，这些原来装有微控制器的家电如今已换成可编程 DSP 来进行大功率电机控制。
3）定点DSP占据主流。 目前，市场上所销售的 DSP 器件中，占据主流产品的依然是16 位的定点可编程 DSP 器件，随着 DSP 定点运算器件成本的不断低，能耗越来越小的优势日渐明显，未来定点 DSP 芯片仍将是市场的主角。
DSP芯片的分类 DSP的芯片可以按照以下的三种方式进行分类。
（1）按基础特性分 这是根据DSP芯片的工作时钟和指令类型来分类的。如果DSP芯片在某时钟频率范围内的任何频率上能正常工作，除计算速度有变化外，没有性能的下降，这类DSP芯片一般称之为静态DSP芯片。
如果有两种或两种以上的DSP芯片,它们的指令集和相应的机器代码机管脚结构相互兼容,则这类DSP芯片称之为一致性的DSP芯片。
（2）按数据格式分 这是根据DSP芯片工作的数据格式来分类的。数据以定点格式工作的DSP芯片称之为定点DSP芯片。以浮点格式工作的称为DSP芯片。不同的浮点DSP芯片所采用的浮点格式不完全一样，有的DSP芯片采用自定义的浮点格式，有的DSP芯片则采用IEEE的标准浮点格式。
（3）按用途分 按照DSP芯片的用途来分，可分为通用型DSP芯片和专用型的DSP芯片。通用型DSP芯片适合普通的DSP应用，如TI公司的一系列DSP芯片。专用型DSP芯片为特定的DSP运算而设计，更适合特殊的运算，如数字滤波，卷积和FFT等。
DSP芯片的基本结构 DSP芯片的基本结构包括： （1）哈佛结构。哈佛结构的主要特点是将程序和数据存储在不同的存储空间中，即程序存储器和数据存储器是两个相互独立的存储器，每个存储器独立编址，独立访问。与两个存储器相对应的是系统中设置了程序总线和数据总线，从而使数据的吞吐率提高了一倍。由于程序和存储器在两个分开的空间中，因此取指和执行能完全重叠。
（2）流水线操作。流水线与哈佛结构相关，DSP芯片广泛采用流水线以减少指令执行的时间，从而增强了处理器的处理能力。处理器可以并行处理二到四条指令，每条指令处于流水线的不同阶段。
（3）专用的硬件乘法器。乘法速度越快，DSP处理器的性能越高。由于具有专用的应用乘法器，乘法可在一个指令周期内完成。
（4）特殊的DSP指令。特殊的DSP指令DSP芯片是采用特殊的指令。
（5）快速的指令周期。快速的指令周期哈佛结构、流水线操作、专用的硬件乘法器、特殊的DSP指令再加上集成电路的优化设计可使DSP芯片的指令周期在200ns以下。
DSP系统的特点、构成和设计过程 数字信号处理系统是以数字信号处理为基础，因此具有数字处理的全部特点：
 接口方便。DSP系统与其它以现代数字技术为基础的系统或设备都是相互兼容，这样的系统接口以实现某种功能要比模拟系统与这些系统接口要容易的多。 编程方便。DSP系统中的可编程DSP芯片可使设计人员在开发过程中灵活方便地对软件进行修改和升级。 稳定性好。DSP系统以数字处理为基础，受环境温度以及噪声的影响较小，可靠性高。 精度高。16位数字系统可以达到的精度。 可重复性好。模拟系统的性能受元器件参数性能变化比较大，而数字系统基本上不受影响，因此数字系统便于测试，调试和大规模生产。 集成方便。DSP系统中的数字部件有高度的规范性，便于大规模集成。   DSP系统的设计过程  根据需求确定DSP系统的性能指标 算法研究及模拟实现和功能验证 选择适合的DSP芯片和外围组件 软件设计及调试 硬件设计及调试 系统集成及测试  定点DSP和浮点DSP的区别 一般来说，定点DSP处理器具有速度快，功耗低，价格便宜的特点；而浮点DSP处理器则计算精确，动态范围大，速度快，易于编程，功耗大，价格高。
而它们的区别，还可以从各方面去比较。
 宏观上  从宏观上讲，浮点DSP比定点DSP的动态范围大得多。定点运算中，程序员必须时刻关注溢出的发生，为了防止溢出，要么不断进行移位定标，要么做截尾。前者耗费大量时间和空间，后者则带来精度的损失。相反，浮点运算DSP扩大了动态范围，提高了精度，节省了运算时间和存储空间，因而大大减少了定标，移位和溢出检查。
硬件上  单纯从技术的角度来看，定点与浮点的区别主要在两个方面，即硬件和软件。硬件上的区别来自于：浮点DSP处理器具有浮点/整数乘法器，整数/浮点算术逻辑运算单元ALU，适合存放扩展精度的浮点结果的寄存器等。
软件上  再看看在软件开发上的不同之处，主要有浮点DSP编程的特点以及注意事项；定点DSP进行浮点运算时的定标，移位，检测溢出操作。比较两个浮点数时，永远不要使用操作符==来判断是否相等。即使比较两个相同的数，还是可能有微小的舍入差别。甚至定义精确的0，也不是很安全，尽管C语言中有0的表示，永远不要写这样的代码(x==0)，而应该写成(fabs(x) &lt; TINY)，其中TINY定义为一个很小的值，也就是处理器的浮点格式舍入误差。
应用实例  另外一个比较重要区别涉及应用场合对定点与浮点dsp处理器的选择。设计师关心的是最后的系统性能、成本以及上市时间。
例如，在移动电视中，没必要进行浮点处理。而在军用雷达中，经常用到浮点处理器。
DSP芯片的选择依据  运算速度 运算精度 功耗 价格 硬件资源 开发工具 ]]></content>
  </entry>
  
  <entry>
    <title>如何为Ubuntu 22.04安装拼音输入法</title>
    <url>/post/linux/how-to-install-pinyin-for-ubuntu-22.04.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>Input Method</tag>
      <tag>Pinyin</tag>
    </tags>
    <content type="html"><![CDATA[本文的指令原本只是针对Vanilla Ubuntu，而且只是针对Ubuntu 22.04来讲述基于简体汉字的基本拼音输入法。
Ubuntu到目前为止都没有提供一个简单，很好的文档化的指南来描述如何添加拼音输入法。但是，为了在Ubuntu 22.04上获得基本的拼音输入法的支持，你可以简单地：
依次打开 Settings(设定) -&gt; Region &amp; Language(区域和语言) -&gt; Manage Installed Languages(管理安装的语言) -&gt; Install / Remove languages(安装/删除语言)。
选择Chinese(Simplified)，确保键盘输入法系统已经选择了Ibus，然后点Apply(应用)，重启系统。
重新登陆系统，再次打开Settings(设定)，依次选择到键盘。
点击输入源的&quot;+&ldquo;图标，选择中文(中国)，然后中文(智能拼音)。
现在你应该可以看到一个小的&quot;en&quot;图表(或者你的Ubuntu安装的任何语言代码)在你的主屏幕的右上角，你可以点击然后看到可用的输入法的列表，包含中文(智能拼音)。
选中中文(智能拼音)，然后打开任何可以接受输入的应用(比如gedit, openoffice, vim, etc.)，你也可以通过快捷键 Win+space来进行输入法的切换。
再次重新启动系统，确保输入法的图表还在那儿。
]]></content>
  </entry>
  
  <entry>
    <title>Linux下大文件切割与合并</title>
    <url>/post/linux/linux-big-file-cut-and-cat.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>file</tag>
    </tags>
    <content type="html"><![CDATA[往往是因为网络传输的限制，导致很多时候，我们需要在 Linux 系统下进行大文件的切割。这样将一个大文件切割成为多个小文件，进行传输，传输完毕之后进行合并即可。
文件切割split 在 Linux 系统下使用 split 命令进行大文件切割很方便  命令语法 split [-a] [-d] [-l &lt;行数&gt;] [-b &lt;字节&gt;] [-C &lt;字节&gt;] [要切割的文件] [输出文件名] 使用实例 $ split -l 300000 users.sql /data/users_ $ split -d -l 300000 users.sql /data/users_ $ split -d -b 100m users.sql /data/users_ 帮助信息 $ split --help 文件合并 - cat 在 Linux 系统下使用 cat 命令进行多个小文件的合并也很方便  命令语法 cat [-n] [-e] [-t] [输出文件名] 使用实例 $ cat /data/users_* &gt; users.sql 帮助信息 $ cat --h ]]></content>
  </entry>
  
  <entry>
    <title>从实用的角度聊聊MOS管</title>
    <url>/post/hardware/mosfet-tube-application-introduction.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>MOS Tube</tag>
    </tags>
    <content type="html"><![CDATA[说起MOS管，有些人的脑子里可能是一团浆糊。大部分的教材都会告诉你长长的一段话：MOS管全称金属氧化半导体场效应晶体管
英文名Metal-Oxide-Semiconductor Field-Effect Transistor，属于绝缘栅极场效晶体管，以硅片为秤体，利用扩散工艺制作&hellip;&hellip;.有N沟道和P沟道两个型。不仅如此，它还有两个兄弟，分别是结型场效应管以及晶体场效应管&hellip;
面对这么大一段话，我不知道你有没有搞明白，反正我大学里是完全没有搞明白，学了一个学期就学了个寂寞。
那么，为什么这些教材要这么的反人类，他们难道就不能好好写说人话吗？
我大概分析了一下，因为同一本教材他需要面对不同专业的学生，所以教材最重要的是严谨。和全面相比是不是通俗易懂就没有那么重要了。而且一般的教材也不会告诉你学了有什么用，这就导致了在学习中你很容易迷失在这些概念中，抓不到重点。
那本文呢，我想根据自己的工作学习经历，抛开书本上这些教条的框架，从应用侧出发来给大家介绍一下MOS管里面最常见的，也是最容易使用的一种：增强型NMOS管，简称NMOS。当你熟悉了这个NMOS的使用之后，再回过头去看这个教材上的内容，我相信就会有不同的体会了。
NMOS的用法 首先来看这么一张简单的图（图1），我们可以用手去控制这个开关的开合，以此来控制这个灯光的亮灭。
图1
那如果我们想要用Arduino或者单片机去控制这个灯泡的话，就需要使用MOS管来替换掉这个开关了。为了更加符合我们工程的实际使用习惯呢，我们需要把这张图稍微转换一下，就像如图2这样子。
图2
那这两张图是完全等价的，我们可以看到MOS管是有三个端口，也就是有三个引脚，分别是GATE、DRAIN和SOURCE。至于为啥这么叫并不重要，只要记住他们分别简称G、D、S就可以。
图3
我们把单片机的一个IO口接到MOS管的gate端口，就可以控制这个灯泡的亮灭了。当然别忘了供电。当这个单片机的IO口输出为高的时候，NMOS就等效为这个被闭合的开关，指示灯光就会被打开；那输出为低的时候呢，这个NMOS就等效为这个开关被松开了，那此时这个灯光就被关闭，是不很简单。
那如果我们不停的切换这个开关，那灯光就会闪烁。如果切换的这个速度再快一点，因为人眼的视觉暂留效应，灯光就不闪烁了。此时我们还能通过调节这个开关的时间来调光，这就是所谓的PWM波调光，以上就是MOS管最经典的用法，它实现了单片机的IO口控制一个功率器件。当然你完全可以把灯泡替换成其他的器件。器件比如说像水泵、电机、电磁铁这样的东西。
图4：PWM波调光
如何选择NMOS 明白了NMOS的用法之后呢，我们来看一下要如何选择一个合适的NMOS，也就是NMOS是如何选型的。
那对于一个初学者来说，有四个比较重要的参数需要来关注一下。第一个是封装，第二个是Vgs(th)，第三个是Rds(on)上，第四个是Cgs。
封装比较简单，它指的就是一个MOS管这个外形和尺寸的种类也有很多。一般来说封装越大，它能承受的电流也就越大。为了搞明白另外三个参数呢，我们先要来介绍一下NMOS的等效模型。
图5：NMOS等效模型
MOS其实可以看成是一个由电压控制的电阻。这个电压指的是G、S的电压差，电阻指的是D、S之间的电阻。这个电阻的大小会随着G、S电压的变化而变化。当然它们不是线性对应的关系，实际的关系差不多像这样的，横坐标是G、S电压差。
图6：Rds与Vgs关系图
纵坐标是电阻的值，当G、S的电压小于一个特定值的时候呢，电阻基本上是无穷大的。然后这个电压值大于这个特定值的时候，电阻就接近于零，至于说等于这个值的时候会怎么样，我们先不用管这个临界的电压值，我们称之为Vgs(th)，也就是打开MOS管需要的G、S电压，这是每一个MOS管的固有属性，我们可以在MOS管的数据手册里面找到它。
图7：MOS管数据手册
显然，Vgs(th)一定要小于这个高电平的电压值，否则就没有办法被正常的打开。所以在你选择这个MOS管的时候，如果你的高电平是对应的5V，那么选3V左右的Vgs(th)是比较合适的。太小的话会因为干扰而误触发，太大的话又打不开这个MOS管。
接下来，我们再来看看NMOS的第二个重要参数Rdson，刚才有提到NMOS被完全打开的时候，它的电阻接近于零。但是无论多小，它总归是有一个电阻值的，这就是所谓的Rds(on)。它指的是NMOS被完全打开之后，D、S之间的电阻值。同样的你也可以在数据手册上找到它。这个电阻值当然是越小越好。越小的话呢，它分压分的少，而且发热也相对比较低。但实际情况一般Rds(on)越小，这个NMOS的价格就越高，而且一般对应的体积也会比较大。所以还是要量力而行，选择恰好合适。
最后说一下Cgs，这个是比较容易被忽视的一个参数，它指的是G跟S之间的寄生电容。所有的NMOS都有，这是一个制造工艺的问题，没有办法被避免。
那它会影响到NMOS打开速度，因为加载到gate端的电压，首先要给这个电容先充电，这就导致了G、S的电压并不能一下子到达给定的一个数值。
图8
它有一个爬升的过程。当然因为Cgs比较小，所以一般情况下我们感觉不到它的存在。但是当我们把这个时间刻度放大的时候，我们就可以发现这个上升的过程了。对于这个高速的PWM波控制场景是致命的。当PWM波的周期接近于这个爬升时间时，这个波形就会失真。一般来说Cgs大小和Rds(on)是成反比的关系。Rds(on)越小，Cgs就越大。所以大家要注意平衡他们之间的关系。
以上就是关于NMOS大家需要初步掌握的知识了，希望能对大家有所帮助。
]]></content>
  </entry>
  
  <entry>
    <title>MOS管驱动电路设计</title>
    <url>/post/hardware/the-drive-circuit-design-of-MOS-tube.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>MOS Tube</tag>
    </tags>
    <content type="html"><![CDATA[MOS管因为其导通内阻低，开关速度快，因此被广泛应用在开关电源上。而用好一个MOS管，其驱动电路的设计就很关键。下面分享几种常用的驱动电路。
电源IC直接驱动 电源IC直接驱动是最简单的驱动方式，应该注意几个参数以及这些参数的影响。
 查看电源IC手册的最大驱动峰值电流，因为不同芯片，驱动能力很多时候是不一样的。 了解MOS管的寄生电容，如图C1、C2的值，这个寄生电容越小越好。如果C1、C2的值比较大，MOS管导通的需要的能量就比较大，如果电源IC没有比较大的驱动峰值电流，那么管子导通的速度就比较慢，就达不到想要的效果。  推挽驱动 当电源IC驱动能力不足时，可用推挽驱动。 这种驱动电路好处是提升电流提供能力，迅速完成对于栅极输入电容电荷的充电过程。这种拓扑增加了导通所需要的时间，但是减少了关断时间，开关管能快速开通且避免上升沿的高频振荡。
加速关断驱动 MOS管一般都是慢开快关。在关断瞬间驱动电路能提供一个尽可能低阻抗的通路供MOSFET栅源极间电容电压快速泄放，保证开关管能快速关断。 为使栅源极间电容电压的快速泄放，常在驱动电阻上并联一个电阻和一个二极管，如上图所示，其中D1常用的是快恢复二极管。这使关断时间减小，同时减小关断时的损耗。Rg2是防止关断的时电流过大，把电源IC给烧掉。
如上图，是我之前用的一个电路，量产至少上万台，推荐使用。 用三极管来泄放栅源极间电容电压是比较常见的。如果Q1的发射极没有电阻，当PNP三极管导通时，栅源极间电容短接，达到最短时间内把电荷放完，最大限度减小关断时的交叉损耗。 还有一个好处，就是栅源极间电容上的电荷泄放时电流不经过电源IC，提高了可靠性。
隔离驱动 为了满足高端MOS管的驱动，经常会采用变压器驱动。其中R1目的是抑制PCB板上寄生的电感与C1形成LC振荡，C1的目的是隔开直流，通过交流，同时也能防止磁芯饱和。
]]></content>
  </entry>
  
  <entry>
    <title>SPI总线详解</title>
    <url>/post/hardware/spi-bus-introduction.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>SPI</tag>
    </tags>
    <content type="html"><![CDATA[SPI是串行外设接口（Serial Peripheral Interface）的缩写，是一种高速的，全双工，同步的通信总线，并且在芯片的管脚上只占用四根线，节约了芯片的管脚，同时为PCB的布局上节省空间，提供方便，正是出于这种简单易用的特性，越来越多的芯片集成了这种通信协议，比如AT91RM9200。
什么是SPI？ SPI是串行外设接口(Serial Peripheral Interface)的缩写，是 Motorola 公司推出的一种同步串行接口技术，是一种高速、全双工、同步的通信总线。
SPI优点  支持全双工通信 通信简单 数据传输速率块 SPI电路图  缺点 没有指定的流控制，没有应答机制确认是否接收到数据，所以跟IIC总线协议比较在数据可靠性上有一定的缺陷。
特点  高速、同步、全双工、非差分、总线式 主从机通信模式  SPI电路连接  SPI的通信原理很简单，它以主从方式工作，这种模式通常有一个主设备和一个或多个从设备，有三线制和四线制之分。信号线包括SDI(串行数据输入 Serial Digital IN)、SDO(串行数据输出 Serial Digital OUT)、SCLK(时钟)、CS(片选)。 SDO/MOSI – 主设备数据输出，从设备数据输入 SDI/MISO – 主设备数据输入，从设备数据输出 SCLK – 时钟信号，由主设备产生; CS/SS – 从设备使能信号，由主设备控制。当有多个从设备的时候，因为每个从设备上都有一个片选引脚接入到主设备机中，当主设备和某个从设备通信时将需要将从设备对应的片选引脚电平拉低(一般低有效)。  SPI通信模式分析 SPI通信有4种不同的模式，不同的从设备在出厂时配置模式已经固定， 这是不能改变的，但通信双方设备必须工作在同一模式下，所以可以对主设备的SPI模式进行配置，通过CPOL（时钟极性）和CPHA（时钟相位）来控制主设备的通信模式。
具体模式具体如下：
 Mode0：CPOL=0，CPHA=0 Mode1：CPOL=0，CPHA=1 Mode2：CPOL=1，CPHA=0 Mode3：CPOL=1，CPHA=1     模式 CPOL CPHA     Mode0 0 0   Mode1 0 1   Mode2 1 0   Mode3 1 1    时钟极性CPOL是用来配置SCLK电平的有效态的;
时钟相位CPHA是用来配置数据采样是发生在第几个边沿的。
 CPOL=0表示当SCLK=0时处于空闲态，所以SCLK处于高电平时有效； CPOL=1表示当SCLK=1时处于空闲态，所以SCLK处于低电平时有效； CPHA=0表示数据采样是在第1个边沿，数据发送在第2个边沿； CPHA=1表示数据采样是在第2个边沿，数据发送在第1个边沿； SPI主模块和与之通信的外设通信时，两者的时钟相位和极性应该保持一致。  SPI 时序详解 CPOL=0，CPHA=0：此时空闲态时，SCLK处于低电平，数据采样是在第1个边沿，也就是SCLK由低电平到高电平的跳变，所以数据采样是在上升沿，数据发送是在下降沿。
CPOL=0，CPHA=1：此时空闲态时，SCLK处于低电平，数据发送是在第1个边沿，也就是SCLK由低电平到高电平的跳变，所以数据采样是在下降沿，数据发送是在上升沿。
CPOL=1，CPHA=0：此时空闲态时，SCLK处于高电平，数据采集是在第1个边沿，也就是SCLK由高电平到低电平的跳变，所以数据采集是在下降沿，数据发送是在上升沿。
CPOL=1，CPHA=1：此时空闲态时，SCLK处于高电平，数据发送是在第1个边沿，也就是SCLK由高电平到低电平的跳变，所以数据采集是在上升沿，数据发送是在下降沿。
注意：SPI主设备能够控制时钟信号，因为SPI通信并不像UART或者IIC通信那样有专门的通信周期、通信起始信号、通信结束信号；所以SPI协议只能通过控制时钟信号线，在没有数据交流的时候，时钟线要么是保持高电平，要么是保持低电平。
例如：工作在模式0这种时序（CPOL＝0，CPHA＝0），如下：
我们来关注SCK的第一个时钟周期，在时钟的前沿采样数据（上升沿，第一个时钟沿），在时钟的后沿输出数据（下降沿，第二个时钟沿）。首先来看主器件，主器件的输出口（MOSI）输出的数据bit1，在时钟的前沿被从器件采样，那主器件是在何时刻输出bit1的呢？bit1的输出时刻实际上在SCK信号有效以前，比SCK的上升沿还要早半个时钟周期。bit1的输出时刻与SSEL信号没有关系。再来看从器件，主器件的输入口MISO同样是在时钟的前沿采样从器件输出的bit1的，那从器件又是在何时刻输出bit1的呢。从器件是在SSEL信号有效后，立即输出bit1，尽管此时SCK信号还没有起效。
从这张图就可以很清楚的看出主从器件的bit1是怎样输出的。
]]></content>
  </entry>
  
  <entry>
    <title>10个超赞的C语言开源项目</title>
    <url>/post/linux/ten-great-c-language-open-source-project.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>C language</tag>
      <tag>open source</tag>
    </tags>
    <content type="html"><![CDATA[今天给大家分享10个超赞的C语言开源项目，希望这些内容能对大家有所帮助！
Webbench Webbench是一个在 Linux 下使用的非常简单的网站压测工具。
它使用fork()模拟多个客户端同时访问我们设定的URL，测试网站在压力下工作的性能。
最多可以模拟 3 万个并发连接去测试网站的负载能力。Webbench使用C语言编写，代码非常简洁，源码加起来不到 600 行。
项目地址 http://home.tiscali.cz/~cz210552/webbench.html Tinyhttpd tinyhttpd是一个超轻量型Http Server，使用C语言开发，全部代码只有 502 行（包括注释），附带一个简单的 Client
可以通过阅读这段代码理解一个 Http Server 的本质。
项目地址 http://sourceforge.net/projects/tinyhttpd/ cJSON cJSON是C语言中的一个JSON编解码器，非常轻量级，C文件只有 500 多行，速度也非常理想。
虽然cJSON功能不是非常强大，但cJSON的小身板和速度是最值得赞赏的。
其代码被非常好地维护着，结构也简单易懂，可以作为一个非常好的C语言项目进行学习。
项目主页 http://sourceforge.net/projects/cjson/ CMockery CMockery是google发布的用于C单元测试的一个轻量级的框架。
它很小巧，对其他开源包没有依赖，对被测试代码侵入性小。
CMockery 的源代码行数不到3K，阅读一下will_return和mock的源代码就一目了然了。
主要特点  免费且开源，google 提供技术支持； 轻量级的框架，使测试更加快速简单； 避免使用复杂的编译器特性，对老版本的编译器来讲，兼容性好； 并不强制要求待测代码必须依赖 C99 标准，这一特性对许多嵌入式系统的开发很有用。   项目地址 http://code.google.com/p/cmockery/downloads/list Libev libev 是一个开源的事件驱动库，基于 epoll、kqueue 等 OS 提供的基础设施。
其以高效出名，它可以将 IO 事件、定时器、和信号统一起来，统一放在事件处理这一套框架下处理。
基于 Reactor 模式，效率较高，并且代码精简（4.15 版本 8000 多行），是学习事件驱动编程的很好的资源。
项目地址 http://software.schmorp.de/pkg/libev.html Memcached Memcached 是一个高性能的分布式内存对象缓存系统，用于动态 Web 应用以减轻数据库负载。
它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提供动态数据库驱动网站的速度。
Memcached 基于一个存储键/值对的 hashmap。Memcached-1.4.7 的代码量还是可以接受的，只有 10K 行左右。
项目地址 http://memcached.org/ Lua Lua 很棒，在任何支持 ANSI C 编译器的平台上都可以轻松编译通过。 Lua 的代码数量足够小，5.1.4 仅仅 1.5W 行，去掉空白行和注释估计能到 1W 行。
项目地址 http://www.lua.org/ SQLite SQLite 是一个开源的嵌入式关系数据库，实现自包容、零配置、支持事务的 SQL 数据库引擎。其特点是高度便携、使用方便、结构紧凑、高效、可靠。
足够小，大致 3 万行C代码，250K。
项目地址 http://www.sqlite.org/ UNIX v6 UNIX V6 的内核源代码包括设备驱动程序在内约有 1 万行，这个数量的源代码，初学者是能够充分理解的。有一种说法是一个人所能理解的代码量上限为 1 万行，UNIX V6 的内核源代码从数量上看正好在这个范围之内。
看到这里，大家是不是也有“如果只有 1 万行的话没准儿我也能学会”的想法呢？
另一方面，最近的操作系统，例如 Linux   最新版的内核源代码据说超过了 1000 万行。
就算不是初学者，想完全理解全部代码基本上也是不可能的。
项目地址 http://minnie.tuhs.org/cgi-bin/utree.pl?file=V6 NETBSD NetBSD 是一个免费的，具有高度移植性的 UNIX-like 操作系统。
NetBSD 计划的口号是：“Of course it runs NetBSD”。
它设计简洁，代码规范，拥有众多先进特性，使得它在业界和学术界广受好评。
由于简洁的设计和先进的特征，使得它在生产和研究方面，都有卓越的表现，而且它也有受使用者支持的完整的源代码。
许多程序都可以很容易地通过 NetBSD Packages Collection 获得。
]]></content>
  </entry>
  
  <entry>
    <title>Linux环境监控工具基础参考</title>
    <url>/post/linux/linux-environment-monitor-tool.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>monitor tool</tag>
    </tags>
    <content type="html"><![CDATA[Linux 操作系统有很多自带和第三方监控工具，这篇文章从不同维度整理了一些，但仅限基础了解，因为，单独讲任何一个指令，都可以成篇文章，具体指令参数，可以检索 man，从中理解。
CPU top(任务管理工具)
top -n 1 -b vmstat(展现给定时间间隔的服务器的状态值，包括服务器的 CPU 使用率，内存使用)
vmstat 1 10 #每1秒采集一次共采集10次 pidstat(进程实时监控)
pidstat -u 1 -p pid mpstat(多 CPU 实时监控工具)
mpstat -P ALL 1 5 sar(性能监控和瓶颈检查)
sar -u dstat(dstat 是一个可以取代 vmstat，iostat，netstat 和 ifstat 这些命令的多功能产品)
dstat 2 10 #每2秒采集一次共采集10次 内存 top
top -n 1 -b pidstat
pidstat -r free(查看当前系统的物理内存使用情况)
free -mh sar(性能监控和瓶颈检查)
sar -r 10 3 #每10秒采样一次，连续采样3次 vmstat
vmstat 2 1 磁盘 IO iostat(IO 实时监控)
iostat -d -x -k 1 10 iotop(监控系统中各个进程对 IO 的使用量)
iotop pidstat
示例: pidstat -d sar
sar -d vmstat
vmstat 2 1 网络 netstat(监控 TCP/IP 网络)
netstat -nltup iftop(实时流量监控工具)
iftop -i em2 ss(获取 socket 统计信息，他可以显示和 netstat 类似的内容)
ss -aA tcp sar
sar -n EDEV 1 5 tcpdump(抓包工具)
tcpdump -i em1 host 192.168.1.1 and port 80 tcpflow(分析网络流量)
tcpflow -i em1 port 80 nload(用于查看 Linux 网络流量状况，实时输出)
nload -t 200 -i 1024 -o 128 -U M 系统负载 (1) CPU 负载说明
 如果某个程序频繁的进行计算、逻辑判断等操作，那么此类程序主要依赖于 CPU 的处理速度，故称之为 &ldquo;计算密集型程序&rdquo;。
 (2) IO 负载说明
 如果某个程序频繁的从磁盘中读取写入文件，那么这种类型的操作主要依赖于磁盘的读取速度，也就是输入输出 (input/output)，简写为 I/O。此类 I/O 负载的程序，称为 I/O 密集型程序。
 top
top uptime
uptime sar
sar -q 1 20 其他工具  htop(类似 top，比 top 更加人性化) glances(类似 top，基于 Python 的系统遥测监控工具) strace(常用来跟踪进程执行时的系统调用和所接收的信号) dtrace(动态跟踪) valgrind(内存泄漏检测) dmesg(内核信息) ]]></content>
  </entry>
  
  <entry>
    <title>详解TCP和UDP协议的原理和区别</title>
    <url>/post/linux/tcp-and-udp-principle-and-difference.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>tcp</tag>
      <tag>udp</tag>
    </tags>
    <content type="html"><![CDATA[ 最近重新认知了一下TCP和UDP的原理以及区别，做一个简单的总结。
 作用 首先，tcp和udp都是工作再传输层，用于程序之间传输数据的。数一般包含：文件类型，视频类型，jpg图片等。
区别 TCP是基于连接的，而UDP是基于非连接的。
tcp传输数据稳定可靠，适用于对网络通讯质量要求较高的场景，需要准确无误的传输给对方，比如，传输文件，发送邮件，浏览网页等等。
udp的优点是速度快，但是可能产生丢包，所以适用于对实时性要求较高但是对少量丢包并没有太大要求的场景。比如：域名查询，语音通话，视频直播等。udp还有一个非常重要的应用场景就是隧道网络，比如：vpn，VXLAN。
以人与人之间的通信为例：UDP协议就相当于是写信给对方，寄出去信件之后不能知道对方是否收到信件，信件内容是否完整，也不能得到及时反馈，而TCP协议就像是打电话通信，在这一系列流程都能得到及时反馈，并能确保对方及时接收到。如下图：
TCP通信的过程 tcp是如何保证以上过程的:分为三个步骤，三次握手，传输确认，四次挥手。三次握手是建立连接的过程。
三次握手 当客户端向服务端发起连接时，会先发一包连接请求数据，过去询问一下，能否与你建立连接？这包数据称之为SYN包，如果对端同意连接，则回复一包SYN+ACK包，客户端收到之后，发送一包ACK包，连接建立，因为这个过程中互相发送了三包数据，所以称之为三次握手。
为什么要三次握手而不是两次握手？
**这是为了防止，因为已失效的请求报文，突然又传到服务器，引起错误，**这是什么意思？
假设采用两次握手建立连接，客户端向服务端发送一个syn包请求建立连接，因为某些未知的原因，并没有到达服务器，在中间某个网络节点产生了滞留，为了建立连接，客户端会重发syn包，这次的数据包正常送达，服务端发送syn+ack之后就建立起了连接，但是第一包数据阻塞的网络突然恢复，第一包syn包又送达到服务端，这是服务端会认为客户端又发起了一个新的连接，从而在两次握手之后进入等待数据状态，服务端认为是两个连接，而客户端认为是一个连接，造成了状态不一致，如果在三次握手的情况下，服务端收不到最后的ack包，自然不会认为连接建立成功，所以三次握手本质上来说就是为了解决网络信道不可靠的问题，为了在不可靠的信道上建立起可靠的连接，经过三次握手之后，客户端和服务端都进入了数据传输状态。
数据传输 数据传输： 一包数据可能会被拆成多包发送,如何处理丢包问题，这些数据包到达的先后顺序不同，如何处理乱序问题？针对这些问题，tcp协议为每一个连接建立了发送缓冲区，从建立链接后的第一个字节的序列号为0，后面每个字节的序列号就会增加1，发送数据时，从数据缓冲区取一部分数据组成发送报文，在tcp协议头中会附带序列号和长度，接收端在收到数据后需要回复确认报文，确认报文中的ack等于接受序列号加长度，也就是下包数据发送的起始序列号，这样一问一答的发送方式，能够使发送端确认发送的数据已经被对方收到，发送端也可以发送一次的连续的多包数据，接受端只需要回复一次ack就可以了如图：
四次挥手 处于连接状态的客户端和服务端，都可以发起关闭连接请求，此时需要四次挥手来进行连接关闭，假设客户端主动发起连接关闭请求，他给服务端发起一包FIN包，标识要关闭连接，自己进入终止等待1装填，服务端收到FIN包，发送一包ACK包，标识自己进入了关闭等待状态，客户端进入终止等待2状态。这是第二次挥手，服务端此时还可以发送未发送的数据，而客户端还可以接受数据，待服务端发送完数据之后，发送一包FIN包，最后进入确认状态，这是第3次挥手，客户端收到之后恢复ACK包，进入超时等待状态，经过超时时间后关闭连接，而服务端收到ACK包后，立即关闭连接，这是第四次挥手。为什么客户端要等待超时时间这是为了保证对方已经收到ACK包，因为假设客户端发送完最后一包ACK包后释放了连接，一旦ACK包在网络中丢失，服务端将一直停留在 最后确认状态，如果等待一段时间，这时服务端会因为没有收到ack包重发FIN包，客户端会响应 这个FIN包进行重发ack包，并刷新超时时间，这个机制跟第三次握手一样。也是为了保证在不可靠的网络链路中进行可靠的连接断开确认。
UDP协议 udp:首先udp协议是非连接的，发送数据就是把简单的数据包封装一下，然后从网卡发出去就可以了，数据包之间并没有状态上的联系，正因为udp这种简单的处理方式，导致他的性能损耗非常少，对于cpu,内存资源的占用也远小于tcp,但是对于网络传输过程中产生的丢包，udp并不能保证，所以udp在传输稳定性上要弱于tcp，所以，tcp和udp的主要却别：tcp传输数据稳定可靠，适用于对网络通讯质量要求较高的场景，需要准确无误的传输给对方，比如，传输文件，发送邮件，浏览网页等等，udp的优点是速度快，但是可能产生丢包，所以适用于对实时性要求较高但是对少量丢包并没有太大要求的场景。比如：域名查询，语音通话，视频直播等。udp还有一个非常重要的应用场景就是隧道网络，比如：vpn，VXLAN。
]]></content>
  </entry>
  
  <entry>
    <title>分享一种通信协议的应用编程原理和思路</title>
    <url>/post/linux/communication-protocol-programming-principle-and-idea.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>programming</tag>
      <tag>uart</tag>
      <tag>can</tag>
      <tag>usb</tag>
    </tags>
    <content type="html"><![CDATA[ 嵌入式开发过程中，UART、 CAN、 USB等通信基本离不开通信协议。
 下面给大家分享一种通信协议（MAVLink）在应用编程中的编程原理和思路。
应用编程主要内容 发送和接收说明 利用MAVLink通信协议进行编程，主要实现的功能就是：
发送端 将需要发送的数据（如：SysState, BatVol），添加MAVLink通信协议，通过硬件（如：UART、CAN）发送出去。
接收端 硬件（如：UART、CAN）接收到的数据，通过MAVLink协议解析，得到一帧完整的MAVLink数据包，提取发送端发送的数据（如：SysState, BatVol），将得到的数据应用到我们程序中。
主要流程：数据 -&gt; MAVLink封装 -&gt; 发送 -&gt; 接收 -&gt; MAVLink解析 -&gt;数据
发送和接收流程图 该流程图是结合我上一篇文章提供的源代码例程画出来，包含的只是主要内容，更多细节没有在流程图中呈现。
提示：
我提供例程是针对初学者提供比较单一发送和接收例程（MDK-ARM和EWARM包含各自的发送和接收工程）。
而实际项目可能会：
  发送和接收在一个工程；
  包含操作系统；
  发送、接收数据FIFO（队列）处理；
  所以，实际项目，请按需修改我提供的源码。
MAVLink函数接口详细说明 这一章节讲述发送和接收主要用到的函数接口，请参考我提供的源代码例程理解。 为方便初学者理解，我将其分为发送和接收两个部分来讲述。
发送主要函数接口 上面是我提供例程的代码，主要讲4个接口。
MAVLink_SendTest 这个接口是根据自己情况进行封装函数，用于应用程序调用，这里不多说。
mavlink_msg_sys_info_pack 这个函数接口主要目的：将变量信息（SysID、CompID、SysState、BatVol）打包，最终得到MAVLink_Msg这个消息包。
mavlink_msg_to_send_buffer 将上一步得到的MAVLink_Msg转换成我们要发送的数据BUF缓存。
MAV_USART_SendNByte 这个函数接口也是我自己根据硬件（UART）封装的，如果你是其它硬件通信，只需要封装一个类似的接口（参数具有BUF，LEN）即可。
发送数据的流程：从应用代码 -&gt; 底层硬件（发送出去）。
如果要深入了解，可以先熟悉软件流程，再结合源代码工程，同时参看接口函数具体实现。相信你很快就明白了。
接收主要函数接口 上面是我提供例程的代码（方便截图，去掉了部分），主要讲以上4点内容。
MAV_USART_GetByte 该函数接口也是硬件底层通信接口，请根据自己情况修改，只需要传递数据（流）进来即可。
mavlink_parse_char MAVLink解析是按照一个一个字符进行解析，我们接收到一个字符，就对其进行解析，直到解析完（根据返回标志判断）一帧数据为止。
if(MAVLINK_MSG_ID_SYS_INFO == MAVLinkMsg.msgid) 这里就是对解析好的一包完整消息进行分类判断吧。其实，我是想说，这个地方还有两个ID需要进行判断，SysID系统ID和CompID部件ID。
我提供例程为方便初学者快速理解，未提供SysID和CompID判断，在后续应用编程中会用到。
mavlink_msg_sys_info_get_voltage_battery 通过该接口获取消息变量，看图中说明文字，前面是消息，后面是消息变量。
接收数据的流程：从底层硬件（接收数据） -&gt; 应用代码。
以上就是发送和接收的主要函数接口，如果你只是简单的进行通信，这几个接口就够你使用了。当然，更高级的编程应用还需要你进一步掌握其中的内容。
]]></content>
  </entry>
  
  <entry>
    <title>Linux之awk使用技巧</title>
    <url>/post/linux/awk-usage-skills.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>awk</tag>
    </tags>
    <content type="html"><![CDATA[AWK 是一种处理文本文件的语言，是一个强大的文本分析工具。
之所以叫 AWK 是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的 Family Name 的首字符。
打印文件的第一列
awk &#39;{print $1}&#39; vxworks.txt 打印文件的前两列
awk &#39;{print $1,$2}&#39; vxworks.txt 打印文件的最后一列
awk &#39;{print $NF}&#39; vxworks.txt 打印文件的总行数
awk &#39;END{print NR}&#39; vxworks.txt 打印文件的第一行
awk &#39;NR==1{print}&#39; vxworks.txt NR是指awk正在处理的记录位于文件中的位置（行号）
打印文件的第3行第2列
sed -n &#39;3,1p&#39; vxworks.txt | awk &#39;{print $2}&#39; 删除空行
awk &#39;NF&#39; vxworks.txt 打印奇数行
awk &#39;b=!b&#39; vxworks.txt 打印文件按#分割后,行长度为3的所有行
awk -F &#39;#&#39; &#39;if(NF==3){print}&#39; vxworks.txt NF是指awk正在处理的记录包含几个域（字段），这与域分隔符有关，默认为空
统计Linux系统中每个用户所用的shell
cat /etc/passwd | awk -F &#34;:&#34; &#39;{print $1&#34; : &#34;$7}&#39; 用awk统计linux系统中所有的用户数
cat /etc/passwd | awk &#39;{count++}END{ print count}&#39; 统计某个文件夹下文件所占的字节数
ls -l | awk &#39;BEGIN{size=0}{size=size+$5}END{print size}&#39; 统计某个文件夹下文件所占的字节数,按M显示
ls -l | awk &#39;BEGIN{size=0}{size=size+$5}END{print size}&#39; netstat结合awk统计TCP连接数
netstat -tunlp | awk &#39;/^tcp/{++a[$6]}END{for(i in a) print i,a[i]}&#39; 过滤空行
awk &#39;/^[^$]/ {print $0}&#39; vxworks.txt 列运算
awk &#39;/^[^$]/ {print $0}&#39; vxworks.txt cat 1.txt 1 2 3 求和
cat 1.txt | awk &#39;{a+=$1}END{print a}&#39; 求平均值
cat 1.txt | awk &#39;{a+=$1}END{print a/NR}&#39; 求列的最大值
cat 1.txt | awk &#39;BEGIN{a=0}{if($1&gt;a) a=$1 fi}END{print a}&#39; ]]></content>
  </entry>
  
  <entry>
    <title>关于Linux下的crontab，你不知道的那些知识点</title>
    <url>/post/linux/linux-crontab-knowledge.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>crontab</tag>
    </tags>
    <content type="html"><![CDATA[实际工作中，crontab出现的问题是多种多样的，下面就深入介绍下crontab在具体工作中容易出现的问题和解决问题的办法。
crontab能干啥 crond是linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。
Linux下的任务调度分为两类，系统任务调度和用户任务调度。
 系统任务调度：系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。 用户任务调度：用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab 文件都被保存在 /var/spool/cron目录中。其文件名与用户名一致。  关于crontab的用途，在企业实际应用中非常广泛，常见的有定时数据备份、定时系统检测、定时数据收集、定时更新配置、定时生成报表等等。
crontab应用实例 crontab使用格式 crontab常用的使用格式有如下两种：
crontab [-u user] [file] crontab [-u user] [-e|-l|-r |-i] 选项含义如下：
  -u user：用来设定某个用户的crontab服务，例如，“-u ixdba”表示设定ixdba用户的crontab服务，此参数一般有root用户来运行。
  file：file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。
  -e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。
  -l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。
  -r：从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。
  -i：在删除用户的crontab文件时给确认提示。
  crontab文件语法 用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下：
minute hour day month week command 其中：
? minute：表示分钟，可以是从0到59之间的任何整数。 ? hour：表示小时，可以是从0到23之间的任何整数。 ? day：表示日期，可以是从1到31之间的任何整数。 ? month：表示月份，可以是从1到12之间的任何整数。 ? week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 ? command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 在以上各个字段中，还可以使用以下特殊字符：
? 星号（）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 ? 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” ? 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” ? 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如/10，如果用在minute字段，表示每十分钟执行一次。 几个crontab例子 0 /3 /usr/local/apache2/apachectl restart 表示每隔3个小时重启apache服务一次。
30 3 6 /webdata/bin/backup.sh 表示每周六的3点30分执行/webdata/bin/backup.sh脚本的操作。
0 0 1,20 fsck /dev/sdb8 表示每个月的1号和20号检查/dev/sdb8磁盘设备。
10 5 /5 * echo &#34;&#34;&gt;/usr/local/apache2/log/access_log 表示每个月的5号、10号、15号、20号、25号、30号的5点10分执行清理apache日志操作。
系统级任务调度/etc/crontab 在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。
/etc/crontab文件包括下面几行：
SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root HOME=/ # run-parts 01 * * * * root run-parts /etc/cron.hourly 02 4 * * * root run-parts /etc/cron.daily 22 4 * * 0 root run-parts /etc/cron.weekly 42 4 1 * * root run-parts /etc/cron.monthly 从上面的示例文件可看出，crontab的任务列表主要由两部分组成：环境变量配置与定时任务配置。可能大家在工作中更多是只用到了任务配置部分。
前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。第六至九行就是crontab执行格式的具体写法。
##crontab调试解析神器
通常在使用crontab添加任务时，我们会依靠自己已有知识编写定时语句。当需要测试语句是否正确时，还需要在服务器上不断调试，，这种方式太不高效了。有没有一款工具，只要我们给出语句，就能告诉具体执行时间以及对错呢？还真有，下面介绍一款老外开发的crontab在线解析工具。
工具地址：https://crontab.guru
给出这个工具的截图如下：
好用不好用，你试试就知道。
crontab使用的各种坑 环境变量问题 当我们刚使用crontab时，运维老鸟们一般会告知所有命令尽量都使用绝对路径，以防错误。这是为什么？这就和我们下面要谈的环境变量有关了。
首先，获取shell终端环境变量，内容如下：
[root@SparkWorker1 dylogs]# env XDG_SESSION_ID=1629 HOSTNAME=SparkWorker1 TERM=linux SHELL=/bin/bash HISTSIZE=1000 SSH_CLIENT=172.16.213.132 50080 22 HADOOP_PREFIX=/opt/hadoop/current CATALINA_BASE=/opt/hadoop/current/share/hadoop/httpfs/tomcat SSH_TTY=/dev/pts/1 QT_GRAPHICSSYSTEM_CHECKED=1 USER=root MAIL=/var/spool/mail/root PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/java/default/bin:/opt/hadoop/current/bin:/opt/hadoop/current/sbin:/root/bin PWD=/data/dylogs LANG=zh_CN.UTF-8 HOME=/root 要获取crontab环境变量信息，可以设置如下计划任务：
* * * * * /usr/bin/env &gt; /tmp/env.txt 等待片刻，env.txt输出内容如下：
[root@SparkWorker1 dylogs]# cat /tmp/env.txt XDG_SESSION_ID=1729 SHELL=/bin/sh USER=root PATH=/usr/bin:/bin PWD=/root LANG=zh_CN.UTF-8 SHLVL=1 HOME=/root LOGNAME=root XDG_RUNTIME_DIR=/run/user/0 _=/usr/bin/env 从上面输出结果可知，shell命令行的PATH值为
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/java/default/bin:/opt/hadoop/current/bin:/opt/hadoop/current/sbin:/root/bin 而crontab中的PATH值为：
PATH=/usr/bin:/bin 对比crontab环境变量与shell终端环境变量的输出，可以发现两者的差异很大。大家可能遇到过，在shell命令行执行脚本都没有问题，而放到crontab后却执行异常，或者执行失败，此时，我们就需要考虑是否命令涉及的环境变量在crontab和shell命令行间存在差异。
例如，我们在crontab中执行了如下定时任务：
20 16 * * * php autosave.php 而如果我们的php是安装在/usr/local/bin/目录下的话，那么上面这个定时任务由于无法找到php命令，会运行失败。
那么，知道了环境变量问题，可能导致计划任务无法正常执行，怎么才能避免这个问题呢，这个交给大家一个终极大招，可以在crontab中加入如下配置，保证你的计划任务执行不会出现环境变量问题：
* * * * * source /$HOME/.bash_profile &amp;&amp; command 这个其实是在执行计划任务命令之前，先加载了用户环境变量信息，由此可保证所有环境变量都可正常加载。
定时时间配置误区 时间是crontab的核心，稍微配置不当，就会出现问题，先看在整点时间设置时可能出现的错误，例如，设定每天2点执行一次任务，很多朋友可能这么写过：
* 2 * * * command 很明显，这个时间写法是错误的，当我们听到每天2点执行一次某任务时，很多人会把重点放在2点，而忽略了执行一次的需求。上面这个定时任务他会在2点开始执行，每分钟执行一次，总共执行60次。
正确的写法应该是这样的：
0 2 * * * command 这个才表示每天2点0分执行command对应的任务。
特殊符号%问题 %在crontab中是特殊符号，具体含义如下：
第一个%表示标准输入的开始，其余%表示换行符，看下面两个例子：
* * * * * cat &gt;&gt; /tmp/cat.txt 2&gt;&amp;1 % stdin out 查看/tmp/cat.txt的内容为：
stdin out 再看下面这个例子：
* * * * * cat &gt;&gt; /tmp/cat1.txt 2&gt;&amp;1 % stdin out 1 % stdin out 2 % stdin out 3 查看 /tmp/cat1.txt的内容如下：
stdin out 1 stdin out 2 stdin out 3 有输出内容可知，第一个%表示标准输入的开始，其余%表示换行符。
既然&quot;%&ldquo;是特殊字符,那么在crontab中使用时，就要特别注意，怎么使用这些特殊字符呢，很明显，使用转移字符即可，例如：
* * * * * cat &gt;&gt; /tmp/cat2.txt 2&gt;&amp;1 % Special character escape \%. 查看输出/tmp/cat2.txt 输出内容如下：
Special character escape %. 可以看到，执行成功了，并成功避开这个坑了。
关于crontab的输出重定向 在crontab执行的计划任务中，有些任务如果不做输出重定向，那么原本会输出到屏幕的信息，会以邮件的形式输出到某个文件中，例如，执行下面这个计划任务：
* * * * * /bin/date 这个计划任务是没有做输出重定向的，他的主要用途是输出时间，由于没有配置输出重定向，那么这个时间信息默认将以邮件的形式输出到/var/spool/mail/$USER（这个$USER对应的是系统用户，这里是root用户）文件中，大致内容如下：
From root@SparkWorker1.localdomain Fri Sep 21 12:58:02 2022 Return-Path: &lt;root@SparkWorker1.localdomain&gt; X-Original-To: root Delivered-To: root@SparkWorker1.localdomain Received: by SparkWorker1.localdomain (Postfix, from userid 0) id F2745192AE; Fri, 21 Sep 2022 12:58:01 +0800 (CST) From: &#34;(Cron Daemon)&#34; &lt;root@SparkWorker1.localdomain&gt; To: root@SparkWorker1.localdomain Subject: Cron &lt;root@SparkWorker1&gt; /bin/date Content-Type: text/plain; charset=UTF-8 Auto-Submitted: auto-generated Precedence: bulk X-Cron-Env: &lt;XDG_SESSION_ID=1820&gt; X-Cron-Env: &lt;XDG_RUNTIME_DIR=/run/user/0&gt; X-Cron-Env: &lt;LANG=zh_CN.UTF-8&gt; X-Cron-Env: &lt;SHELL=/bin/sh&gt; X-Cron-Env: &lt;HOME=/root&gt; X-Cron-Env: &lt;PATH=/usr/bin:/bin&gt; X-Cron-Env: &lt;LOGNAME=root&gt; X-Cron-Env: &lt;USER=root&gt; Message-Id: &lt;20220921045801.F2745192AE@SparkWorker1.localdomain&gt; Date: Fri, 21 Sep 2022 12:58:01 +0800 (CST) 2022年 09月 21日 星期五 12:58:01 CST 由此可见，输出内容还是很多的，如遇到任务有大量输出的话，会占用大量磁盘空间，显然，这个邮件输出最好关闭，怎么关闭呢，只需设置MAILTO环境变量为空即可，上面的计划任务，可做如下修改：
MAILTO=&#34;&#34; * * * * * /bin/date 这样，就不会发邮件信息到/var/spool/mail/$USER下了，但是问题并没有彻底解决，关闭mail功能后，输出内容将继续写入到/var/spool/clientmqueue中，长期下去，可能占满分区的inode资源，导致任务无法执行。
为了避免此类问题发生，建议任务都加上输出重定向，例如，可以在crontab文件中设置如下形式，忽略日志输出：
0 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1 其中，“/dev/null 2&gt;&amp;1”表示先将标准输出重定向到/dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了/dev/null，因此标准错误也会重定向到/dev/null，这样日志输出问题就解决了。
调试crontab问题的一般思路 要解决crontab相关异常问题，可按照如下思路进行调试：
（1）、通过/var/log/cron日志确认任务是否执行
（2）、如未执行则分析定时语句，是否是环境变量问题、特殊字符问题、时间配置问题、权限问题等。
（3）、确认crond服务开启，如果定时语句也正确，检查crond服务是否开启。
Systemd方式(centos7及以上)
[root@SparkWorker1 spool]# systemctl status crond.service SysVinit方式(centos7以下)
[root@SparkWorker1 spool]# service crond status （4）确认定时任务中命令是否执行成功
这个问题可通过输出获取错误信息进行调试，方法就是利用重定向获取输出，然后进行分析。举例如下：
* * * * * python /usr/local/dyserver/dypos.py &gt;&gt; /tmp/dypos.log 2&gt;&amp;1 通过加上“/tmp/dypos.log 2&gt;&amp;1”，就可以很快定位问题，因为这个dypos.py脚本在执行的时候会把错误信息都输出到dypos.log 中，接着查看dypos.log文件，问题一目了然：
[root@SparkWorker1 spool]# cat /tmp/dypos.log /bin/sh: python: 未找到命令 /bin/sh: python: 未找到命令 显示python命令没有找到，很明显的就可以确定是环境变量的问题。这种方式定位问题非常有效。
]]></content>
  </entry>
  
  <entry>
    <title>I2C总线接上拉电阻的原因</title>
    <url>/post/hardware/why-add-pull-up-resistor-for-i2c.html</url>
    <categories><category>Hardware</category>
    </categories>
    <tags>
      <tag>I2C</tag>
    </tags>
    <content type="html"><![CDATA[I2C为什么要接上拉电阻？因为它是开漏输出。
为什么是开漏输出？ I2C协议支持多个主设备与多个从设备在一条总线上，如果不用开漏输出，而用推挽输出，会出现主设备之间短路的情况。所以总线一般会使用开漏输出。
为什么要接上拉电阻？ 接上拉电阻是因为I2C通信需要输出高电平的能力。一般开漏输出无法输出高电平，如果在漏极接上拉电阻，则可以进行电平转换。
I2C由两条总线SDA和SCL组成。连接到总线的器件的输出级必须是漏极开路，都通过上拉电阻连接到电源，这样才能够实现“线与”功能。当总线空闲时，这两条线路都是高电平。
上拉电阻阻值怎么确定？ 一般IO端口的驱动能力在2mA～4mA量级。
考虑到功耗问题，阻值不能过小
如果上拉阻值过小，VDD灌入端口的电流将较大，功耗会很大，导致端口输出的低电平值增大(I2C协议规定，端口输出低电平的最高允许值为0.4V)。故通常上拉电阻应选取不低于1K的电阻（当VDD＝3V时，灌入电流不超过3mA）。
考虑到速度问题，阻值不能过大
它取决于上拉电阻和线上电容形成的RC延时，RC延时越大，波形越偏离方波趋向于正弦波，数据读写正确的概率就越低，所以上拉电阻不能过大。
I2C总线上的负载电容不能超过400pF。当I2C总线上器件逐渐增多时，总线负载电容也相应增加。当总的负载电容大于400pF时，就不能可靠的工作。这也是I2C的局限性。
建议上拉电阻可选用1.5K，2.2K，4.7K。
I2C总线基本操作 根据I2C总线规范，总线空闲时两根线都必须为高。假设主设备A需要启动I2C，他需要在SCL高电平时，将SDA由高电平转换为低电平作为启动信号。
主设备A在把SDA拉高后，它需要再检查一下SDA的电平。为什么? 因为线与，如果主设备A拉高SDA时，已经有其他主设备将SDA拉低了，由于 1 &amp; 0 = 0 那么主设备A在检查SDA电平时, 会发现不是高电平，而是低电平。说明其他主设备抢占总线的时间比它早，主设备A只能放弃占用总线。如果SDA是高电平，说明主设备A可以占用总线，然后主设备A将SDA拉低，开始通信。
因此，模拟I2C一定要将GPIO端口设置为开漏输出并加上拉电阻。
]]></content>
  </entry>
  
  <entry>
    <title>Linux编程之经典多级时间轮定时器</title>
    <url>/post/linux/linux-programming-multiple-time-wheel-timer.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>timer</tag>
    </tags>
    <content type="html"><![CDATA[mmap用于把文件映射到内存空间中，简单说mmap就是把一个文件的内容在内存里面做一个映像。
多级时间轮实现框架 上图是5个时间轮级联的效果图。中间的大轮是工作轮，只有在它上的任务才会被执行；其他轮上的任务时间到后迁移到下一级轮上，他们最终都会迁移到工作轮上而被调度执行。
多级时间轮的原理也容易理解：就拿时钟做说明，秒针转动一圈分针转动一格；分针转动一圈时针转动一格；同理时间轮也是如此：当低级轮转动一圈时，高一级轮转动一格，同时会将高一级轮上的任务重新分配到低级轮上。从而实现了多级轮级联的效果。
多级时间轮对象 多级时间轮应该至少包括以下内容：
 每一级时间轮对象 轮子上指针的位置 关于轮子上指针的位置有一个比较巧妙的办法：那就是位运算。比如定义一个无符号整型的数：  通过获取当前的系统时间便可以通过位操作转换为时间轮上的时间，通过与实际时间轮上的时间作比较，从而确定时间轮要前进调度的时间，进而操作对应时间轮槽位对应的任务。
为什么至少需要这两个成员呢？
 定义多级时间轮，首先需要明确的便是级联的层数，也就是说需要确定有几个时间轮。 轮子上指针位置，就是当前时间轮运行到的位置，它与真实时间的差便是后续时间轮需要调度执行，它们的差值是时间轮运作起来的驱动力。  多级时间轮对象的定义
//实现5级时间轮 范围为0~ (2^8 * 2^6 * 2^6 * 2^6 *2^6)=2^32 struct tvec_base { unsigned long current_index; pthread_t thincrejiffies; pthread_t threadID; struct tvec_root tv1; /*第一个轮*/ struct tvec tv2; /*第二个轮*/ struct tvec tv3; /*第三个轮*/ struct tvec tv4; /*第四个轮*/ struct tvec tv5; /*第五个轮*/ }; 时间轮对象 我们知道每一个轮子实际上都是一个哈希表，上面我们只是实例化了五个轮子的对象，但是五个轮子具体包含什么，有几个槽位等等没有明确(即struct tvec和struct tvec_root)。
#define TVN_BITS 6 #define TVR_BITS 8 #define TVN_SIZE (1&lt;&lt;TVN_BITS) #define TVR_SIZE (1&lt;&lt;TVR_BITS) struct tvec { struct list_head vec[TVN_SIZE];/*64个格子*/ }; struct tvec_root{ struct list_head vec[TVR_SIZE];/*256个格子*/ }; 此外，每一个时间轮都是哈希表，因此它的类型应该至少包含两个指针域来实现双向链表的功能。这里我们为了方便使用通用的struct list_head的双向链表结构。
定时任务对象 定时器的主要工作是为了在未来的特定时间完成某项任务，而这个任务经常包含以下内容：
 任务的处理逻辑(回调函数) 任务的参数 双向链表节点 到时时间  定时任务对象的定义
typedef void (*timeouthandle)(unsigned long ); struct timer_list{ struct list_head entry; //将时间连接成链表  unsigned long expires; //超时时间  void (*function)(unsigned long); //超时后的处理函数  unsigned long data; //处理函数的参数  struct tvec_base *base; //指向时间轮 }; 在时间轮上的效果图：
双向链表 在时间轮上我们采用双向链表的数据类型。采用双向链表的除了操作上比单链表复杂，多占一个指针域外没有其他不可接收的问题。而多占一个指针域在今天大内存的时代明显不是什么问题。至于双向链表操作的复杂性，我们可以通过使用通用的struct list结构来解决，因为双向链表有众多的标准操作函数，我们可以通过直接引用list.h头文件来使用他们提供的接口。
struct list可以说是一个万能的双向链表操作框架，我们只需要在自定义的结构中定义一个struct list对象即可使用它的标准操作接口。同时它还提供了一个类似container_of的接口，在应用层一般叫做list_entry，因此我们可以很方便的通过struct list成员找到自定义的结构体的起始地址。
关于应用层的log.h, 我将在下面的代码中附上该文件。如果需要内核层的实现，可以直接从linux源码中获取。
联结方式 多级时间轮效果图：
多级时间轮C语言实现 双向链表头文件: list.h 提到双向链表，很多的源码工程中都会实现一系列的统一的双向链表操作函数。它们为双向链表封装了统计的接口，使用者只需要在自定义的结构中添加一个struct list_head结构，然后调用它们提供的接口，便可以完成双向链表的所有操作。这些操作一般都在list.h的头文件中实现。Linux源码中也有实现（内核态的实现）。他们实现的方式基本完全一样，只是实现的接口数量和功能上稍有差别。可以说这个list.h文件是学习操作双向链表的不二选择，它几乎实现了所有的操作：增、删、改、查、遍历、替换、清空等等。这里我拼凑了一个源码中的log.h函数，终于凑够了多级时间轮中使用到的接口。
#if !defined(_BLKID_LIST_H) &amp;&amp; !defined(LIST_HEAD) #define _BLKID_LIST_H #ifdef __cplusplus extern &#34;C&#34; { #endif /* * Simple doubly linked list implementation. * * Some of the internal functions (&#34;__xxx&#34;) are useful when * manipulating whole lists rather than single entries, as * sometimes we already know the next/prev entries and we can * generate better code by using them directly rather than * using the generic single-entry routines. */ struct list_head { struct list_head *next, *prev; }; #define LIST_HEAD_INIT(name) { &amp;(name), &amp;(name) } #define LIST_HEAD(name) \ struct list_head name = LIST_HEAD_INIT(name) #define INIT_LIST_HEAD(ptr) do { \ (ptr)-&gt;next = (ptr); (ptr)-&gt;prev = (ptr); \ } while (0) static inline void __list_add(struct list_head *entry, struct list_head *prev, struct list_head *next) { next-&gt;prev = entry; entry-&gt;next = next; entry-&gt;prev = prev; prev-&gt;next = entry; } /** * Insert a new element after the given list head. The new element does not * need to be initialised as empty list. * The list changes from: * head → some element → ... * to * head → new element → older element → ... * * Example: * struct foo *newfoo = malloc(...); * list_add(&amp;newfoo-&gt;entry, &amp;bar-&gt;list_of_foos); * * @param entry The new element to prepend to the list. * @param head The existing list. */ static inline void list_add(struct list_head *entry, struct list_head *head) { __list_add(entry, head, head-&gt;next); } /** * Append a new element to the end of the list given with this list head. * * The list changes from: * head → some element → ... → lastelement * to * head → some element → ... → lastelement → new element * * Example: * struct foo *newfoo = malloc(...); * list_add_tail(&amp;newfoo-&gt;entry, &amp;bar-&gt;list_of_foos); * * @param entry The new element to prepend to the list. * @param head The existing list. */ static inline void list_add_tail(struct list_head *entry, struct list_head *head) { __list_add(entry, head-&gt;prev, head); } static inline void __list_del(struct list_head *prev, struct list_head *next) { next-&gt;prev = prev; prev-&gt;next = next; } /** * Remove the element from the list it is in. Using this function will reset * the pointers to/from this element so it is removed from the list. It does * NOT free the element itself or manipulate it otherwise. * * Using list_del on a pure list head (like in the example at the top of * this file) will NOT remove the first element from * the list but rather reset the list as empty list. * * Example: * list_del(&amp;foo-&gt;entry); * * @param entry The element to remove. */ static inline void list_del(struct list_head *entry) { __list_del(entry-&gt;prev, entry-&gt;next); } static inline void list_del_init(struct list_head *entry) { __list_del(entry-&gt;prev, entry-&gt;next); INIT_LIST_HEAD(entry); } static inline void list_move_tail(struct list_head *list, struct list_head *head) { __list_del(list-&gt;prev, list-&gt;next); list_add_tail(list, head); } /** * Check if the list is empty. * * Example: * list_empty(&amp;bar-&gt;list_of_foos); * * @return True if the list contains one or more elements or False otherwise. */ static inline int list_empty(struct list_head *head) { return head-&gt;next == head; } /** * list_replace - replace old entry by new one * @old : the element to be replaced * @new : the new element to insert * * If @old was empty, it will be overwritten. */ static inline void list_replace(struct list_head *old, struct list_head *new) { new-&gt;next = old-&gt;next; new-&gt;next-&gt;prev = new; new-&gt;prev = old-&gt;prev; new-&gt;prev-&gt;next = new; } /** * Retrieve the first list entry for the given list pointer. * * Example: * struct foo *first; * first = list_first_entry(&amp;bar-&gt;list_of_foos, struct foo, list_of_foos); * * @param ptr The list head * @param type Data type of the list element to retrieve * @param member Member name of the struct list_head field in the list element. * @return A pointer to the first list element. */ #define list_first_entry(ptr, type, member) \ list_entry((ptr)-&gt;next, type, member) static inline void list_replace_init(struct list_head *old, struct list_head *new) { list_replace(old, new); INIT_LIST_HEAD(old); } /** * list_entry - get the struct for this entry * @ptr: the &amp;struct list_head pointer. * @type: the type of the struct this is embedded in. * @member: the name of the list_struct within the struct. */ #define list_entry(ptr, type, member) \ ((type *)((char *)(ptr)-(unsigned long)(&amp;((type *)0)-&gt;member))) /** * list_for_each - iterate over elements in a list * @pos: the &amp;struct list_head to use as a loop counter. * @head: the head for your list. */ #define list_for_each(pos, head) \ for (pos = (head)-&gt;next; pos != (head); pos = pos-&gt;next) /** * list_for_each_safe - iterate over elements in a list, but don&#39;t dereference * pos after the body is done (in case it is freed) * @pos: the &amp;struct list_head to use as a loop counter. * @pnext: the &amp;struct list_head to use as a pointer to the next item. * @head: the head for your list (not included in iteration). */ #define list_for_each_safe(pos, pnext, head) \ for (pos = (head)-&gt;next, pnext = pos-&gt;next; pos != (head); \ pos = pnext, pnext = pos-&gt;next) #ifdef __cplusplus } #endif #endif /* _BLKID_LIST_H */这里面一般会用到一个重要实现：container_of, 它的原理这里不叙述
调试信息头文件: log.h 这个头文件实际上不是必须的，我只是用它来添加调试信息(代码中的errlog(), log()都是log.h中的宏函数)。它的效果是给打印的信息加上颜色，效果如下：
log.h的代码如下：
#ifndef _LOG_h_ #define _LOG_h_ #include &lt;stdio.h&gt;#define COL(x) &#34;\033[;&#34; #x &#34;m&#34; #define RED COL(31) #define GREEN COL(32) #define YELLOW COL(33) #define BLUE COL(34) #define MAGENTA COL(35) #define CYAN COL(36) #define WHITE COL(0) #define GRAY &#34;\033[0m&#34; #define errlog(fmt, arg...) do{ \ printf(RED&#34;[#ERROR: Toeny Sun:&#34;GRAY YELLOW&#34; %s:%d]:&#34;GRAY WHITE fmt GRAY, __func__, __LINE__, ##arg);\ }while(0) #define log(fmt, arg...) do{ \ printf(WHITE&#34;[#DEBUG: Toeny Sun: &#34;GRAY YELLOW&#34;%s:%d]:&#34;GRAY WHITE fmt GRAY, __func__, __LINE__, ##arg);\ }while(0) #endif 时间轮代码: timewheel.c /* *毫秒定时器 采用多级时间轮方式 借鉴linux内核中的实现 *支持的范围为1 ~ 2^32 毫秒(大约有49天) *若设置的定时器超过最大值 则按最大值设置定时器 **/ #include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;pthread.h&gt;#include &lt;sys/time.h&gt;#include &#34;list.h&#34;#include &#34;log.h&#34; #define TVN_BITS 6 #define TVR_BITS 8 #define TVN_SIZE (1&lt;&lt;TVN_BITS) #define TVR_SIZE (1&lt;&lt;TVR_BITS)  #define TVN_MASK (TVN_SIZE - 1) #define TVR_MASK (TVR_SIZE - 1)  #define SEC_VALUE 0 #define USEC_VALUE 2000  struct tvec_base; #define INDEX(N) ((ba-&gt;current_index &gt;&gt; (TVR_BITS + (N) * TVN_BITS)) &amp; TVN_MASK)  typedef void (*timeouthandle)(unsigned long ); struct timer_list{ struct list_head entry; //将时间连接成链表  unsigned long expires; //超时时间  void (*function)(unsigned long); //超时后的处理函数  unsigned long data; //处理函数的参数  struct tvec_base *base; //指向时间轮 }; struct tvec { struct list_head vec[TVN_SIZE]; }; struct tvec_root{ struct list_head vec[TVR_SIZE]; }; //实现5级时间轮 范围为0~ (2^8 * 2^6 * 2^6 * 2^6 *2^6)=2^32 struct tvec_base { unsigned long current_index; pthread_t thincrejiffies; pthread_t threadID; struct tvec_root tv1; /*第一个轮*/ struct tvec tv2; /*第二个轮*/ struct tvec tv3; /*第三个轮*/ struct tvec tv4; /*第四个轮*/ struct tvec tv5; /*第五个轮*/ }; static void internal_add_timer(struct tvec_base *base, struct timer_list *timer) { struct list_head *vec; unsigned long expires = timer-&gt;expires; unsigned long idx = expires - base-&gt;current_index; #if 1  if( (signed long)idx &lt; 0 ) /*这里是没有办法区分出是过时还是超长定时的吧?*/ { vec = base-&gt;tv1.vec + (base-&gt;current_index &amp; TVR_MASK);/*放到第一个轮的当前槽*/ } else if ( idx &lt; TVR_SIZE ) /*第一个轮*/ { int i = expires &amp; TVR_MASK; vec = base-&gt;tv1.vec + i; } else if( idx &lt; 1 &lt;&lt; (TVR_BITS + TVN_BITS) )/*第二个轮*/ { int i = (expires &gt;&gt; TVR_BITS) &amp; TVN_MASK; vec = base-&gt;tv2.vec + i; } else if( idx &lt; 1 &lt;&lt; (TVR_BITS + 2 * TVN_BITS) )/*第三个轮*/ { int i = (expires &gt;&gt; (TVR_BITS + TVN_BITS)) &amp; TVN_MASK; vec = base-&gt;tv3.vec + i; } else if( idx &lt; 1 &lt;&lt; (TVR_BITS + 3 * TVN_BITS) )/*第四个轮*/ { int i = (expires &gt;&gt; (TVR_BITS + 2 * TVN_BITS)) &amp; TVN_MASK; vec = base-&gt;tv4.vec + i; } else /*第五个轮*/ { int i; if (idx &gt; 0xffffffffUL) { idx = 0xffffffffUL; expires = idx + base-&gt;current_index; } i = (expires &gt;&gt; (TVR_BITS + 3 * TVN_BITS)) &amp; TVN_MASK; vec = base-&gt;tv5.vec + i; } #else  /*上面可以优化吧*/; #endif  list_add_tail(&amp;timer-&gt;entry, vec); } static inline void detach_timer(struct timer_list *timer) { struct list_head *entry = &amp;timer-&gt;entry; __list_del(entry-&gt;prev, entry-&gt;next); entry-&gt;next = NULL; entry-&gt;prev = NULL; } static int __mod_timer(struct timer_list *timer, unsigned long expires) { if(NULL != timer-&gt;entry.next) detach_timer(timer); internal_add_timer(timer-&gt;base, timer); return 0; } //修改定时器的超时时间外部接口 int mod_timer(void *ptimer, unsigned long expires) { struct timer_list *timer = (struct timer_list *)ptimer; struct tvec_base *base; base = timer-&gt;base; if(NULL == base) return -1; expires = expires + base-&gt;current_index; if(timer-&gt;entry.next != NULL &amp;&amp; timer-&gt;expires == expires) return 0; if( NULL == timer-&gt;function ) { errlog(&#34;timer&#39;s timeout function is null\n&#34;); return -1; } timer-&gt;expires = expires; return __mod_timer(timer,expires); } //添加一个定时器 static void __ti_add_timer(struct timer_list *timer) { if( NULL != timer-&gt;entry.next ) { errlog(&#34;timer is already exist\n&#34;); return; } mod_timer(timer, timer-&gt;expires); } /*添加一个定时器 外部接口 *返回定时器 */ void* ti_add_timer(void *ptimewheel, unsigned long expires,timeouthandle phandle, unsigned long arg) { struct timer_list *ptimer; ptimer = (struct timer_list *)malloc( sizeof(struct timer_list) ); if(NULL == ptimer) return NULL; bzero( ptimer,sizeof(struct timer_list) ); ptimer-&gt;entry.next = NULL; ptimer-&gt;base = (struct tvec_base *)ptimewheel; ptimer-&gt;expires = expires; ptimer-&gt;function = phandle; ptimer-&gt;data = arg; __ti_add_timer(ptimer); return ptimer; } /* *删除一个定时器 外部接口 * * */ void ti_del_timer(void *p) { struct timer_list *ptimer =(struct timer_list*)p; if(NULL == ptimer) return; if(NULL != ptimer-&gt;entry.next) detach_timer(ptimer); free(ptimer); } /*时间轮级联*/ static int cascade(struct tvec_base *base, struct tvec *tv, int index) { struct list_head *pos,*tmp; struct timer_list *timer; struct list_head tv_list; /*将tv[index]槽位上的所有任务转移给tv_list,然后清空tv[index]*/ list_replace_init(tv-&gt;vec + index, &amp;tv_list);/*用tv_list替换tv-&gt;vec + index*/ list_for_each_safe(pos, tmp, &amp;tv_list)/*遍历tv_list双向链表，将任务重新添加到时间轮*/ { timer = list_entry(pos,struct timer_list,entry);/*struct timer_list中成员entry的地址是pos, 获取struct timer_list的首地址*/ internal_add_timer(base, timer); } return index; } static void *deal_function_timeout(void *base) { struct timer_list *timer; int ret; struct timeval tv; struct tvec_base *ba = (struct tvec_base *)base; for(;;) { gettimeofday(&amp;tv, NULL); while( ba-&gt;current_index &lt;= (tv.tv_sec*1000 + tv.tv_usec/1000) )/*单位：ms*/ { struct list_head work_list; int index = ba-&gt;current_index &amp; TVR_MASK;/*获取第一个轮上的指针位置*/ struct list_head *head = &amp;work_list; /*指针指向0槽时，级联轮需要更新任务列表*/ if(!index &amp;&amp; (!cascade(ba, &amp;ba-&gt;tv2, INDEX(0))) &amp;&amp;( !cascade(ba, &amp;ba-&gt;tv3, INDEX(1))) &amp;&amp; (!cascade(ba, &amp;ba-&gt;tv4, INDEX(2))) ) cascade(ba, &amp;ba-&gt;tv5, INDEX(3)); ba-&gt;current_index ++; list_replace_init(ba-&gt;tv1.vec + index, &amp;work_list); while(!list_empty(head)) { void (*fn)(unsigned long); unsigned long data; timer = list_first_entry(head, struct timer_list, entry); fn = timer-&gt;function; data = timer-&gt;data; detach_timer(timer); (*fn)(data); } } } } static void init_tvr_list(struct tvec_root * tvr) { int i; for( i = 0; i&lt;TVR_SIZE; i++ ) INIT_LIST_HEAD(&amp;tvr-&gt;vec[i]); } static void init_tvn_list(struct tvec * tvn) { int i; for( i = 0; i&lt;TVN_SIZE; i++ ) INIT_LIST_HEAD(&amp;tvn-&gt;vec[i]); } //创建时间轮 外部接口 void *ti_timewheel_create(void ) { struct tvec_base *base; int ret = 0; struct timeval tv; base = (struct tvec_base *) malloc( sizeof(struct tvec_base) ); if( NULL==base ) return NULL; bzero( base,sizeof(struct tvec_base) ); init_tvr_list(&amp;base-&gt;tv1); init_tvn_list(&amp;base-&gt;tv2); init_tvn_list(&amp;base-&gt;tv3); init_tvn_list(&amp;base-&gt;tv4); init_tvn_list(&amp;base-&gt;tv5); gettimeofday(&amp;tv, NULL); base-&gt;current_index = tv.tv_sec*1000 + tv.tv_usec/1000;/*当前时间毫秒数*/ if( 0 != pthread_create(&amp;base-&gt;threadID,NULL,deal_function_timeout,base) ) { free(base); return NULL; } return base; } static void ti_release_tvr(struct tvec_root *pvr) { int i; struct list_head *pos,*tmp; struct timer_list *pen; for(i = 0; i &lt; TVR_SIZE; i++) { list_for_each_safe(pos,tmp,&amp;pvr-&gt;vec[i]) { pen = list_entry(pos,struct timer_list, entry); list_del(pos); free(pen); } } } static void ti_release_tvn(struct tvec *pvn) { int i; struct list_head *pos,*tmp; struct timer_list *pen; for(i = 0; i &lt; TVN_SIZE; i++) { list_for_each_safe(pos,tmp,&amp;pvn-&gt;vec[i]) { pen = list_entry(pos,struct timer_list, entry); list_del(pos); free(pen); } } } /* *释放时间轮 外部接口 * */ void ti_timewheel_release(void * pwheel) { struct tvec_base *base = (struct tvec_base *)pwheel; if(NULL == base) return; ti_release_tvr(&amp;base-&gt;tv1); ti_release_tvn(&amp;base-&gt;tv2); ti_release_tvn(&amp;base-&gt;tv3); ti_release_tvn(&amp;base-&gt;tv4); ti_release_tvn(&amp;base-&gt;tv5); free(pwheel); } /************demo****************/ struct request_para{ void *timer; int val; }; void mytimer(unsigned long arg) { struct request_para *para = (struct request_para *)arg; log(&#34;%d\n&#34;,para-&gt;val); mod_timer(para-&gt;timer,3000); //进行再次启动定时器  sleep(10);/*定时器依然被阻塞*/ //定时器资源的释放是在这里完成的  //ti_del_timer(para-&gt;timer); } int main(int argc,char *argv[]) { void *pwheel = NULL; void *timer = NULL; struct request_para *para; para = (struct request_para *)malloc( sizeof(struct request_para) ); if(NULL == para) return 0; bzero(para,sizeof(struct request_para)); //创建一个时间轮  pwheel = ti_timewheel_create(); if(NULL == pwheel) return -1; //添加一个定时器  para-&gt;val = 100; para-&gt;timer = ti_add_timer(pwheel, 3000, &amp;mytimer, (unsigned long)para); while(1) { sleep(2); } //释放时间轮  ti_timewheel_release(pwheel); return 0; } 编译运行 peng@ubuntu:/mnt/hgfs/timer/4. timerwheel/2. 多级时间轮$ ls a.out list.h log.h mutiTimeWheel.c toney@ubantu:/mnt/hgfs/timer录/4. timerwheel/2. 多级时间轮$ gcc mutiTimeWheel.c -lpthread toney@ubantu:/mnt/hgfs/timer/4. timerwheel/2. 多级时间轮$ ./a.out [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 [#DEBUG: Toeny Sun: mytimer:370]:100 从结果可以看出：如果添加的定时任务是比较耗时的操作，那么后续的任务也会被阻塞，可能一直到超时，甚至一直阻塞下去，这个取决于当前任务是否耗时。
这个理论上是绝不能接受的：一个任务不应该也不能去影响其他的任务吧。但是目前没有对此问题进行改进和完善，以后有机会再继续完善吧。
]]></content>
  </entry>
  
  <entry>
    <title>Linux mmap内存映射详解</title>
    <url>/post/linux/linux-mmap-explanation.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>device driver</tag>
    </tags>
    <content type="html"><![CDATA[mmap用于把文件映射到内存空间中，简单说mmap就是把一个文件的内容在内存里面做一个映像。
mmap基础概念 mmap是一种内存映射的方法，这一功能可以用在文件的处理上，即将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。在编程时可以使某个磁盘文件的内容看起来像是内存中的一个数组。如果文件由记录组成，而这些记录又能够用结构体来描述的话，可以通过访问结构体来更新文件的内容。
实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写到页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用read,write等系统调用函数。内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。如图所示：
进程的虚拟地址空间，由多个虚拟内存区域构成。虚拟内存区域是进程的虚拟地址空间中的一个同质区间，即具有同样特性的连续地址范围。上图中所示的text数据段（代码段）、初始数据段、BSS数据段、堆、栈和内存映射，都是一个独立的虚拟内存区域。而为内存映射服务的地址空间处在堆栈之间的空余部分。
内核为系统中的每个进程维护一个单独的任务结构（task_struct）。任务结构中的元素包含或者指向内核运行该进程所需的所有信息(PID、指向用户栈的指针、可执行目标文件的名字、程序计数器等)。Linux内核使用vm_area_struct结构来表示一个独立的虚拟内存区域，由于每个不同质的虚拟内存区域功能和内部机制都不同，因此一个进程使用多个vm_area_struct结构来分别表示不同类型的虚拟内存区域。各个vm_area_struct结构使用链表或者树形结构链接，方便进程快速访问，如下图所示：
vm_area_struct结构中包含区域起始和终止地址以及其他相关信息，同时也包含一个vm_ops指针，其内部可引出所有针对这个区域可以使用的系统调用函数。这样，进程对某一虚拟内存区域的任何操作需要用要的信息，都可以从vm_area_struct中获得。mmap函数就是要创建一个新的vm_area_struct结构，并将其与文件的物理磁盘地址相连。
mm_struct：描述了虚拟内存的当前状态。pgd指向一级页表的基址（当内核运行这个进程时， pgd会被存放在CR3控制寄存器，也就是页表基址寄存器中），mmap指向一个vm_area_structs 的链表，其中每个vm_area_structs都描述了当前虚拟地址空间的一个区域。 vm_starts 指向这个区域的起始处。 vm_end 指向这个区域的结束处。 vm_prot 描述这个区域内包含的所有页的读写许可权限。 vm_flags 描述这个区域内的页面是与其他进程共享的，还是这个进程私有的以及一些其他信息。 vm_next 指向链表的下一个区域结构。  mmap内存映射原理 mmap内存映射的实现过程，总的来说可以分为三个阶段：
(一)启动映射过程，并在虚拟地址空间中为映射创建虚拟映射区域    进程在用户空间调用库函数mmap，原型：void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset);
  在当前进程的虚拟地址空间中，寻找一段空闲的满足要求的连续的虚拟地址。
  为此虚拟区分配一个vm_area_struct结构，接着对这个结构的各个域进行了初始化。
  将新建的虚拟区结构（vm_area_struct）插入进程的虚拟地址区域链表或树中。
  (二)调用内核空间的系统调用函数mmap（不同于用户空间函数），实现文件物理地址和进程虚拟地址的一一映射关系   为映射分配了新的虚拟地址区域后，通过待映射的文件指针，在文件描述符表中找到对应的文件描述符，通过文件描述符，链接到内核“已打开文件集”中该文件的文件结构体（struct file），每个文件结构体维护者和这个已打开文件相关的各项信息。
  通过该文件的文件结构体，链接到file_operations模块，调用内核函数mmap，其原型为：int mmap(struct file *filp, struct vm_area_struct *vma)，不同于用户空间库函数。
  内核mmap函数通过虚拟文件系统inode模块定位到文件磁盘物理地址。
  通过remap_pfn_range函数建立页表，即实现了文件地址和虚拟地址区域的映射关系。此时，这片虚拟地址并没有任何数据关联到主存中。
  (三)进程发起对这片映射空间的访问，引发缺页异常，实现文件内容到物理内存（主存）的拷贝  注：前两个阶段仅在于创建虚拟区间并完成地址映射，但是并没有将任何文件数据的拷贝至主存。真正的文件读取是当进程发起读写操作时。
 进程的读或写操作访问虚拟地址空间这一段映射地址，通过查询页表，发现这一段地址并不在物理页面上。因为目前只建立了地址映射，真正的硬盘数据还没有拷贝到内存中，因此引发缺页异常。
  缺页异常进行一系列判断，确定无非法操作后，内核发起请求调页过程。
  调页过程先在交换缓存空间（swap cache）中寻找需要访问的内存页，如果没有则调用nopage函数把所缺的页从磁盘装入到主存中。
  之后进程即可对这片主存进行读或者写的操作，如果写操作改变了其内容，一定时间后系统会自动回写脏页面到对应磁盘地址，也即完成了写入到文件的过程。
  注意：修改过的脏页面并不会立即更新回文件中，而是有一段时间的延迟，可以调用msync()来强制同步, 这样所写的内容就能立即保存到文件里了。
mmap 示例代码 mmap (内存映射)函数的作用是建立一段可以被两个或更多个程序读写的内存。一个程序对它所做出的修改可以被其他程序看见。这要通过使用带有特殊权限集的虚拟内存段来实现。对这类虚拟内存段的读写会使操作系统去读写磁盘文件中与之对应的部分。mmap 函数创建一个指向一段内存区域的指针，该内存区域与可以通过一个打开的文件描述符访问的文件的内容相关联。mmap 函数原型如下：
#include &lt;sys/mman.h&gt;void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset); 可以通过传递 off 参数来改变共享内存段访问的文件中数据的起始偏移值。打开的文件描述符由 fildes 参数给出。可以访问的数据量(即内存段的长度)由 len 参数设置。
可以通过 addr 参数来请求使用某个特定的内存地址。如果它的取值是零，结果指针就将自动分配。这是推荐的做法，否则会降低程序的可移植性，因为不同系统上的可用地址范围是不一样的。
prot 参数用于设置内存段的访问权限。它是下列常数值的按位或的结果：
PROT_READ 内存段可读。 PROT_WRITE 内存段可写。 PROT_EXEC 内存段可执行。 PROT_NONE 内存段不能被访问。 flags 参数控制程序对该内存段的改变所造成的影响：
msync 函数的作用是：把在该内存段的某个部分或整段中的修改写回到被映射的文件中(或者从被映射文件里读出)。
#include &lt;sys/mman.h&gt;int msync(void *addr, size_t len, int flags); 内存段需要修改的部分由作为参数传递过来的起始地址 addr 和长度 len 确定。flags 参数控制着执行修改的具体方式，可以使用的选项如下：
MS_ASYNC 采用异步写方式 MS_SYNC 采用同步写方式 MS_INVALIDATE 从文件中读回数据 munmap 函数的作用是释放内存段：
#include &lt;sys/mman.h&gt;int munmap(void *addr, size_t length); 示例代码：
(1) 定义一个 RECORD 数据结构，然后创建出 NRECORDS 每个记录，每个记录中保存着它们各自的编号。然后把这些记录都追加到文件 records.dat 里去。
(2) 接着，把第 43 记录中的整数值由 43 修改为 143，并把它写入第 43 条记录中的字符串。
(3) 把这些记录映射到内存中，然后访问第 43 条记录，把它的整数值修改为 243 (同时更新该记录中的字符串)，使用的还是内存映射的方法。
可以将上述 (2) (3) 分别编写程序验证结果。
#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/mman.h&gt;#include &lt;fcntl.h&gt;#include &lt;stdlib.h&gt;typedef struct{ int integer; char string[24]; }RECORD; #define NRECORDS (100) int main() { RECORD record, *mapped; int i, f; FILE *fp; fp = fopen(&#34;records.dat&#34;, &#34;w+&#34;); for( i = 0; i &lt; NRECORDS; i++) { record.integer = i; sprintf(record.string, &#34;[RECORD-%d]&#34;, i); fwrite(&amp;record, sizeof(record), 1, fp); } fclose(fp); fp = fopen(&#34;records.dat&#34;, &#34;r+&#34;); fseek(fp, 43 * sizeof(record), SEEK_SET); fread(&amp;record, sizeof(record), 1, fp); record.integer = 143; sprintf(record.string, &#34;[RECORD-%d]&#34;, record.integer); fseek(fp, 43 * sizeof(record), SEEK_SET); fwrite(&amp;record, sizeof(record), 1, fp); fclose(fp); f = open(&#34;records.dat&#34;, O_RDWR); mapped = (RECORD*)mmap(0, NRECORDS * sizeof(record), PROT_READ | PROT_WRITE, MAP_SHARED, f, 0); printf(&#34;f:[%d]\n&#34;, f); //open是系统调用，返回文件描述符。fopen是库函数，返回指针。 	mapped[43].integer = 243; sprintf(mapped[43].string, &#34;[RECORD-%d]&#34;, mapped[43].integer); msync((void *) mapped, NRECORDS * sizeof(record), MS_ASYNC); munmap((void *)mapped, NRECORDS * sizeof(record)); close(f); return 0;	} mmap 和常规文件操作的区别 使用系统调用，函数的调用过程：
  进程发起读文件请求。
  内核通过查找进程文件描述符表，定位到内核已打开文件集上的文件信息，从而找到此文件的inode。
  inode在address_space上查找要请求的文件页是否已经缓存在页缓存中。如果存在，则直接返回这片文件页的内容。
  如果不存在，则通过inode定位到文件磁盘地址，将数据从磁盘复制到页缓存。之后再次发起读页面过程，进而将页缓存中的数据发给用户进程。
  总结来说，常规文件操作为了提高读写效率和保护磁盘，使用了页缓存机制。这样造成读文件时需要先将文件页从磁盘拷贝到页缓存中，由于页缓存处在内核空间，不能被用户进程直接寻址，所以还需要将页缓存中数据页再次拷贝到内存对应的用户空间中。这样，通过了两次数据拷贝过程，才能完成进程对文件内容的获取任务。写操作也是一样，待写入的buffer在内核空间不能直接访问，必须要先拷贝至内核空间对应的主存，再写回磁盘中（延迟写回），也是需要两次数据拷贝。
而使用mmap操作文件中，创建新的虚拟内存区域和建立文件磁盘地址和虚拟内存区域映射这两步，没有任何文件拷贝操作。而之后访问数据时发现内存中并无数据而发起的缺页异常过程，可以通过已经建立好的映射关系，只使用一次数据拷贝，就从磁盘中将数据传入内存的用户空间中，供进程使用。
总而言之，常规文件操作需要从磁盘到页缓存再到用户主存的两次数据拷贝。而mmap操控文件，只需要从磁盘到用户主存的一次数据拷贝过程。说白了，mmap的关键点是实现了用户空间和内核空间的数据直接交互而省去了空间不同、数据不通的繁琐过程。因此mmap效率更高。
由上文讨论可知，mmap优点共有一下几点：
  对文件的读取操作跨过了页缓存，减少了数据的拷贝次数，用内存读写取代I/O读写，提高了文件读取效率。
  实现了用户空间和内核空间的高效交互方式。两空间的各自修改操作可以直接反映在映射的区域内，从而被对方空间及时捕捉。
  提供进程间共享内存及相互通信的方式。不管是父子进程还是无亲缘关系的进程，都可以将自身用户空间映射到同一个文件或匿名映射到同一片区域。从而通过各自对映射区域的改动，达到进程间通信和进程间共享的目的。
  同时，如果进程A和进程B都映射了区域C，当A第一次读取C时通过缺页从磁盘复制文件页到内存中；但当B再读C的相同页面时，虽然也会产生缺页异常，但是不再需要从磁盘中复制文件过来，而可直接使用已经保存在内存中的文件数据。
可用于实现高效的大规模数据传输。内存空间不足，是制约大数据操作的一个方面，解决方案往往是借助硬盘空间协助操作，补充内存的不足。但是进一步会造成大量的文件I/O操作，极大影响效率。这个问题可以通过mmap映射很好的解决。换句话说，但凡是需要用磁盘空间代替内存的时候，mmap都可以发挥其功效。  mmap 使用的细节   使用mmap需要注意的一个关键点是，mmap映射区域大小必须是物理页大小(page_size)的整倍数（32位系统中通常是4k字节）。原因是，内存的最小粒度是页，而进程虚拟地址空间和内存的映射也是以页为单位。为了匹配内存的操作，mmap从磁盘到虚拟地址空间的映射也必须是页。
  内核可以跟踪被内存映射的底层对象（文件）的大小，进程可以合法的访问在当前文件大小以内又在内存映射区以内的那些字节。也就是说，如果文件的大小一直在扩张，只要在映射区域范围内的数据，进程都可以合法得到，这和映射建立时文件的大小无关。
  映射建立之后，即使文件关闭，映射依然存在。因为映射的是磁盘的地址，不是文件本身，和文件句柄无关。同时可用于进程间通信的有效地址空间不完全受限于被映射文件的大小，因为是按页映射。
 ]]></content>
  </entry>
  
  <entry>
    <title>几道简单的Linux驱动相关面试题</title>
    <url>/post/linux/linux-device-driver-questions-and-answers.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>linux</tag>
      <tag>device driver</tag>
    </tags>
    <content type="html"><![CDATA[今天给大家分享几道Linux设备驱动相关的面试题，希望能对需要的网友一些帮助！
Linux基础 任意3种网络操作的Linux命令,并说明他们的含义 ifconfig 命令 ifconfig 用于查看和配置 Linux 系统的网络接口。 查看所有网络接口及其状态：ifconfig -a 。 使用 up 和 down 命令启动或停止某个接口：ifconfig eth0 up 和 ifconfig eth0 down 。
iptables 命令 iptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。
netstat 命令 Linux netstat命令用于显示网络状态。
利用netstat指令可让你得知整个Linux系统的网络情况。
ping 命令 Linux ping命令用于检测主机。
执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。
telnet 命令 Linux telnet命令用于远端登入。
执行telnet指令开启终端机阶段作业，并登入远端主机。
Linux支持的文件类型  普通文件类型 - 目录文件类型 d 块设备文件类型 b 字符设备类型 c 套接字文件类型 s FIFO管道文件类型 p 链接文件类型 l  Linux系统编程 嵌入式操作系统进程间有哪些同步通信服务？ Linux进程间通信方式主要有  信号(signal) 信号量 管道(pipe)、流管道(s_pipe)、有名管道(FIFO)。 消息队列 共享内存 套接字（本地的还有域套接字）  ARM 请问ARM支持哪几种异常类型？ 异常源分类
要进入异常模式，一定要有异常源，ARM规定有7种异常源：
   异常源 描述     Reset 上电时执行   Undef 当流水线中的某个非法指令到达执行状态时执行   SWI 当一个软中断指令被执行完的时候执行   Prefetch 当一个指令被从内存中预取时，由于某种原因而失败，如果它能到达执行状态这个异常才会产生   Data 如果一个预取指令试图存取一个非法的内存单元，这时异常产生   IRQ 通常的中断   FIQ 快速中断    请简述什么是中断？中断发生后，CPU做了哪些操作 中断：是指CPU在执行程序的过程中，出现了某些突发事件时CPU必须暂停执行当前的程序，转去处理突发事件，处理完毕后CPU又返回源程序被中断的位置并继续执行。
中断发生后，ARM核的操作步骤可以总结为4大步3小步。
4大步3小步  保存执行状态：将CPSR复制到发生的异常模式下SPSR中； 模式切换：   CPSR模式位强制设置为与异常类型相对应的值， 处理器进入到ARM执行模式， 禁止所有IRQ中断，当进入FIQ快速中断模式时禁止FIQ中断；   保存返回地址：将下一条指令的地址（被打断程序）保存在LR(异常模式下LR_excep)中。 跳入异常向量表：强制设置PC的值为相应异常向量地址，跳转到异常处理程序中。  什么是GPIO？ general purpose input/output GPIO是相对于芯片本身而言的，如某个管脚是芯片的GPIO脚，则该脚可作为输入或输出高或低电平使用，当然某个脚具有复用的功能，即可做GPIO也可做其他用途。
也就是说你可以把这些引脚拿来用作任何一般用途的输入输出，例如用一根引脚连到led的一极来控制它的亮灭，也可以用一根（一些）引脚连到一个传感器上以获得该传感器的状态，这给cpu提供了一个方便的控制周边设备的途经。如果没有足够多的gpio管脚，在控制一些外围设备时就会力有不逮，这时可采取的方案是使用CPLD来帮助管理。
IIC引脚名称及功能？  SDA 数据线，用于传输数据 SCL 时钟线，用于同步数据  IIC的S、P信号如何发出？ 每次通信都必须由主设备发起，当主设备决定开始通讯时，需要发送开始（S）信号，需要执行以下动作；
 空闲时SCL默认是高电平； 将SDA线从高压电平切换到低压电平； 然后将SCL从高电平切换到低电平。 在主设备发送开始条件信号之后，所有从机即使处于睡眠模式也将变为活动状态，并等待接收地址位。 当双方决定结束通讯时，需要发送停止（P）信号，需要执行以下动作； 先将SDA、SCL设置为低电平； 然后将SCL从低电平切换到高电平； 将SDA从低电平切换到高电平。 在停止条件信号之后，I2C总线即处于空闲状态。  SPI引脚名称及功能？ 串行时钟线（SCK）、 主机输入/从机输出数据线MISO、 主机输出/从机输入数据线MOSI 从机选择线SS
(有的SPI接口芯片带有中断信号线INT或INT、有的SPI接口芯片没有主机输出/从机输入数据线MOSI)
驱动  查看驱动模块中打印信息应该使用什么命令？如何查看内核中已有的字符设备的信息？如何查看正在使用的有哪些中断号？
 查看驱动模块中打印信息的命令： dmesg 查看加载模块信息可以用 lsmod 已经分配的字符设备块设备号信息可以查看下面文件 cat /proc/devices 内核会为每一个驱动模块建立一个文件夹，如下： ls /sys/module/ 显示当前使用的中断号 cat /proc/interrupts  如何手动创建字符设备？并简述主设备号和次设备号的用途。
 创建字符设备命令如下:
mknod chartest c 4 64， mknod : 创建设备节点 chartest ：设备节点名字 c ： 字符设备， 4 ： 主设备号 64： 次设备号 主设备号：主设备号标识设备对应的驱动程序。虽然现代的linux内核允许多个驱动程序共享主设备号，但我们看待的大多数设备仍然按照“一个主设备对应一个驱动程序”的原则组织。
次设备号：次设备号由内核使用，用于正确确定设备文件所指的设备。依赖于驱动程序的编写方式，我们可以通过次设备号获得一个指向内核设备的直接指针，也可将此设备号当作设备本地数组的索引。
比如：
硬件平台可能又4个串口，他们驱动非常类似，区别仅仅是个字对应的SFR基地址不同， 那么我们可以让着几个串口共用同一个串口设备驱动 通过次设备号来区别具体是哪一个串口  内核中使用共享资源时，为了使之满足互斥条件，通常有哪些方法？
 原子操作，自旋锁，信号量，互斥锁
 Linux内核包括那几个子系统？
 Linux内核主要由进程调度（SCHED）、内存管理（MM）、虚拟文件系统（VFS）、网络接口（NET）和进程间通信（IPC）5个子系统组成
]]></content>
  </entry>
  
  <entry>
    <title>10个Python脚本来自动化你的日常任务</title>
    <url>/post/python/ten-python-script-to-automatically-execute-your-daily-task.html</url>
    <categories><category>Python</category>
    </categories>
    <tags>
      <tag>python</tag>
    </tags>
    <content type="html"><![CDATA[ 在这个自动化时代，我们有很多重复无聊的工作要做。 想想这些你不再需要一次又一次地做的无聊的事情，让它自动化，让你的生活更轻松。 那么在本文中，我将向您介绍 10 个 Python 自动化脚本，以使你的工作更加自动化，生活更加轻松。 因此，没有更多的重复任务将这篇文章放在您的列表中，让我们开始吧。
 解析和提取 HTML  此自动化脚本将帮助你从网页 URL 中提取 HTML，然后还为你提供可用于解析 HTML 以获取数据的功能。这个很棒的脚本对于网络爬虫和那些想要解析 HTML 以获取重要数据的人来说是一种很好的享受。
 # Parse and Extract HTML # pip install gazpacho import gazpacho # Extract HTML from URL url = &#39;https://www.example.com/&#39; html = gazpacho.get(url) print(html) # Extract HTML with Headers headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;} html = gazpacho.get(url, headers=headers) print(html) # Parse HTML parse = gazpacho.Soup(html) # Find single tags tag1 = parse.find(&#39;h1&#39;) tag2 = parse.find(&#39;span&#39;) # Find multiple tags tags1 = parse.find_all(&#39;p&#39;) tags2 = parse.find_all(&#39;a&#39;) # Find tags by class tag = parse.find(&#39;.class&#39;) # Find tags by Attribute tag = parse.find(&#34;div&#34;, attrs={&#34;class&#34;: &#34;test&#34;}) # Extract text from tags text = parse.find(&#39;h1&#39;).text text = parse.find_all(&#39;p&#39;)[0].text 二维码扫描仪  拥有大量二维码图像或只想扫描二维码图像，那么此自动化脚本将帮助你。该脚本使用 Qrtools 模块，使你能够以编程方式扫描 QR 图像。
 # Qrcode Scanner # pip install qrtools from qrtools import Qr def Scan_Qr(qr_img): qr = Qr() qr.decode(qr_img) print(qr.data) return qr.data print(&#34;Your Qr Code is: &#34;, Scan_Qr(&#34;qr.png&#34;)) 截图  现在，你可以使用下面这个很棒的脚本以编程方式截取屏幕截图。使用此脚本，你可以直接截屏或截取特定区域的屏幕截图。
 # Grab Screenshot # pip install pyautogui # pip install Pillow from pyautogui import screenshot import time from PIL import ImageGrab # Grab Screenshot of Screen def grab_screenshot(): shot = screenshot() shot.save(&#39;my_screenshot.png&#39;) # Grab Screenshot of Specific Area def grab_screenshot_area(): area = (0, 0, 500, 500) shot = ImageGrab.grab(area) shot.save(&#39;my_screenshot_area.png&#39;) # Grab Screenshot with Delay def grab_screenshot_delay(): time.sleep(5) shot = screenshot() shot.save(&#39;my_screenshot_delay.png&#39;) 创建有声读物  厌倦了手动将您的 PDF 书籍转换为有声读物，那么这是你的自动化脚本，它使用 GTTS 模块将你的 PDF 文本转换为音频。
 # Create Audiobooks # pip install gTTS # pip install PyPDF2 from PyPDF2 import PdfFileReader as reader from gtts import gTTS def create_audio(pdf_file): read_Pdf = reader(open(pdf_file, &#39;rb&#39;)) for page in range(read_Pdf.numPages): text = read_Pdf.getPage(page).extractText() tts = gTTS(text, lang=&#39;en&#39;) tts.save(&#39;page&#39; + str(page) + &#39;.mp3&#39;) create_audio(&#39;book.pdf&#39;) PDF 编辑器  使用以下自动化脚本使用 Python 编辑 PDF 文件。该脚本使用 PyPDF4 模块，它是 PyPDF2 的升级版本，下面我编写了 Parse Text、Remove pages 等常用功能。当你有大量 PDF 文件要编辑或需要以编程方式在 Python 项目中使用脚本时，这是一个方便的脚本。
 # PDF Editor # pip install PyPDf4 import PyPDF4 # Parse the Text from PDF def parse_text(pdf_file): reader = PyPDF4.PdfFileReader(pdf_file) for page in reader.pages: print(page.extractText()) # Remove Page from PDF def remove_page(pdf_file, page_numbers): filer = PyPDF4.PdfReader(&#39;source.pdf&#39;, &#39;rb&#39;) out = PyPDF4.PdfWriter() for index in page_numbers: page = filer.pages[index] out.add_page(page) with open(&#39;rm.pdf&#39;, &#39;wb&#39;) as f: out.write(f) # Add Blank Page to PDF def add_page(pdf_file, page_number): reader = PyPDF4.PdfFileReader(pdf_file) writer = PyPDF4.PdfWriter() writer.addPage() with open(&#39;add.pdf&#39;, &#39;wb&#39;) as f: writer.write(f) # Rotate Pages def rotate_page(pdf_file): reader = PyPDF4.PdfFileReader(pdf_file) writer = PyPDF4.PdfWriter() for page in reader.pages: page.rotateClockwise(90) writer.addPage(page) with open(&#39;rotate.pdf&#39;, &#39;wb&#39;) as f: writer.write(f) # Merge PDFs def merge_pdfs(pdf_file1, pdf_file2): pdf1 = PyPDF4.PdfFileReader(pdf_file1) pdf2 = PyPDF4.PdfFileReader(pdf_file2) writer = PyPDF4.PdfWriter() for page in pdf1.pages: writer.addPage(page) for page in pdf2.pages: writer.addPage(page) with open(&#39;merge.pdf&#39;, &#39;wb&#39;) as f: writer.write(f) 迷你 Stackoverflow  作为一名程序员，我知道我们每天都需要 StackOverflow，但你不再需要在 Google 上搜索它。现在，在您继续处理项目的同时，在你的 CMD 中获得直接解决方案。通过使用 Howdoi 模块，你可以在命令提示符或终端中获得 StackOverflow 解决方案。你可以在下面找到一些可以尝试的示例。
 # Automate Stackoverflow # pip install howdoi # Get Answers in CMD #example 1 &gt; howdoi how do i install python3 # example 2 &gt; howdoi selenium Enter keys # example 3 &gt; howdoi how to install modules # example 4 &gt; howdoi Parse html with python # example 5 &gt; howdoi int not iterable error # example 6 &gt; howdoi how to parse pdf with python # example 7 &gt; howdoi Sort list in python # example 8 &gt; howdoi merge two lists in python # example 9 &gt;howdoi get last element in list python # example 10 &gt; howdoi fast way to sort list 自动化手机  此自动化脚本将帮助你使用 Python 中的 Android 调试桥 (ADB) 自动化你的智能手机。下面我将展示如何自动执行常见任务，例如滑动手势、呼叫、发送短信等等。您可以了解有关 ADB 的更多信息，并探索更多令人兴奋的方法来实现手机自动化，让您的生活更轻松。
 # Automate Mobile Phones # pip install opencv-python import subprocess def main_adb(cm): p = subprocess.Popen(cm.split(&#39; &#39;), stdout=subprocess.PIPE, shell=True) (output, _) = p.communicate() return output.decode(&#39;utf-8&#39;) # Swipe  def swipe(x1, y1, x2, y2, duration): cmd = &#39;adb shell input swipe {}{}{}{}{}&#39;.format(x1, y1, x2, y2, duration) return main_adb(cmd) # Tap or Clicking def tap(x, y): cmd = &#39;adb shell input tap {}{}&#39;.format(x, y) return main_adb(cmd) # Make a Call def make_call(number): cmd = f&#34;adb shell am start -a android.intent.action.CALL -d tel:{number}&#34; return main_adb(cmd) # Send SMS def send_sms(number, message): cmd = &#39;adb shell am start -a android.intent.action.SENDTO -d sms:{}--es sms_body &#34;{}&#34;&#39;.format(number, message) return main_adb(cmd) # Download File From Mobile to PC def download_file(file_name): cmd = &#39;adb pull /sdcard/{}&#39;.format(file_name) return main_adb(cmd) # Take a screenshot def screenshot(): cmd = &#39;adb shell screencap -p&#39; return main_adb(cmd) # Power On and Off def power_off(): cmd = &#39;&#34;adb shell input keyevent 26&#34;&#39; return main_adb(cmd) 监控 CPU/GPU 温度  你可能使用 CPU-Z 或任何规格监控软件来捕获你的 Cpu 和 Gpu 温度，但你也可以通过编程方式进行。好吧，这个脚本使用 Pythonnet 和 OpenhardwareMonitor 来帮助你监控当前的 Cpu 和 Gpu 温度。你可以使用它在达到一定温度时通知自己，也可以在 Python 项目中使用它来简化日常生活。
 # Get CPU/GPU Temperature # pip install pythonnet import clr clr.AddReference(&#34;OpenHardwareMonitorLib&#34;) from OpenHardwareMonitorLib import * spec = Computer() spec.GPUEnabled = True spec.CPUEnabled = True spec.Open() # Get CPU Temp def Cpu_Temp(): while True: for cpu in range(0, len(spec.Hardware[0].Sensors)): if &#34;/temperature&#34; in str(spec.Hardware[0].Sensors[cpu].Identifier): print(str(spec.Hardware[0].Sensors[cpu].Value)) # Get GPU Temp def Gpu_Temp() while True: for gpu in range(0, len(spec.Hardware[0].Sensors)): if &#34;/temperature&#34; in str(spec.Hardware[0].Sensors[gpu].Identifier): print(str(spec.Hardware[0].Sensors[gpu].Value)) Instagram 上传机器人  Instagram 是一个著名的社交媒体平台，你现在不需要通过智能手机上传照片或视频。你可以使用以下脚本以编程方式执行此操作。
 # Upload Photos and Video on Insta # pip install instabot from instabot import Bot def Upload_Photo(img): robot = Bot() robot.login(user) robot.upload_photo(img, caption=&#34;Medium Article&#34;) print(&#34;Photo Uploaded&#34;) def Upload_Video(video): robot = Bot() robot.login(user) robot.upload_video(video, caption=&#34;Medium Article&#34;) print(&#34;Video Uploaded&#34;) def Upload_Story(img): robot = Bot() robot.login(user) robot.upload_story(img, caption=&#34;Medium Article&#34;) print(&#34;Story Photos Uploaded&#34;) Upload_Photo(&#34;img.jpg&#34;) Upload_Video(&#34;video.mp4&#34;) 视频水印  使用此自动化脚本为你的视频添加水印，该脚本使用 Moviepy，这是一个方便的视频编辑模块。在下面的脚本中，你可以看到如何添加水印并且可以自由使用它。
 # Video Watermark with Python # pip install moviepy from moviepy.editor import * clip = VideoFileClip(&#34;myvideo.mp4&#34;, audio=True) width,height = clip.size text = TextClip(&#34;WaterMark&#34;, font=&#39;Arial&#39;, color=&#39;white&#39;, fontsize=28) set_color = text.on_color(size=(clip.w + text.w, text.h-10), color=(0,0,0), pos=(6,&#39;center&#39;), col_opacity=0.6) set_textPos = set_color.set_pos( lambda pos: (max(width/30,int(width-0.5* width* pos)),max(5*height/6,int(100* pos))) ) Output = CompositeVideoClip([clip, set_textPos]) Output.duration = clip.duration Output.write_videofile(&#34;output.mp4&#34;, fps=30, codec=&#39;libx264&#39;) ]]></content>
  </entry>
  
  <entry>
    <title>FPGA硬核和软核处理器的区别</title>
    <url>/post/fpga/difference-between-hard-core-processor-and-soft-core-processor-of-fpga.html</url>
    <categories><category>FPGA</category>
    </categories>
    <tags>
      <tag>CPU</tag>
      <tag>fpga</tag>
      <tag>processor</tag>
      <tag>Altera</tag>
      <tag>Xilinx</tag>
    </tags>
    <content type="html"><![CDATA[ 从架构的角度来说，SOPC和SoC FPGA是统一的，都是由FPGA部分和处理器部分组成。在SoC FPGA 中，嵌入的是纯硬件基础的硬核处理器，简称HPS(Hardware Processor System)，而SOPC技术中，嵌入的是使用FPGA逻辑资源实现的软核处理器，两者指令集不一样，处理器性能也不一样。
 软核处理器 SOPC技术，即软核处理器，最早是由Altera公司提出来的，它是基于 FPGA  的SOC片上系统设计技术。是使用FPGA的逻辑和资源搭建的一个软核CPU系统，由于是使用FPGA的通用逻辑搭建的CPU，因此具有一定的灵活性，用户可以根据自己的需求对CPU进行定制裁剪，增加一些专用功能，例如除法或浮点运算单元，用于提升CPU在某些专用运算方面的性能，或者删除一些在系统里面使用不到的功能，以节约逻辑资源。
另外也可以根据用户的实际需求，为CPU添加各种标准或定制的外设，例如UART，SPI，IIC等标准接口外设，同时，用户也可以自己使用FPGA的逻辑资源，编写各种专用的外设，然后连接到CPU总线上，由CPU进行控制，以实现软硬件的协同工作，在保证系统性能的同时，增加了系统的灵活性。
而且，如果单个的软核CPU无法满足用户需求，可以添加多个CPU软核，搭建多核系统，通过多核CPU协同工作，让系统拥有更加灵活便捷的控制能力。
由于是使用FPGA资源实现的，所以具有很大的灵活性，可以实现根据需要实现多种处理器，如8051，RISC-V，Xilinx的 MicroBlaze ，Altera的Nios-II等等。
硬核处理器 由于软核CPU是使用FPGA的通用逻辑资源搭建的，相较使用经过布局布线优化的硬核处理器来说，软核处理器够运行的最高实时钟主频要低一些，而且也会相应的消耗较多的FPGA逻辑资源以及片上存储器资源，因此SOPC方案仅适用于对于数处理器整体性能要求不高的应用，例如整个系统的初始化配置，人机交互，多个功能模块间的协调控制等功能。
所以，各大FPGA厂家推出了SoC FPGA技术，是在芯片设计之初，就在内部的硬件电路上添加了硬核处理器，是纯硬件实现的，不会消耗FPGA的逻辑资源，硬核处理器和FPGA逻辑在一定程度上是相互独立的，简单的说，就是SoC FPGA就是把一块ARM处理器和一块FPGA芯片封装成了一个芯片。
例如比较有名的Xilinx的ZYNQ/PYNQ系列集成ARM Cortex-A9处理器，同时具有ARM软件的可编程性和FPGA 的硬件可编程性，不仅可实现重要分析与硬件加速，同时还在单个器件上高度集成 CPU、DSP、ASSP 以及混合信号功能。
ZYNQ开发板 Intel的Cyclone V系列，集成双核Cortex-A9，于2013年发布，在单一芯片上集成了双核的ARM Cortex-A9处理器和FPGA逻辑资源的新型SoC芯片，相较于传统的单一ARM处理器或FPGA芯片，它既拥有了ARM处理器灵活高效的数据运算和事务处理能力，同时又集成了FPGA的高速并行处理优势，同时，基于两者独特的片上互联结构，使用时可以将FPGA上的通用逻辑资源经过配置，映射为ARM处理器的一个或多个具有特定功能的外设，通过高达128位宽的AXI高速总线进行通信，完成数据和控制命令的交互。由于片上的ARM处理器是经过布局布线的硬线逻辑，因此其能工作的时钟主频较高，因此单位时间内能够执行的指令也更多。
区别和联系 从架构的角度来说，SOPC和SoC FPGA是统一的，都是由FPGA部分和处理器部分组成。在SoC FPGA 中，嵌入的是纯硬件基础的硬核处理器，简称HPS(Hardware Processor System)，而SOPC技术中，嵌入的是使用FPGA逻辑资源实现的软核处理器，两者指令集不一样，处理器性能也不一样。
一般来说，硬核处理器的性能要远远高于软核处理器。另外，硬核处理器除了CPU部分，还集成了各种高性能外设，如MMU、DDR3控制器、Nand FLASH控制器等，可以运行成熟的Linux操作系统和应用程序，提供统一的系统API，降低开发者的软件开发难度。而软核CPU虽然可以通过配置，用逻辑资源来搭建相应的控制器以支持相应功能，但是从性能和开发难度上来说，基于SoC FPGA架构进行设计开发是比较好的选择。
ZYNQ内部框图 另外，虽然SoC FPGA芯片上既包含了有ARM，又包含了有FPGA，但是两者一定程度上是相互独立的，SoC芯片上的ARM处理器核并非是包含于FPGA逻辑单元内部的，FPGA和ARM（HPS）处理器只是封装到同一个芯片中，JTAG接口、电源引脚和外设的接口引脚都是独立的，因此，如果使用SoC FPGA芯片进行设计，即使不使用到片上的ARM处理器，ARM处理器部分占用的芯片资源也无法释放出来，不能用作通用的FPGA资源。
而SOPC则是使用FPGA通用逻辑和存储器资源搭建的CPU，当不使用CPU时，CPU部分占用的资源可以被释放，重新用作通用FPGA资源。
]]></content>
  </entry>
  
  <entry>
    <title>详解嵌入式LCD的接口类型</title>
    <url>/post/fpga/embedded-lcd-interface-model.html</url>
    <categories><category>FPGA</category>
    </categories>
    <tags>
      <tag>LCD</tag>
      <tag>RGB</tag>
    </tags>
    <content type="html"><![CDATA[ 从架构的角度来说，SOPC和SoC FPGA是统一的，都是由FPGA部分和处理器部分组成。在SoC FPGA 中，嵌入的是纯硬件基础的硬核处理器，简称HPS(Hardware Processor System)，而SOPC技术中，嵌入的是使用FPGA逻辑资源实现的软核处理器，两者指令集不一样，处理器性能也不一样。
 LCD的接口有多种，分类很细。主要看LCD的驱动方式和控制方式，目前手机上的彩色LCD的连接方式一般有这么几种：MCU模式，RGB模式，SPI模式，VSYNC模式，MDDI模式，DSI模式。MCU模式(也写成MPU模式的)。只有TFT模块才有RGB接口。
但应用比较多的就是MUC模式和RGB模式，区别有以下几点：
MCU接口: 会解码命令，由timing generator产生时序信号，驱动COM和SEG驱器。
RGB接口: 在写LCD register setting时，和MCU接口没有区别。区别只在于图像的写入方式。
用MCU模式时由于数据可以先存到IC内部GRAM后再往屏上写，所以这种模式LCD可以直接接在MEMORY的总线上。
用RGB模式时就不同了，它没有内部RAM，HSYNC，VSYNC，ENABLE，CS，RESET，RS可以直接接在MEMORY的GPIO口上，用GPIO口来模拟波形.
MPU接口方式: 显示数据写入DDRAM，常用于静止图片显示。
RGB接口方式: 显示数据不写入DDRAM，直接写屏，速度快，常用于显示视频或动画用。
主要的区别是: MCU接口方式: 显示数据写入DDRAM，常用于静止图片显示。 RGB接口方式: 显示数据不写入DDRAM，直接写屏，速度快，常用于显示视频或动画用。
MCU模式 因为主要针对单片机的领域在使用,因此得名.后在中低端手机大量使用,其主要特点是价格便宜的。MCU-LCD接口的标准术语是Intel提出的8080总线标准，因此在很多文档中用I80 来指MCU-LCD屏。主要又可以分为8080模式和6800模式，这两者之间主要是时序的区别。数据位传输有8位，9位，16位，18位，24位。连线分为：CS/，RS(寄存器选择)，RD/，WR/，再就是数据线了。优点是：控制简单方便，无需时钟和同步信号。缺点是：要耗费GRAM，所以难以做到大屏(3.8以上)。对于MCU接口的LCM，其内部的芯片就叫LCD驱动器。主要功能是对主机发过的数据/命令，进行变换，变成每个象素的RGB数据，使之在屏上显示出来。这个过程不需要点、行、帧时钟。
MCU接口的LCD的DriverIC都带GRAM，Driver IC作为MCU的一片协处理器，接受MCU发过来的Command/Data，可以相对独立的工作。对于MCU接口的LCM(LCD Module)，其内部的芯片就叫LCD驱动器。主要功能是对主机发过的数据/命令，进行变换，变成每个象素的RGB数据，使之在屏上显示出来。这个过程不需要点、行、帧时钟。
M6800模式 M6800模式支持可选择的总线宽度8/9/16/18-bit(默认为8位)，其实际设计思想是与I80的思想是一样的，主要区别就是该模式的总线控制读写信号组合在一个引脚上(/WR)，而增加了一个锁存信号(E)数据位传输有8位，9位，16位和18位。
I8080模式 I80模式连线分为：CS/，RS(寄存器选择)，RD/，WR/，再就是数据线了。优点是：控制简单方便，无需时钟和同步信号。缺点是：要耗费GRAM，所以难以做到大屏(QVGA以上)。
 MCU接口标准名称是I80，管脚的控制脚有5个： CS 片选信号 RS (置1为写数据,置0为写命令) /WR (为0表示写数据) 数据命令区分信号 /RD (为0表示读数据) RESET 复位LCD( 用固定命令系列 0 1 0来复位)  VSYNC模式 该模式其实就是就是在MCU模式上加了一个VSYNC信号，应用于运动画面更新，这样就与上述两个接口有很大的区别。该模式支持直接进行动画显示的功能，它提供了一个对MCU接口最小的改动，实现动画显示的解决方案。在这种模式下，内部的显示操作与外部VSYNC信号同步。可以实现比内部操作更高的速率的动画显示。但由于其操作方式的不同，该模式对速率有一个限制，那就是对内部SRAM的写速率一定要大于显示读内部SRAM的速率。
RGB模式 大屏采用较多的模式，数据位传输也有6位，16位和18位，24位之分。连线一般有：VSYNC，HSYNC，DOTCLK，CS，RESET，有的也需要RS，剩下就是数据线。它的优缺点正好和MCU模式相反。
MCU-LCD屏它与RGB-LCD屏主要区别在于显存的位置。RGB-LCD的显存是由系统内存充当的，因此其大小只受限于系统内存的大小，这样RGB-LCD可以做出较大尺寸，象现在4.3&quot;只能算入门级，而MID中7&quot;,10&quot;的屏都开始大量使用。而MCU-LCD的设计之初只要考虑单片机的内存较小，因此都是把显存内置在LCD模块内部.然后软件通过专门显示命令来更新显存，因此MCU屏往往不能做得很大。同时显示更新速度也比RGB-LCD慢。显示数据传输模式也有差别。RGB屏只需显存组织好数据。启动显示后，LCD-DMA会自动把显存中的数据通过RGB接口送到LCM。而MCU屏则需要发送画点的命令来修改MCU内部的RAM(即不能直接写MCU屏的RAM)。所以RGB显示速度明显比MCU快，而且播放视频方面，MCU-LCD也比较慢。
对于RGB接口的LCM，主机输出的直接是每个象素的RGB数据，不需要进行变换(GAMMA校正等除外)，对于这种接口，需要在主机部分有个LCD控制器，以产生RGB数据和点、行、帧同步信号。
彩色TFT液晶屏主要有2种接口：TTL接口(RGB颜色接口)， LVDS接口(将RGB颜色打包成差分信号传输)。TTL接口主要用于12.1寸一下的小尺寸TFT屏，LVDS接口主要用于8寸以上的大尺寸TFT屏。TTL接口线多，传输距离短;LVDS接口传输距离长，线的数量少。大屏采用较多的模式，控制脚是VSYNC，HSYNC，VDEN，VCLK， S3C2440最高支持24个数据脚，数据脚是VD[23-0]。
CPU或显卡发出的图像数据是TTL信号(0-5V、0-3.3V、0-2.5V、或0-1.8V)，LCD本身接收的也是TTL信号，由于TTL信号在高速率的长距离传输时性能不佳，抗干扰能力比较差，后来又提出了多种传输模式，比如LVDS、TDMS、GVIF、P&amp;D、DVI和DFP等。他们实际上只是将CPU或显卡发出的TTL信号编码成各种信号以传输，在LCD那边将接收到的信号进行解码得到TTL信号。
但是不管采用何种传输模式，本质的TTL信号是一样的。
注意: TTL/LVDS分别是两种信号的传输模式，TTL是高电平表示1，低电平表示0的模式，LVDS是正负两个对应波形，用两个波形的差值来表示当前是1还是0
SPI模式 采用较少，有3线和4线的，连线为CS/，SLK，SDI，SDO四根线，连线少但是软件控制比较复杂。
MDDI模式(MobileDisplayDigitalInterface) 高通公司于2004年提出的接口MDDI，通过减少连线可提高移动电话的可靠性并降低功耗，这将取代SPI模式而成为移动领域的高速串行接口。 连线主要是host_data,host_strobe,client_data,client_strobe,power,GND几根线。
DSI模式 该模式串行的双向高速命令传输模式，连线有D0P，D0N，D1P，D1N，CLKP，CLKN。
]]></content>
  </entry>
  
  <entry>
    <title>带你走进Linux内核源码中最常见的数据结构之「mutex」</title>
    <url>/post/linux/linux-kernel-source-code-data-structure-mutex.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>kernel</tag>
      <tag>mutex</tag>
    </tags>
    <content type="html"><![CDATA[定义 互斥锁（英语：Mutual exclusion，缩写 Mutex）是一种用于多线程编程中，防止两条线程同时对同一公共资源（比如全域变量）进行读写的机制。
该目的通过将代码切片成一个一个的**临界区域（critical section）**达成。临界区域指的是一块对公共资源进行存取的代码，并非一种机制或是算法。一个程序、进程、线程可以拥有多个临界区域，但是并不一定会应用互斥锁。
例如：一段代码（甲）正在分步修改一块数据。这时，另一条线程（乙）由于一些原因被唤醒。如果乙此时去读取甲正在修改的数据，而甲碰巧还没有完成整个修改过程，这个时候这块数据的状态就处在极大的不确定状态中，读取到的数据当然也是有问题的。更严重的情况是乙也往这块地方写数据，这样的一来，后果将变得不可收拾。因此，多个线程间共享的数据必须被保护。达到这个目的的方法，就是确保同一时间只有一个临界区域处于运行状态，而其他的临界区域，无论是读是写，都必须被挂起并且不能获得运行机会。
互斥锁实现多线程同步的核心思想是：有线程访问进程空间中的公共资源时，该线程执行“加锁”操作（将资源“锁”起来），阻止其它线程访问。访问完成后，该线程负责完成“解锁”操作，将资源让给其它线程。当有多个线程想访问资源时，谁最先完成“加锁”操作，谁就最先访问资源。
当有多个线程想访问“加锁”状态下的公共资源时，它们只能等待资源“解锁”，所有线程会排成一个等待（阻塞）队列。资源解锁后，操作系统会唤醒等待队列中的所有线程，第一个访问资源的线程会率先将资源“锁”起来，其它线程则继续等待。当有多个线程想访问“加锁”状态下的公共资源时，它们只能等待资源“解锁”，所有线程会排成一个等待（阻塞）队列。资源解锁后，操作系统会唤醒等待队列中的所有线程，第一个访问资源的线程会率先将资源“锁”起来，其它线程则继续等待。
mutex有什么缺点？ 不同于mutex最初的设计与目的，现在的struct mutex是内核中最大的锁之一，比如在x86-64上，它差不多有32bytes的大小，而struct samaphore是24bytes，rw_semaphore为40bytes，更大的数据结构意味着占用更多的CPU缓存和更多的内存占用。
什么时候应该使用mutex？ 除非mutex的严格语义要求不合适或者临界区域阻止锁的共享，否则相较于其他锁原语来说更倾向于使用mutex
mutex与spinlock的区别？ spinlock是让一个尝试获取它的线程在一个循环中等待的锁，线程在等待时会一直查看锁的状态。而mutex是一个可以让多个进程轮流分享相同资源的机制
spinlock通常短时间持有，mutex可以长时间持有
spinlock任务在等待锁释放时不可以睡眠，mutex可以
看到一个非常有意思的解释：
spinlock就像是坐在车后座的熊孩子，一直问“到了吗？到了吗？到了吗？…”
mutex就像一个司机返回的信号，说“我们到了！”
实现 看一下Linux kernel-5.8是如何实现mutex的2 实现
struct mutex { atomic_long_t owner; spinlock_t wait_lock; #ifdef CONFIG_MUTEX_SPIN_ON_OWNER  struct optimistic_spin_queue osq; /* Spinner MCS lock */ #endif  struct list_head wait_list; #ifdef CONFIG_DEBUG_MUTEXES  void *magic; #endif #ifdef CONFIG_DEBUG_LOCK_ALLOC  struct lockdep_map dep_map; #endif }; 可以看到，mutex使用了原子变量owner来追踪锁的状态，owner实际上是指向当前mutex锁拥有者的struct task_struct *指针，所以当锁没有被持有时，owner为NULL。
/* * This is the control structure for tasks blocked on mutex, * which resides on the blocked task&#39;s kernel stack: * 表示等待队列wait_list中进程的结构体 */ struct mutex_waiter { struct list_head list; struct task_struct *task; struct ww_acquire_ctx *ww_ctx; #ifdef CONFIG_DEBUG_MUTEXES  void *magic; #endif }; 上锁 当要获取mutex时，通常有三种路径方式
fastpath: 通过 cmpxchg() 当前任务与所有者来尝试原子性的获取锁。这仅适用于无竞争的情况（cmpxchg() 检查 0UL，因此上面的所有 3 个状态位都必须为 0）。如果锁被争用，它会转到下一个可能的路径。
midpath: 又名乐观旋转（optimistic spinning）—在锁的持有者正在运行并且没有其他具有更高优先级（need_resched）的任务准备运行时，通过旋转来获取锁。理由是如果锁的所有者正在运行，它很可能很快就会释放锁。mutex spinner使用 MCS 锁排队，因此只有一个spinner可以竞争mutex。
MCS 锁（由 Mellor-Crummey 和 Scott 提出）是一个简单的自旋锁，具有公平的理想属性，每个 cpu 都试图获取在本地变量上旋转的锁，排队采用的是链表实现的FIFO。它避免了常见的test-and-set自旋锁实现引起的昂贵的cacheline bouncing。类似MCS的锁是专门为睡眠锁的乐观旋转而量身定制的（毕竟如果只是短暂的自旋比休眠效率要高）。自定义 MCS 锁的一个重要特性是它具有额外的属性，即当spinner需要重新调度时，它们能够直接退出 MCS 自旋锁队列。这有助于避免需要重新调度的 MCS spinner持续在mutex持有者上自旋，而仅需直接进入慢速路径获取MCS锁。
slowpath: 最后的手段，如果仍然无法获得锁，则将任务添加到等待队列并休眠，直到被解锁路径唤醒。在正常情况下它阻塞为 TASK_UNINTERRUPTIBLE。 虽然正式的内核互斥锁是可休眠的锁，但midpath路径 (ii) 使它们更实际地成为混合类型。通过简单地不中断任务并忙于等待几个周期而不是立即休眠，此锁的性能已被视为显着改善了许多工作负载。请注意，此技术也用于 rw 信号量。
具体代码调用链很长…
/*不可中断的获取锁*/ void __sched mutex_lock(struct mutex *lock) { might_sleep(); /*fastpath*/ if (!__mutex_trylock_fast(lock)) /*midpath and slowpath*/ __mutex_lock_slowpath(lock); } __mutex_trylock_fast(lock) -&gt; atomic_long_try_cmpxchg_acquire(&amp;lock-&gt;owner, &amp;zero, curr) -&gt; atomic64_try_cmpxchg_acquire(v, (s64 *)old, new); __mutex_lock_slowpath(lock)-&gt;__mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_) -&gt; __mutex_lock_common(lock, state, subclass, nest_lock, ip, NULL, false) /*可中断的获取锁*/ int mutex_lock_interruptible(struct mutex *lock); 尝试上锁 int __sched mutex_trylock(struct mutex *lock) { bool locked; #ifdef CONFIG_DEBUG_MUTEXES  DEBUG_LOCKS_WARN_ON(lock-&gt;magic != lock); #endif  locked = __mutex_trylock(lock); if (locked) mutex_acquire(&amp;lock-&gt;dep_map, 0, 1, _RET_IP_); return locked; } static inline bool __mutex_trylock(struct mutex *lock) { return !__mutex_trylock_or_owner(lock); } 释放锁 void __sched mutex_unlock(struct mutex *lock) { #ifndef CONFIG_DEBUG_LOCK_ALLOC  if (__mutex_unlock_fast(lock)) return; #endif  __mutex_unlock_slowpath(lock, _RET_IP_); } 跟加锁对称，也有fastpath, midpath, slowpath三条路径。 判断锁状态
bool mutex_is_locked(struct mutex *lock) { return __mutex_owner(lock) != NULL; } 很显而易见，mutex持有者不为NULL即表示锁定状态。
实际案例 实验：
#include &lt;pthread.h&gt;#include &lt;stdio.h&gt; #define LOOP 1000000  int cnt = 0; int cs1 = 0, cs2 = 0; void* task(void* args) { while(1) { if(cnt &gt;= LOOP) { break; } cnt++; if((int)args == 1) cs1 ++; else cs2++; } return NULL; } int main() { pthread_t tid1; pthread_t tid2; /* create the thread */ pthread_create(&amp;tid1, NULL, task, (void*)1); pthread_create(&amp;tid2, NULL, task, (void*)2); /* wait for thread to exit */ pthread_join(tid1, NULL); pthread_join(tid2, NULL); printf(&#34;cnt = %d cs1=%d cs2=%d total=%d\n&#34;, cnt,cs1,cs2,cs1+cs2); return 0; } 输出：
cnt = 1000000 cs1=958560 cs2=1520226 total=2478786 正确结果不应该是1000000吗？为什么会出错呢，我们可以从汇编角度来分析一下。
$&gt; g++ -E test.c -o test.i $&gt; g++ -S test.i -o test.s $&gt; vim test.s .file &#34;test.c&#34; .globl _cnt .bss .align 4 _cnt: .space 4 .text .globl __Z5task1Pv .def __Z5task1Pv; .scl 2; .type 32; .endef __Z5task1Pv: ... 我们可以看到一个简单的cnt++，对应
movl _cnt, %eax addl $1, %eax movl %eax, _cnt CPU先将cnt的值读到寄存器eax中，然后将[eax] + 1，最后将eax的值返回到cnt中，这些操作不是**原子性质(atomic)**的，这就导致cnt被多个线程操作时，+1过程会被打断。
加入mutex保护临界资源
#include &lt;pthread.h&gt;#include &lt;stdio.h&gt; #define LOOP 1000000  pthread_mutex_t mutex; int cnt = 0; int cs1 = 0, cs2 = 0; void* task(void* args) { while(1) { pthread_mutex_lock(&amp;mutex); if(cnt &gt;= LOOP) { pthread_mutex_unlock(&amp;mutex); break; } cnt++; pthread_mutex_unlock(&amp;mutex); if((int)args == 1) cs1 ++; else cs2++; } return NULL; } int main() { pthread_mutex_init(&amp;mutex , NULL); pthread_t tid1; pthread_t tid2; /* create the thread */ pthread_create(&amp;tid1, NULL, task, (void*)1); pthread_create(&amp;tid2, NULL, task, (void*)2); /* wait for thread to exit */ pthread_join(tid1, NULL); pthread_join(tid2, NULL); printf(&#34;cnt = %d cs1=%d cs2=%d total=%d\n&#34;, cnt,cs1,cs2,cs1+cs2); return 0; } 输出：
cnt = 1000000 cs1=517007 cs2=482993 total=1000000 ]]></content>
  </entry>
  
  <entry>
    <title>openssl命令</title>
    <url>/post/linux/openssl.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>openssl</tag>
      <tag>系统管理</tag>
      <tag>系统安全</tag>
    </tags>
    <content type="html"><![CDATA[ 强大的安全套接字层密码库
 OpenSSL是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及SSL协议，并提供丰富的应用程序供测试或其它目的使用。在OpenSSL被曝出现严重安全漏洞后，发现多数通过SSL协议加密的网站使用名为OpenSSL的开源软件包。由于这是互联网应用最广泛的安全传输方法，被网银、在线支付、电商网站、门户网站、电子邮件等重要网站广泛使用，所以该漏洞影响范围广大。
OpenSSL有两种运行模式：交互模式和批处理模式。
直接输入openssl回车进入交互模式，输入带命令选项的openssl进入批处理模式。
OpenSSL整个软件包大概可以分成三个主要的功能部分：密码算法库、SSL协议库以及应用程序。OpenSSL的目录结构自然也是围绕这三个功能部分进行规划的。 对称加密算法 OpenSSL一共提供了8种对称加密算法，其中7种是分组加密算法，仅有的一种流加密算法是RC4。这7种分组加密算法分别是AES、DES、Blowfish、CAST、IDEA、RC2、RC5，都支持电子密码本模式（ECB）、加密分组链接模式（CBC）、加密反馈模式（CFB）和输出反馈模式（OFB）四种常用的分组密码加密模式。其中，AES使用的加密反馈模式（CFB）和输出反馈模式（OFB）分组长度是128位，其它算法使用的则是64位。事实上，DES算法里面不仅仅是常用的DES算法，还支持三个密钥和两个密钥3DES算法。 非对称加密算法 OpenSSL一共实现了4种非对称加密算法，包括DH算法、RSA算法、DSA算法和椭圆曲线算法（EC）。DH算法一般用户密钥交换。RSA算法既可以用于密钥交换，也可以用于数字签名，当然，如果你能够忍受其缓慢的速度，那么也可以用于数据加密。DSA算法则一般只用于数字签名。 信息摘要算法 OpenSSL实现了5种信息摘要算法，分别是MD2、MD5、MDC2、SHA（SHA1）和RIPEMD。SHA算法事实上包括了SHA和SHA1两种信息摘要算法，此外，OpenSSL还实现了DSS标准中规定的两种信息摘要算法DSS和DSS1。 密钥和证书管理 密钥和证书管理是PKI的一个重要组成部分，OpenSSL为之提供了丰富的功能，支持多种标准。 首先，OpenSSL实现了ASN.1的证书和密钥相关标准，提供了对证书、公钥、私钥、证书请求以及CRL等数据对象的DER、PEM和BASE64的编解码功能。OpenSSL提供了产生各种公开密钥对和对称密钥的方法、函数和应用程序，同时提供了对公钥和私钥的DER编解码功能。并实现了私钥的PKCS#12和PKCS#8的编解码功能。OpenSSL在标准中提供了对私钥的加密保护功能，使得密钥可以安全地进行存储和分发。 在此基础上，OpenSSL实现了对证书的X.509标准编解码、PKCS#12格式的编解码以及PKCS#7的编解码功能。并提供了一种文本数据库，支持证书的管理功能，包括证书密钥产生、请求产生、证书签发、吊销和验证等功能。 事实上，OpenSSL提供的CA应用程序就是一个小型的证书管理中心（CA），实现了证书签发的整个流程和证书管理的大部分机制。
实例 1、消息摘要算法应用例子 用SHA1算法计算文件file.txt的哈西值，输出到stdout：
# openssl dgst -sha1 file.txt 用SHA1算法计算文件file.txt的哈西值，输出到文件digest.txt：
# openssl sha1 -out digest.txt file.txt 用DSS1(SHA1)算法为文件file.txt签名，输出到文件dsasign.bin。签名的private key必须为DSA算法产生的，保存在文件dsakey.pem中。
# openssl dgst -dss1 -sign dsakey.pem -out dsasign.bin file.txt 用dss1算法验证file.txt的数字签名dsasign.bin，验证的private key为DSA算法产生的文件dsakey.pem。
# openssl dgst -dss1 -prverify dsakey.pem -signature dsasign.bin file.txt 用sha1算法为文件file.txt签名,输出到文件rsasign.bin，签名的private key为RSA算法产生的文件rsaprivate.pem。
# openssl sha1 -sign rsaprivate.pem -out rsasign.bin file.txt # 用sha1算法验证file.txt的数字签名rsasign.bin，验证的public key为RSA算法生成的rsapublic.pem。 # openssl sha1 -verify rsapublic.pem -signature rsasign.bin file.txt 2、对称加密应用例子 对称加密应用例子，用DES3算法的CBC模式加密文件plaintext.doc，加密结果输出到文件ciphertext.bin。
# openssl enc -des3 -salt -in plaintext.doc -out ciphertext.bin 用DES3算法的OFB模式解密文件ciphertext.bin，提供的口令为trousers，输出到文件plaintext.doc。注意：因为模式不同，该命令不能对以上的文件进行解密。
# openssl enc -des-ede3-ofb -d -in ciphertext.bin -out plaintext.doc -pass pass:trousers 用Blowfish的CFB模式加密plaintext.doc，口令从环境变量PASSWORD中取，输出到文件ciphertext.bin。
# openssl bf-cfb -salt -in plaintext.doc -out ciphertext.bin -pass env:PASSWORD 给文件ciphertext.bin用base64编码，输出到文件base64.txt。
# openssl base64 -in ciphertext.bin -out base64.txt 用RC5算法的CBC模式加密文件plaintext.doc，输出到文件ciphertext.bin，salt、key和初始化向量(iv)在命令行指定。
# openssl rc5 -in plaintext.doc -out ciphertext.bin -S C62CB1D49F158ADC -iv E9EDACA1BD7090C6 -K 89D4B1678D604FAA3DBFFD030A314B29 3、Diffie-Hellman应用例子 使用生成因子2和随机的1024-bit的素数产生D0ffie-Hellman参数，输出保存到文件dhparam.pem
# openssl dhparam -out dhparam.pem -2 1024 从dhparam.pem中读取Diffie-Hell参数，以C代码的形式，输出到stdout。
# openssl dhparam -in dhparam.pem -noout -C 4、DSA应用例子应用例子 生成1024位DSA参数集，并输出到文件dsaparam.pem。
# openssl dsaparam -out dsaparam.pem 1024 使用参数文件dsaparam.pem生成DSA私钥匙，采用3DES加密后输出到文件dsaprivatekey.pem
# openssl gendsa -out dsaprivatekey.pem -des3 dsaparam.pem 使用私钥匙dsaprivatekey.pem生成公钥匙，输出到dsapublickey.pem
# openssl dsa -in dsaprivatekey.pem -pubout -out dsapublickey.pem 从dsaprivatekey.pem中读取私钥匙，解密并输入新口令进行加密，然后写回文件dsaprivatekey.pem
# openssl dsa -in dsaprivatekey.pem -out dsaprivatekey.pem -des3 -passin 5、RSA应用例子 产生1024位RSA私匙，用3DES加密它，口令为trousers，输出到文件rsaprivatekey.pem
# openssl genrsa -out rsaprivatekey.pem -passout pass:trousers -des3 1024 从文件rsaprivatekey.pem读取私匙，用口令trousers解密，生成的公钥匙输出到文件rsapublickey.pem
# openssl rsa -in rsaprivatekey.pem -passin pass:trousers -pubout -out rsapubckey.pem 用公钥匙rsapublickey.pem加密文件plain.txt，输出到文件cipher.txt
# openssl rsautl -encrypt -pubin -inkey rsapublickey.pem -in plain.txt -out cipher.txt 使用私钥匙rsaprivatekey.pem解密密文cipher.txt，输出到文件plain.txt
# openssl rsautl -decrypt -inkey rsaprivatekey.pem -in cipher.txt -out plain.txt 用私钥匙rsaprivatekey.pem给文件plain.txt签名，输出到文件signature.bin
# openssl rsautl -sign -inkey rsaprivatekey.pem -in plain.txt -out signature.bin 用公钥匙rsapublickey.pem验证签名signature.bin，输出到文件plain.txt
# openssl rsautl -verify -pubin -inkey rsapublickey.pem -in signature.bin -out plain 从X.509证书文件cert.pem中获取公钥匙，用3DES加密mail.txt，输出到文件mail.enc
# openssl smime -encrypt -in mail.txt -des3 -out mail.enc cert.pem 从X.509证书文件cert.pem中获取接收人的公钥匙，用私钥匙key.pem解密S/MIME消息mail.enc，结果输出到文件mail.txt
# openssl smime -decrypt -in mail.enc -recip cert.pem -inkey key.pem -out mail.txt cert.pem为X.509证书文件，用私匙key,pem为mail.txt签名，证书被包含在S/MIME消息中，输出到文件mail.sgn
# openssl smime -sign -in mail.txt -signer cert.pem -inkey key.pem -out mail.sgn 验证S/MIME消息mail.sgn，输出到文件mail.txt，签名者的证书应该作为S/MIME消息的一部分包含在mail.sgn中
# openssl smime -verify -in mail.sgn -out mail.txt ]]></content>
  </entry>
  
  <entry>
    <title>syslog命令</title>
    <url>/post/linux/syslog.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>syslog</tag>
      <tag>系统管理</tag>
      <tag>系统安全</tag>
    </tags>
    <content type="html"><![CDATA[ 系统默认的日志守护进程
 syslog是Linux系统默认的日志守护进程。
概述 默认的syslog配置文件是/etc/syslog.conf文件。程序，守护进程和内核提供了访问系统的日志信息。因此，任何希望生成日志信息的程序都可以向 syslog 接口呼叫生成该信息。
几乎所有的网络设备都可以通过syslog协议，将日志信息以用户数据报协议(UDP)方式传送到远端服务器，远端接收日志服务器必须通过syslogd监听UDP 端口514，并根据 syslog.conf配置文件中的配置处理本机，接收访问系统的日志信息，把指定的事件写入特定文件中，供后台数据库管理和响应之用。意味着可以让任何事件都登录到一台或多台服务器上，以备后台数据库用off-line(离线) 方法分析远端设备的事件。
通常，syslog 接受来自系统的各种功能的信息，每个信息都包括重要级。/etc/syslog.conf 文件通知 syslogd 如何根据设备和信息重要级别来报告信息。
使用方法 在/var/log中创建并写入日志信息是由syslog协议处理的，是由守护进程sylogd负责执行。每个标准的进程都可以用syslog记录日志。可以使用logger命令通过syslogd记录日志。
要向syslog文件/var/log/messages中记录日志信息：
logger this is a test log line 输出： tail -n 1 messages Jan 5 10:07:03 localhost root: this is a test log line 如果要记录特定的标记（tag）可以使用：
logger -t TAG this is a test log line 输出： tail -n 1 messages Jan 5 10:37:14 localhost TAG: this is a test log line ]]></content>
  </entry>
  
  <entry>
    <title>如何在Ubuntu Linux下将mp4转成mp3</title>
    <url>/post/linux/how-to-convert-mp4-to-mp3-in-ubuntu-linux.html</url>
    <categories><category>Linux</category>
    </categories>
    <tags>
      <tag>ubuntu</tag>
      <tag>ffmpeg</tag>
      <tag>mp4</tag>
      <tag>mp3</tag>
    </tags>
    <content type="html"><![CDATA[ FFmpeg是一款开源软件，用于生成处理多媒体数据的各类库和程序。FFmpeg可以转码、处理视频和图片（调整视频、图片大小，去噪等）、打包、传输及播放视频。
 本文描述了如何在Ubuntu Linux系统下，通过ffmpeg将mp4文件转成mp3文件。
为什么要将mp4转成mp3 因为这样可以节省空间，一些基本的设备是不支持mp4扩展名的文件，在这个例子里，我们将使用ffmpeg将mp4文件转成mp3文件。
FFmpeg是一个完整的跨平台的解决方案，用来录制，转化以及分流音视频，它包括业界领先的音视频编码库 labavcodec 。
在ubuntu上安装ffmpeg sudo apt-get install ffmpeg libavcodec-extra-53 将mp4转成mp3 基本的命令
ffmpeg -i filename.mp4 filename.mp3 可以用命令`man ffmpeg&rsquo;来查看更多选项
ffmpeg -i filename.mp4 -b:a 192K -vn filename.mp3 一个流的说明符可以匹配一些流，这些选项会适用于所有的流，比如，在-b:a 128k选项中的流说明符可以匹配所有的音频流。
通过脚本 下面这个脚本会将Music目录下的带有.mp4扩展名的文件转成.mp3扩展名的文件。
#!/bin/bash MP4FILE=$(ls ~/Music/ |grep .mp4) for filename in $MP4FILE do name=`echo &#34;$filename&#34; | sed -e &#34;s/.mp4$//g&#34;` ffmpeg -i ~/Music/$filename -b:a 192K -vn ~/Music/$name.mp3 done ]]></content>
  </entry>
  
  <entry>
    <title>风河携手TCS建构5G/Open RAN分布式移动网络基础设施生态系统</title>
    <url>/post/news/windriver-and-TCS-build-5G-Open-Ran-ecos.html</url>
    <categories><category>News</category>
    </categories>
    <tags>
      <tag>WindRiver</tag>
      <tag>vRan</tag>
      <tag>TCS</tag>
      <tag>5G</tag>
    </tags>
    <content type="html"><![CDATA[ 全球领先的关键任务智能系统软件提供商风河公司®宣布，正在与塔塔咨询服务公司（TCS）合作，在Wind River Studio上托管vRAN解决方案。这项战略合作将创建一个全栈移动基础设施解决方案，在4G-5G vRAN下一代网络中开展TCS部署和工程服务，并以Studio作为云平台。
 TCS网络解决方案与服务副总裁Vimal Kumar表示：“我们很高兴与风河合作，帮助我们的客户借助5G技术改善他们的业务。我们的Cognitive Network Operations平台运行在Wind River Studio之上，由此帮助电信网络运营商运用AI和ML技术来监测网络健康状况，预测可能发生的故障，提供以客户为中心的网络体验，并确保卓越的服务质量。”
风河公司首席产品官Avijit Sinha表示：“运营商正在致力于创造数字化、云原生的未来，他们正在寻求灵活、经济的解决方案，以便降低部署复杂度并进行持续性维护。风河公司提供了成熟的生产就绪产品，与领先运营商实现了实用化部署，其基础正是经过广泛验证的Wind River Studio技术。”
风河公司印度销售主管Rajeev Rawal表示：“与TCS携手，提供敏捷、安全、可靠和超低延迟解决方案，以支持新的应用场景，让云计算、边缘计算和智能化技术承担起更加重要的任务。”
作为5G市场的领导者，风河在世界首次成功5G数据会话和商业vRAN/O-RAN项目中发挥了关键作用，其中包括世界上最大的Open RAN网络。
Wind River Studio提供了一个完全基于云原生、Kubernetes和容器的体系结构，可用于大规模分布式边缘网络的开发、部署、运营和服务。这套平台为地理分布的管理解决方案提供了基础，能够为数千个节点提供单一窗口（SPoG）、零接触的自动化管理，从而简化Day 1和Day 2运营，而且与节点的物理位置无关。Studio解决了部署和管理物理地理分散云原生vRAN基础设施的复杂挑战，在vRAN部署中提供了传统的RAN性能。
 塔塔咨询服务 (TCS）简介
 塔塔咨询服务公司是一家IT服务、咨询和业务解决方案提供商，50多年来一直与许多全球最大企业合作，帮助他们实现转型。TCS提供以咨询为主导、以认知为动力的综合性商业、技术和工程服务以及解决方案。所有这些都通过独特的Location Independent Agile™ 模式来提供，被作为卓越软件开发的基准指标。
作为印度最大的跨国商业集团塔塔集团的一部分，TCS在55个国家拥有超过606,000名训练有素的咨询师。在截至2022年3月31日的财年中，TCS创造了257亿美元的合并营收，并在印度的BSE和NSE上市。
]]></content>
  </entry>
  
  <entry>
    <title>VxWorks 6.8下基于QT的串口编程</title>
    <url>/post/vxworks/vxworks-6.8-qt-uart-programming.html</url>
    <categories><category>VxWorks</category>
    </categories>
    <tags>
      <tag>VxWorks</tag>
      <tag>VxWorks 6.8</tag>
      <tag>UART</tag>
      <tag>QT</tag>
      <tag>串口</tag>
      <tag>编程</tag>
    </tags>
    <content type="html"><![CDATA[文章简要记录了VxWorks 6.8下基于Qt实现的串口编程。
相关的VxWorks 和 串口，请参阅 VxWorks下的串口测试程序设计和源码  。
VxWorks简介 VxWorks 操作系统是美国WindRiver公司于1983年设计开发的一种嵌入式实时操作系统（RTOS），是嵌入式开发环境的关键组成部分。良好的持续发展能力、高性能的内核以及友好的用户开发环境，在嵌入式实时操作系统领域占据一席之地。它以其良好的可靠性和卓越的实时性被广泛地应用在通信、军事、航空、航天等高精尖技术及实时性要求极高的领域中，如卫星通讯、军事演习、弹道制导、飞机导航等。在美国的 F-16、FA-18战斗机、B-2 隐形轰炸机和爱国者导弹上，甚至连1997年4月在火星表面登陆的火星探测器、2008年5月登陆的凤凰号，和2012年8月登陆的好奇号也都使用到了VxWorks。
串口简介 串行接口(Serial Interface) 简称串口，也称串行通信接口或串行通讯接口（通常指COM接口），是采用串行通信方式的扩展接口，指数据一位一位地顺序传送。
串行接口的特点是通信线路简单，只要一对传输线就可以实现双向通信（可以直接利用电话线作为传输线），从而大大降低了成本，特别适用于远距离通信，但传送速度较慢。常见的有一般计算机应用的RS-232（使用 25 针或 9 针连接器）和工业计算机应用的半双工RS-485与全双工RS-422。
我这里使用了232和422传输方式，在我本人理解这两种方式根据需求硬件已经做好的传输方式（也可以在BIOS设置），我们知道是什么传输方式，做到心中有数和如何搭建测试环境，今天在这里教大家个简单的232-9针连接器的接线方式，一般没接触过的拿过来一脸懵逼，好家伙9跟针都不知道是干嘛的，那么我告诉你如果是 232-9针，什么也别管直接找到第2针和第3针用杜邦线回连，这时你就具备环境自己检测板卡串口模块是否好用，如果测试程序一定记得把第5跟针要连接上，否则会出现数据不精准的情况（文章底部有贴图）。
在软件层面上只需要关注数据位、停止位、奇偶效验、读取方式和效率即可；
232串口接线说明 RS232串口接线方法：直连和交叉接法
一般情况下，设备和电脑的连接通讯，需用到RS232串口线直连线；而设备和设备的连接通讯，就会用到RS232串口线的交叉线。用户在选择的时候，应根据两个设备之间连接的实际情况，选择不同接法的RS232串口线。
代码实例 VxWorks串口所需要包含的头文件 #include &#34;vxWorks.h&#34;#include &#34;stdIo.h&#34;#include &#34;ioLib.h&#34;#include &#34;sysLib.h&#34;#include &#34;string.h&#34;#include &#34;taskLib.h&#34;VxWorks串口配置函数 ioctl(m_SeriPort,SIO_HW_OPTS_SET, CLOCAL | CS8 | PARODD | PARENB);	//8位数据位|1位停止位|偶效验 ioctl(m_SeriPort,FIOBAUDRATE,9600);	//波特率9600 ioctl(m_SeriPort,FIOSETOPTIONS,OPT_RAW);	//设置串口raw模式 ioctl(m_SeriPort,FIOFLUSH,0);	//清空输入输出的缓冲区 open函数 #define SERI_NAME &#34;/tyCo/0&#34; int m_SeriPort = open(SERI_NAME ,O_RDWR,0); int m_SeriPort = open(SERI_NAME ,O_WRONLY,0); write函数 char* sendData; int writeCom = write(m_SeriPort, sendData,strlen(sendData)); read函数 char data; int readCom = read(m_SeriPort,&amp;data,1); Seri_Demo_Qt_Vx #ifndef THREAD_H #define THREAD_H #include &lt;QThread&gt;#include &lt;QDebug&gt;#include &#34;vxWorks.h&#34;#include &#34;stdIo.h&#34;#include &#34;ioLib.h&#34;#include &#34;sysLib.h&#34;#include &#34;string.h&#34;#include &#34;taskLib.h&#34;class Thread : public QThread { Q_OBJECT public: explicit Thread(QObject *parent = 0); ~Thread(); void run(); //重写run函数 public: bool openSeri(QString comPort,int baudRate); //打开串口  void closeSeri(); //关闭串口  void writeSeri(char* sendData); //发送数据  void setFlag(bool flag = true); //线程数据标志位 signals: void RecvData(char data); private: bool seriStop; //读取数据标志位 true读取数据 false退出循环  int m_SeriPort; //串口文件描述符  QString m_SeriName; //串口名  int m_baud; //波特率 }; #endif //THREAD_H #include &#34;thread.h&#34; Thread::Thread(QObject *parent) : QThread(parent) { } Thread::~Thread() { } void Thread::run() { sysClkRateSet(1000); char rData; while(1) { int readCom = read(m_SeriPort,&amp;rData,1); if(readCom &gt; 0) { printf(&#34;%c\n&#34;,rData); emit RecvData(rData); if(seriStop == false) { qDebug()&lt;&lt; &#34;isStop == false break&#34;; break; } } else { taskDelay(10); } } } bool Thread::openSeri(QString comPort, int baudRate) { this-&gt;m_SeriName = comPort; this-&gt;m_baud = baudRate; qDebug()&lt;&lt; &#34;Thread::openSeri&#34; &lt;&lt; comPort.toUtf8().data() &lt;&lt; baudRate; m_SeriPort = open(comPort.toUtf8().data(),O_RDWR,0); if(m_SeriPort == ERROR) { qDebug()&lt;&lt; &#34;open :&#34; &lt;&lt; comPort.toUtf8().data() &lt;&lt; &#34; = &#34; &lt;&lt;m_SeriPort &lt;&lt; &#34;failed !&#34;; return false; } ioctl(m_SeriPort,SIO_HW_OPTS_SET, CLOCAL | CS8 | PARODD | PARENB); ioctl(m_SeriPort,FIOBAUDRATE,baudRate); ioctl(m_SeriPort,FIOSETOPTIONS,OPT_RAW); ioctl(m_SeriPort,FIOFLUSH,0); qDebug()&lt;&lt; &#34;open :&#34; &lt;&lt; comPort.toUtf8().data() &lt;&lt; &#34; = &#34; &lt;&lt; m_SeriPort &lt;&lt; &#34;succeeded !&#34;; return true; } void Thread::closeSeri() { if(seriStop == false) { qDebug()&lt;&lt; &#34;Thread::closeSeri&#34;; close(m_SeriPort); } } void Thread::writeSeri(char* sendData) { if(m_SeriPort == ERROR) { openSeri(m_SeriName,m_baud); } int writeCom = write(m_SeriPort, sendData,strlen(sendData)); qDebug()&lt;&lt; sendData &lt;&lt; writeCom; } void Thread::setFlag(bool flag) { this-&gt;seriStop = flag; qDebug()&lt;&lt; &#34;Thread::setFlag&#34; &lt;&lt; flag; } TestSeri_Demo_Qt_Vx_Demo #ifndef SERI_H #define SERI_H  #include &lt;QObject&gt;#include &lt;QDebug&gt;#include &#34;thread.h&#34; class Seri : public QObject { Q_OBJECT public: explicit Seri(QObject *parent = 0); ~Seri(); public: /*	open_Seri	打开串口 * comName	串口名 * comBaud	串口波特率 *	return 成功 true 失败 false */ bool open_Seri(QString comName,int comBaud); /* write_Seri	发送数据 * comData	发送数据内容 */ void write_Seri(QByteArray comData); /*	* close_Seri	关闭串口 */ void close_Seri(); signals: send_Seri(char data); private: Thread* m_pThread; }; #endif // SERI_H #include &#34;Seri.h&#34; Seri::Seri(QObject *parent) : QObject(parent) { m_pThread = new Thread; } Seri::~Seri() { if(m_pThread){ delete m_pThread; m_pThread=NULL; } } bool Seri::open_Seri(QString comName,int comBaud) { if(m_pThread-&gt;openSeri(comName,comBaud))//如果打开成功 	{ m_pThread-&gt;setFlag(true); m_pThread-&gt;start(); } return false; } void Seri::write_Seri(QByteArray comData) { m_pThread-&gt;writeSeri(comData.data()); } void Seri::close_Seri() { if(m_pThread-&gt;isRunning())//如果线程还在运行 --&gt; 退出循环接收数据 --&gt; 关闭串口 --&gt; 退出线程 --&gt; 回收线程 	{ m_pThread-&gt;setFlag(false); m_pThread-&gt;closeSeri(); m_pThread-&gt;quit(); m_pThread-&gt;wait(); } } 程序代码说明：  thread类为配置串口类 seri类为外部使用类 接收到的数据是利用信号槽为接口把数据传输出去 ]]></content>
  </entry>
  
  <entry>
    <title>VxWorks操作系统下的串口读写程序</title>
    <url>/post/vxworks/vxworks-uart-read-write-programming.html</url>
    <categories><category>VxWorks</category>
    </categories>
    <tags>
      <tag>VxWorks</tag>
      <tag>UART</tag>
      <tag>串口</tag>
      <tag>编程</tag>
    </tags>
    <content type="html"><![CDATA[关于传统的串口编程，在各大操作系统下的流程基本是一致的，只是针对不同的操作系统，函数接口可能有所差异而已，下面讲述VxWorks操作系统下对于串口读写的编程步骤和代码
相关的VxWorks 和 串口，请参阅 VxWorks下的串口测试程序设计和源码  。
串口配置过程 打开串口 fd = open(&#34;/tyCo/0&#34;, O_RDWR, 0);  &ldquo;/tyCo/0&rdquo;: 串口1的设备名 O_RDWR: 按照读写方式打开串口  设置串口raw模式，清空输入输出的缓冲区 在VxWorks中配置串口可以直接通过ioctl的控制命令来实现
ioctl(fd,FIOSETOPTIONS,OPT_RAW); ioctl(fd,FIOFLUSH,0); ioctl(int fd,int function,int arg); function的参数如下：
   参数 说明     FIOBAUDRATE 设置波特率，arg为一整数，表示要设定的波特率   FIOGETOPTIONS 取得设备控制字，arg表示读出的内容存放的位置   FIOSETOPTIONS 设置设备控制字，arg表示要设置的选项   FIOGETNAME 取得文件描述符对应的文件名，arg存放文件名的缓冲区   FIOREAD 取得输入缓冲区内未读取的字符数，arg用于接收结果的整型指针   FIOWRITE 取得输出缓冲区内的字符个数，arg用于接收结果的整型指针   FIOFLUSH 清空输入输出缓冲区的字符   FIOCANCEL 取消读和写    设置波特率，数据位，停止位，校验方式 在 VxWorks 中设置串口也是用 &lsquo;ioctl&rsquo; 系统调用加控制命令实现，其控制命令为&rsquo;SIO_HW_OPTS_SET'，第三个参数跟配置参数，如：数据位为8，停止位为1，无奇偶校验位，无流控可以这样配置
ioctl(fd,SIO_HW_OPTS_SET,CS8|PARENB|CLOCAL|CREAD); 具体各项参数意义如下：
   参数 说明     CLOCAL 忽略modem控制信号   CREAD 启动接收器   CSIZE 指定数据位：CS5~CS8   HUPCL 最后关闭时挂断modem连接   STOP8 被设置时指定2位停止位，否则默认为1位停止位   PARENB 被设置时启用奇偶校验，否则默认为无奇偶校验   PARODD 被设置时启用奇校验，否则默认为偶校验(PARENB设置时才有效)    串口读写操作 在VxWorks系统中串口的读写操作非常简单，直接使用系统调用函数 read() 和 write() 就能实现串口的读写操作。
int read(int fd, char *buffer, size_t maxbytes) 参数说明：
 fd: 用open函数打开串口设备返回的文件描述符 buffer: 读取的内容将要存放的地址，为指针变量 maxbytes: 读取的最大字节数  int write(int fd, char *buffer, size_t nbytes) 参数说明：
 fd: 用open函数打开串口设备返回的文件描述符 buffer: 将要写的内容的地址，为指针变量，通常为字符串首地址 nbytes: 将要写入的字节数，通常为要写入的字符串的长度  实例代码 VxWorks系统下串口读写的实例代码，仅供参考。
#include &#34;vxWorks.h&#34;#include &#34;stdio.h&#34;#include &#34;ioLib.h&#34;#include &#34;taskLib.h&#34;#include &#34;sioLib.h&#34;#include &#34;sdLib.h&#34;#include &#34;semLib.h&#34;#include &#34;msgQLib.h&#34; char wbuf[] = &#34;hello&#34;; #define DEV_NAME &#34;/tyCo/2&#34; #define MAX_BUF_SIZE 20 #define SD_COMMDATA_NAME &#34;share_data&#34; #define SD_COMMDATA_MUTEX &#34;share_sem&#34; #define SHARE_DATA_LENGTH 20  typedef struct unix_clock_struct { UINT32 sec; /* ms */ UINT32 msec; /* s */ UINT8 quality; /* 时标质量 */ } UNIX_CLOCK_STRUCT; char *comdata; int set_serial(int fd); SEM_ID mutexComdata; void taskUart(void); int main(void) { int ret; int sdCommId; char r_buff[MAX_BUF_SIZE]; mutexComdata = semOpen(SD_COMMDATA_MUTEX, SEM_TYPE_MUTEX, SEM_FULL, SEM_Q_PRIORITY | SEM_DELETE_SAFE | \ SEM_INVERSION_SAFE, OM_CREATE | OM_DELETE_ON_LAST_CLOSE, NULL); if(mutexComdata == NULL) { /*致命错误，无法创建互斥锁*/ printf(&#34;ERROR TO OPEN SD_COMMDATA_MUTEX\n&#34;); taskExit(0); } /* 申请公共数据共享内存 */ sdCommId = sdOpen(SD_COMMDATA_NAME, SD_LINGER, OM_CREATE, SHARE_DATA_LENGTH, 0, SD_ATTR_RW|SD_CACHE_OFF, &amp;comdata); if(sdCommId == NULL) { /*致命错误，无法分配公共数据内存，报错退出*/ printf(&#34;ERROR TO OPEN SD_COMMDATA\n&#34;); taskExit(0); } if((ret = taskSpawn(&#34;taskUart&#34;,90,0x100, 20000, (FUNCPTR)taskUart,\ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)) &lt; 0) { printf(&#34;taskSpawn failed:ret = %s\n&#34;); } return 0; } void taskUart(void) { int ret; int fd = -1; UNIX_CLOCK_STRUCT w_buff; if((fd = open(DEV_NAME, O_RDWR,0)) &lt; 0) { printf(&#34;open %s failed.\n&#34;,DEV_NAME); } /*配置串口参数*/ if((ret = set_serial(fd)) &lt; 0) { printf(&#34;ret = %d\nset_serial failed.\n&#34;); } while(1) { semRTake(mutexComdata,WAIT_FOREVER); #if 0/*清空输入输出缓冲*/ if((ret = ioctl(fd, FIOFLUSH, 0))&lt;0) { printf(&#34; ret = %d\nset FIOFLUSH failed.\n&#34;,ret); } memset(r_buff,0,sizeof(r_buff)); /*读取串口中的值*/ if((ret = read(fd,r_buff,sizeof(r_buff)))&lt;0) { printf(&#34;ret = %d:read %s failed.\n&#34;,ret,DEV_NAME); } else printf(&#34;Received:%s\n&#34;,r_buff); #endif  #if 1  /*清空输入输出缓冲*/ if((ret = ioctl(fd, FIOFLUSH, 0))&lt;0) { printf(&#34; ret = %d\nset FIOFLUSH failed.\n&#34;,ret); } if(NULL == bzero(&amp;w_buff,sizeof(w_buff))) { printf(&#34;memset failed.\n&#34;); } if(NULL == memcpy(&amp;w_buff,comdata,sizeof(w_buff))) { printf(&#34;memset failed.\n&#34;); } if(&amp;w_buff != NULL) { /*往串口中写值*/ if((ret = write(fd, &amp;w_buff.sec, sizeof(ret)))&lt;0) // if((ret = write(fd, wbuf, sizeof(wbuf)))&lt;0)  { printf(&#34;ret = %d:write %s failed.\n&#34;,ret,DEV_NAME); } else { printf(&#34;write success:%d\n&#34;,w_buff.sec); } } semGive(mutexComdata); #endif  taskDelay(sysClkRateGet()*2); } } int set_serial(int fd) { int error = -1; int ok = 0; int ret; if(fd&lt;0) { printf(&#34;error:fd is %d\n&#34;,fd); } /*设定波特率为9600*/ if((ret = ioctl(fd, FIOBAUDRATE, 9600))&lt;0) { printf(&#34;ret = %d\nset baudrate failed\n&#34;,ret); return error; } /*设定：数据位为8，无奇偶校验，1位停止位*/ /*CLOCAL:忽略modem控制信号 * CREAD：启动接收器 * CS8:设定数据位为8*/ if((ret = ioctl(fd, SIO_HW_OPTS_SET,CREAD|CS8 |CLOCAL))&lt;0) { printf(&#34;ret = %d\nset SIO_HW_OPTS_SET failed.\n&#34;); return error; } return ok; } ]]></content>
  </entry>
  
  <entry>
    <title>针对VxWorks的QT 5.15.10发布了</title>
    <url>/post/vxworks/qt-5-15-10-for-vxworks-released.html</url>
    <categories><category>VxWorks</category>
    </categories>
    <tags>
      <tag>VxWorks</tag>
      <tag>QT</tag>
      <tag>图像</tag>
    </tags>
    <content type="html"><![CDATA[Qt是一个多平台的C++图形用户界面应用程序框架。它提供给应用程序开发者建立艺术级的图形用户界面所需的所用功能。Qt是完全面向对象的编程，所以具有易扩展和组件编程的优势。
相关的VxWorks 和 QT的文章，请参阅 VxWorks 6.8操作系统下QT的安装设置和运行方法  。
我们非常激动地发布了支持VxWorks的QT 5.15.10 支持VxWorks的Qt 5.15.10长期支持的商业发行是基于我们最新的QT 5.15.10(LTS)之上的源代码发布。这个发行从早期的QT 5的版本官方升级了针对VxWorks的QT支持，这是对诸如航空和国防以及医疗等行业的市场需求的积极回应。它提供了QT版本的升级同时也提供了VxWorks系统具体的问题解决，还有别的一些改进。
这次发行支持基于iMX6硬件的Ubuntu主机，我们也同时在准备基于x68和基于Windows主机的支持。此次的发行包开放给拥有QT账户的客户，也同时通过git仓库的形式开放给具有商业许可证的客户，请和我们联系以获取更多细节。
从这儿开始 这儿有关于安装和配置的独立的文章，要获取更多关于QT和支持VxWorks的QT的详细信息，请查看这儿关于QT 5.15的在线文档。
   https://doc.qt.io/qt-5/vxworks.html  
   https://doc.qt.io/qt-5/index.html  
   https://wiki.qt.io/Getting_Commercial_Qt_Sources  
 ]]></content>
  </entry>
  
  <entry>
    <title>北南南北</title>
    <url>/about.html</url>
    <categories>
    </categories>
    <tags>
    </tags>
    <content type="html"><![CDATA[北南南北 是众多使用 VxWorks 嵌入式实时操作系统的网友分享经验的平台，为的就是让 VxWorks 的学习和应用变得相对开放一些，在此也欢迎你的加入！
我们的愿景 技术创新是技术持续发展的生命力，紧跟技术的发展趋势，研究最新的技术，保持对新技术的热情和好奇心，让技术为生产和生活服务。
使用反馈  加入 VxWorks Club   或 Google AI TPU     欢迎你的加入
 ]]></content>
  </entry>
  
  <entry>
    <title>VxWorks实时性能探究</title>
    <url>/post/vxworks/vxworks-real-time-feature-explore.html</url>
    <categories><category>VxWorks</category>
    </categories>
    <tags>
      <tag>VxWorks</tag>
      <tag>实时性</tag>
    </tags>
    <content type="html"><![CDATA[ VxWorks操作系统是一款硬实时操作系统，一直听闻其实时性能非常优秀，但是一直没有一个直观地概念。
 笔者最近在使用 VxWorks  , 由大名鼎鼎的风河（WindRiver）开发。本篇文章就是将VxWorks操作系统和市面上几种其他实时操作系统的实时性能进行对比。
前期知识准备 实时性能和响应时间有关，为此，先对计算机操作系统中的时间概念和时间尺度进行一下介绍。
1 s = 1000 ms = 1000000 us = 1000000000 ns，看不出来1 s时间还是很长的嘛
  时钟周期：主频为4 GHz的CPU的时钟周期为1/4G = 0.25 ns，时钟周期是计算机中最基本的、最小的[时间单位。在一个时钟周期内，CPU仅完成一个最基本的动作。
  CPU周期：CPU周期亦称机器周期，一条指令执行过程被划分为若干阶段，每一阶段完成所需时间。完成一个基本操作所需要的时间称为机器周期。通常用内存中读取一个指令字的最短时间来规定CPU周期。
  指令周期：取出并执行一条指令的时间。想要详细了解可以看这篇文章【浅析】CPU中的指令周期、CPU周期和时钟周期
  内存时钟周期：相比CPU，一般的DDR内存芯片速率仅为400 MHz，时钟周期达2.5 ns, 再加上总线延时，导致内存访问时间达到几十纳秒。CPU运行速率与内存访问速率比大致为100：1。
  硬盘读取时间：硬盘的读写速度就更慢了，一般的机械硬盘的完成一次读写所需要的时间，主要取决寻道时间+旋转时间，完成一次读或者写的时间量级大致为ms级别，因此内存访问速率与磁盘存取速度比大致为1000:1。
  上面是有关硬件方面的时间周期情况，对于操作系统或者应用程序来说，我们一般关注的是算法的时间复杂度和空间复杂度，这是从整理理想的情况来衡量一个算法的优劣。如果想要详细了解每条代码的执行所耗时间，我们需要更深入了解代码是怎么在计算机上执行的。
C语言代码都是经过预处理、编译，产生汇编代码（汇编代码几乎已经接近机器码了），一句高级语言代码相当于汇编语言的几行甚至几十行。而学过汇编语言的都应该知道，不同的汇编代码指令执行所耗费的时间也是不同的。一般来说,移位,加法,取反这种指令只需要一个时钟周期,而乘法,除法等指令需要几个乃至几十个时钟周期执行。
实时操作系统（RTOS）的实时性能评价指标 实时操作系统的实时性能评价指标一般有两个：
 任务切换时间  当多任务应用程序运行在操作系统上时，它把正在运行的任务的状态保存到任务自己的栈区之中，然后把下一个将要运行的任务的当前状态从该任务的栈区装入CPU的寄存器，并开始这个任务的执行，这个过程就叫做任务切换。
 中断响应时间  计算机接收到中断信号到操作系统做出响应，并完成切换转入中断服务程序的时间。
下图是几种实时操作系统的实时性能对比：
可以看出不管是任务切换时间还是中断响应，VxWorks都是最好的，当然VxWorks也是最贵的。
此外我们还可以看出不管是任务切换还是中断响应，时间尺度都是在几个us，根据CPU主频的不同，大概是几千个时钟周期的样子。 下面代码是测试执行100万次简单循环语句所耗费的时间:
int i = 1000000; int j = 0; while(i){ j += 0; i--; } timer = 2033 us //执行100万次该循环所耗时间，可以将执行每次的时间和任务切换的时间进行对比 ]]></content>
  </entry>
  
  <entry>
    <title>风河公司的资本交易历史</title>
    <url>/post/vxworks/windriver-capital-transaction.html</url>
    <categories><category>VxWorks</category>
    </categories>
    <tags>
      <tag>WindRiver</tag>
    </tags>
    <content type="html"><![CDATA[日前，安波福宣布同意以43亿美元现金从私募股权公司TPG Capital收购风河公司（ Wind River  ），以帮助其在多个行业的关键软件领域建立独特地位，继续其智能转型，向边缘支持、软件定义的未来迈进。
该交易预计将于2022年年中完成，在被收购之后，风河将隶属于安波福主动安全与用户体验事业部，继续在公司总裁兼首席执行官Kevin Dallas的领导下作为独立业务单位运营。
实时操作系统 作为实时操作系统领域，全球最优秀的选手，它值得我们所有的溢美之词，无论怎么夸它，都不过分。
VxWorks是风河公司推出的实时多任务操作系统（RTOS）。过去40年间，风河和VxWorks在嵌入式OS领域一直处于领先地位，在航空航天、通信、工业控制等行业有着广泛的应用，在业内被称为嵌入式OS的常青树。
风河公司目前有2个嵌入式OS平台：Linux和VxWorks。
VxWorks是由支持多核、32/64位嵌入式处理器、内存包含和内存管理的VxWorks 6.x和VxWorks5.x，Workbench开发工具（包括多种C/C++编译器和调试器），连接组件（USB、IPv4/v6、多种文件系统等），先进的网络协议和图像多媒体等模块组成。除了通用平台外，VxWorks还包括支持工业、网络、医疗和消费电子等的特定平台产品。
老当益壮 风河成立于1981年，2021年收入大约4亿美元，毛利率超过80%。
1987年风河基于VRTX推出VxWorks，1993年IPO上市，1995年VxWorks在NASA Clementine月球探测器上，被发射入太空。
1997年NASA火星探险者号飞船的实时操作系统，登陆火星。
风河是全球第一大嵌入式RTOS厂家，也是全球第一大嵌入式Linux厂家，硬实时操作系统长达30年的霸主，市场占有率超30%。
它的主要收入来自4个领域：
 宇航与国防 工业与医疗 电信 汽车  宇航与国防所占比例最高，接近50%，各种飞船或者说航天飞行器基本都是风河VxWorks的市场，SpaceX也是它的忠实用户，中国神舟系列的SpaceOS也有借鉴VxWorks653。
除了航天飞行器，AH-64阿帕奇武装直升机、F-16V（全球空军主力机型）、F-18大黄蜂，B-2战略轰炸机，X-47A，波音787都是VxWorks。
美国的F-22猛禽、F-35、B-52轰炸机、B-1B轰炸机、C-17运输机和F-16改进型，以及欧洲的A-400M运输机，X-47B无人机，还有民航空客的A380，爱国者防空导弹，都是Vxworks的忠实用户。
把竞争对手买下来，然后干掉！
1999年风河收购一个主要竞争对手，pSOS的发明者，一家集成系统公司。从那以后风河公司不再支持pSOS产品线，并推荐现存的pSOS客户转向VxWorks。
2004年针对网络和通信市场，推出便携的Linux平台，正式进军嵌入式Linux市场。
VxWorks通过了汽车领域最高的ASIL-D级认证，以及远超汽车标准的DO-178C A级认证，它也通过了，已经准备好了对汽车行业进行降维打击。  卖来卖去 2009年英特尔以8.84亿美元收购风河；
2018年4月英特尔出售风河给投资公司TPG。
英特尔刚刚收购4年不到，就卖给了TPG，英特尔也是颇具渣男属性了。
不过，风河公司貌似还不是最后一个被卖来卖去的此类企业，另外一个汽车级嵌入式系统的大牛供应商，Green Hills也在被卖的路上了，我们接下来的文章会保持对它的追踪，及时报道相关信息。
戳穿实时操作系统 在日常的HIL测试工作中，几乎没有哪个测试任务是因为“实时仿真机的实时性不够高”而导致出问题。
HIL工作最容易出问题的地方，往往是功能定义不明确、工具链不完整、协同自动化测试做不起来、线束掉链子以及项目上各种瞎搞等等。
换句话说，在汽车HIL测试领域，哪怕是最low逼的实时操作系统，也足够了，人家Vector公司用wince做实时机，照样玩得飞起，不耽误事。
HIL实时机诞生的历史环境，已经不复存在了，当年的PC机真是太鸡肋了。
而且，在近些年大火的自动驾驶测试领域，我见过太多实时性差得一批的测试系统，响应滞后得跟PID调节似的，却闭口不谈实时性问题，忽忽悠悠就验收通过了……
资本市场 通过这些收购案例，我们也能看到资本唯利是图的本性，什么来钱快干什么，脑子一热就买了，兴奋劲儿过了之后又卖了，不符合自己的产品路线，也照样卖掉。
]]></content>
  </entry>
  
  <entry>
    <title>Mermaid支持流程图</title>
    <url>/post/mermaid-charts.html</url>
    <categories><category>示例</category>
    </categories>
    <tags>
      <tag>流程图</tag>
      <tag>时序图</tag>
    </tags>
    <content type="html"><![CDATA[本主题已支持 Mermaid 实现以纯文本的方式绘制流程图、序列图、甘特图、状态图、关系图行等等，随着 Mermaid 也在逐步发展，后续还会有各种各样的图被引入进来，更多的类型及使用方式可关注其官方网站： https://mermaid-js.github.io/  。
使用说明  通过 hugo new 命令创建一篇新的文章 在文章头部配置 mermaid: true 使用短代码书写各种类型的图，自带2个参数： align（对齐） 和 bc（背景色），可参考如下使用示例   流程图 {{&lt; mermaid align=&#34;left&#34; &gt;}} graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; {{&lt; /mermaid &gt;}} graph TD; A--B; A--C; B--D; C--D;  时序图 {{&lt; mermaid bc=&#34;#eee&#34; &gt;}} sequenceDiagram participant Alice participant Bob Alice-&gt;&gt;John: Hello John, how are you? loop Healthcheck John-&gt;&gt;John: Fight against hypochondria end Note right of John: Rational thoughts &lt;br/&gt;prevail! John--&gt;&gt;Alice: Great! John-&gt;&gt;Bob: How about you? Bob--&gt;&gt;John: Jolly good! {{&lt; /mermaid &gt;}} sequenceDiagram participant Alice participant Bob Alice-John: Hello John, how are you? loop Healthcheck John-John: Fight against hypochondria end Note right of John: Rational thoughts prevail! John--Alice: Great! John-Bob: How about you? Bob--John: Jolly good!  类图 {{&lt; mermaid &gt;}} classDiagram Class01 &lt;|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --&gt; C2 : Where am i? Class09 --* C3 Class09 --|&gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 &lt;--&gt; C2: Cool label {{&lt; /mermaid &gt;}} classDiagram Class01 C2 : Where am i? Class09 --* C3 Class09 --| Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08  C2: Cool label  甘特图 {{&lt; mermaid &gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d {{&lt; /mermaid &gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d  实体关系图 {{&lt; mermaid &gt;}} erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses {{&lt; /mermaid &gt;}} erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses  用户旅程 {{&lt; mermaid &gt;}} journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 5: Me {{&lt; /mermaid &gt;}} journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 5: Me ]]></content>
  </entry>
  
  <entry>
    <title>数学公式渲染</title>
    <url>/post/math-formula.html</url>
    <categories><category>示例</category>
    </categories>
    <tags>
      <tag>数学公式</tag>
      <tag>mathjax</tag>
      <tag>katex</tag>
    </tags>
    <content type="html"><![CDATA[本主题支持 mathjax 和 katex 两种不的方案支持数学公式的渲染，可根据自已的需求进行选择。
接下的示例中，将使用 MathJax   方案来展示渲染效果。
 使用 hugo new 命令创建一篇新的文章 可以全局启用数据公式渲染，请在项目配置参数 math: katex 或 math: mathjax 或是将该参数配置到需要显示数学公式的页面头部（减少不必要的加载消耗）   注意： 使用 支持的TeX功能  的联机参考资料。
例子 重复的分数 $$ \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} \equiv 1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}} {1+\frac{e^{-6\pi}} {1+\frac{e^{-8\pi}} {1+\cdots} } } } $$
总和记号 $$ \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) $$
几何级数之和 我把接下来的两个例子分成了几行，这样它在手机上表现得更好。这就是为什么它们包含 \displaystyle。
$$ \displaystyle\sum_{i=1}^{k+1}i $$
$$ \displaystyle= \left(\sum_{i=1}^{k}i\right) +(k+1) $$
$$ \displaystyle= \frac{k(k+1)}{2}+k+1 $$
$$ \displaystyle= \frac{k(k+1)+2(k+1)}{2} $$
$$ \displaystyle= \frac{(k+1)(k+2)}{2} $$
$$ \displaystyle= \frac{(k+1)((k+1)+1)}{2} $$
乘记号 $$ \displaystyle 1 + \frac{q^2}{(1-q)}+\frac{q^6}{(1-q)(1-q^2)}+\cdots = \displaystyle \prod_{j=0}^{\infty}\frac{1}{(1-q^{5j+2})(1-q^{5j+3})}, \displaystyle\text{ for }\lvert q\rvert &lt; 1. $$
随文数式 这是一些线性数学: $$ k_{n+1} = n^2 + k_n^2 - k_{n-1} $$ ， 然后是更多的文本。
希腊字母 $$ \Gamma\ \Delta\ \Theta\ \Lambda\ \Xi\ \Pi\ \Sigma\ \Upsilon\ \Phi\ \Psi\ \Omega \alpha\ \beta\ \gamma\ \delta\ \epsilon\ \zeta\ \eta\ \theta\ \iota\ \kappa\ \lambda\ \mu\ \nu\ \xi \ \omicron\ \pi\ \rho\ \sigma\ \tau\ \upsilon\ \phi\ \chi\ \psi\ \omega\ \varepsilon\ \vartheta\ \varpi\ \varrho\ \varsigma\ \varphi $$
箭头 $$ \gets\ \to\ \leftarrow\ \rightarrow\ \uparrow\ \Uparrow\ \downarrow\ \Downarrow\ \updownarrow\ \Updownarrow $$
$$ \Leftarrow\ \Rightarrow\ \leftrightarrow\ \Leftrightarrow\ \mapsto\ \hookleftarrow \leftharpoonup\ \leftharpoondown\ \rightleftharpoons\ \longleftarrow\ \Longleftarrow\ \longrightarrow $$
$$ \Longrightarrow\ \longleftrightarrow\ \Longleftrightarrow\ \longmapsto\ \hookrightarrow\ \rightharpoonup $$
$$ \rightharpoondown\ \leadsto\ \nearrow\ \searrow\ \swarrow\ \nwarrow $$
符号 $$ \surd\ \barwedge\ \veebar\ \odot\ \oplus\ \otimes\ \oslash\ \circledcirc\ \boxdot\ \bigtriangleup $$
$$ \bigtriangledown\ \dagger\ \diamond\ \star\ \triangleleft\ \triangleright\ \angle\ \infty\ \prime\ \triangle $$
微积分学 $$ \int u \frac{dv}{dx},dx=uv-\int \frac{du}{dx}v,dx $$
$$ f(x) = \int_{-\infty}^\infty \hat f(\xi),e^{2 \pi i \xi x} $$
$$ \oint \vec{F} \cdot d\vec{s}=0 $$
洛伦茨方程 $$ \begin{aligned} \dot{x} &amp; = \sigma(y-x) \\ \dot{y} &amp; = \rho x - y - xz \\ \dot{z} &amp; = -\beta z + xy \end{aligned} $$
交叉乘积 这在KaTeX中是可行的，但在这种环境中馏分的分离不是很好。
$$ \mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix} \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\ \frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp; 0 \\ \frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp; 0 \end{vmatrix} $$
这里有一个解决方案:使用“mfrac”类(在MathJax情况下没有区别)的额外类使分数更小:
$$ \mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix} \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\ \frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp; 0 \\ \frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp; 0 \end{vmatrix} $$
强调 $$ \hat{x}\ \vec{x}\ \ddot{x} $$
有弹性的括号 $$ \left(\frac{x^2}{y^3}\right) $$
评估范围 $$ \left.\frac{x^3}{3}\right|_0^1 $$
诊断标准 $$ f(n) = \begin{cases} \frac{n}{2}, &amp; \text{if } n\text{ is even} \\ 3n+1, &amp; \text{if } n\text{ is odd} \end{cases} $$
麦克斯韦方程组 $$ \begin{aligned} \nabla \times \vec{\mathbf{B}} -, \frac1c, \frac{\partial\vec{\mathbf{E}}}{\partial t} &amp; = \frac{4\pi}{c}\vec{\mathbf{j}} \\ \nabla \cdot \vec{\mathbf{E}} &amp; = 4 \pi \rho \\ \nabla \times \vec{\mathbf{E}}, +, \frac1c, \frac{\partial\vec{\mathbf{B}}}{\partial t} &amp; = \vec{\mathbf{0}} \\ \nabla \cdot \vec{\mathbf{B}} &amp; = 0 \end{aligned} $$
统计学 固定词组：
$$ \frac{n!}{k!(n-k)!} = {^n}C_k {n \choose k} $$
分数在分数 $$ \frac{\frac{1}{x}+\frac{1}{y}}{y-z} $$
ｎ次方根 $$ \sqrt[n]{1+x+x^2+x^3+\ldots} $$
矩阵 $$ \begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13}\\ a_{21} &amp; a_{22} &amp; a_{23}\\ a_{31} &amp; a_{32} &amp; a_{33} \end{pmatrix} \begin{bmatrix} 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 \end{bmatrix} $$
标点符号 $$ f(x) = \sqrt{1+x} \quad (x \ge -1) f(x) \sim x^2 \quad (x\to\infty) $$
现在用标点符号:
$$ f(x) = \sqrt{1+x}, \quad x \ge -1 f(x) \sim x^2, \quad x\to\infty $$
]]></content>
  </entry>
  
  <entry>
    <title>支持用户自定义设计</title>
    <url>/post/custom-files.html</url>
    <categories><category>示例</category>
    </categories>
    <tags>
      <tag>自定义</tag>
      <tag>个性化</tag>
      <tag>布局</tag>
    </tags>
    <content type="html"><![CDATA[对于熟悉前端开发的用户来说，可以通过自定义文件配置，实现对站点的样式和布局进行个性化的调整。其中布局方面主要是支持左侧边栏的站点概览部分，以及站点底部2个位置，但样式的重置可以是整个站点的任意位置。
打开配置参数 首先要明确在配置文件的 params 区域中有配置如下参数：
customFilePath: sidebar: custom_sidebar.html footer: custom_footer.html style: /css/custom_style.css 注意： sidebar 和 footer 的文件命名不可以与它们的参数名称相同，不然会影响系统默认的布局设计，切记！！！ 😄  然后在站点的根目录下创建 layouts/partials 2个目录，用于存放自定布局设计文件，另外在站点根目录下创建 statics/css 2个目录，用于存放自定义 CSS 样式文件。一切就绪后，就可以参考如下的步骤，完成自己的设计想法。
侧边栏设计 在前面创建 partials 目录中新一个后缀名为 html 的文件，可以在里面书写你所想表达的设计或内容，比如引入一些第三方组件内容。示例如下：
&lt;div class=&#34;mydefined animated&#34; itemprop=&#34;custom&#34;&gt; &lt;span&gt;支持自定义CSS和Sidebar布局啦💄💄💄&lt;/span&gt; &lt;/div&gt; 再把该文件的路径配置到相应的参数中，效果请查看左侧边栏底部的效果。
底部设计 在前面创建 partials 目录中新一个后缀名为 html 的文件，可以在里面书写你所想表达的设计或内容，比如引入一些第三方组件内容。示例如下：
&lt;div class=&#34;custom-footer&#34;&gt; Website source code &lt;a href=&#34;https://github.com/hugo-next/hugo-theme-next/tree/develop/exampleSite/layouts/partials/custom-footer.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; &lt;/div&gt; 再把该文件的路径配置到相应的参数中，效果请查看站点底部的效果。
自定义样式 在前面创建 css 目录中新一个后缀名为 css 的文件，然后可以在里面把站点的样式进行重定义，或是增加一些自己定义的样式设计，在写文章时进行引用，示例如下：
.custom-head5 { font-size: 1.2em; color: #ed6c24; font-weight: bold; } 再把该文件的路径配置到相应的参数中，效果参考如下：
我是自定义的标题样式效果!!!
]]></content>
  </entry>
  
  <entry>
    <title>自定义短语示例</title>
    <url>/post/shortcodes.html</url>
    <categories><category>示例</category>
    </categories>
    <tags>
      <tag>短代码</tag>
      <tag>语法</tag>
    </tags>
    <content type="html"><![CDATA[虽然 Markdown 语法已经非常丰富能够满足我们写文章的绝大部分需求，但是为更好的对文章内容进行更友好的排版，为引设计一套自定义的短语，便于在使用时能够快速引用。
块引用 在引用一些经典名言名句时，可以采用此短语，语法参考如下：
{{&lt; quote &gt;}} ### block quote 写下你想表达的话语！ {{&lt; /quote &gt;}} 实际效果：
希望是无所谓有，无所谓无的，这正如地上的路。
其实地上本没有路，走的人多了，也便成了路。
鲁迅
 信息块 支持 default，info，success，warning，danger 等五种不同效果的展示，语法参考如下：
{{&lt; note [class] [no-icon] &gt;}} 书写表达的信息 支持 Markdown 语法 {{&lt; /note &gt;}} 实际效果：
Default Header without icon Welcome to Hugo NexT!  Default Header Welcome to Hugo NexT!  Info Header Welcome to Hugo NexT!  Success Header Welcome to Hugo NexT!  Warning Header Welcome to Hugo NexT!  Danger Header Welcome to Hugo NexT! ]]></content>
  </entry>
  
  <entry>
    <title>文章目录导航</title>
    <url>/post/table-of-content.html</url>
    <categories><category>示例</category>
    </categories>
    <tags>
      <tag>目录</tag>
      <tag>导航</tag>
      <tag>博客</tag>
    </tags>
    <content type="html"><![CDATA[巴顿将军说过：“衡量一个人是否成功，不是看他站到顶峰，而是从顶峰跌落之后的反弹力”，褚时健的人生便是如此，中年发家致富，名利双收，之后又跌落到谷底，等到74岁再创业，10年后带着褚橙归来，东山再起收获亿万财富，他的发展轨迹就是反弹的过程。
早年的故事 起始 2014年的春天，在云南省华宁县和宜良县的交界处，一座名叫矣则的小山村里，一处已经有上百年历史的古旧四合院宅子被拆掉。村委会正带领村民们进行“美丽乡村”的建设，一年以后，旧有村居将再也看不到，代之而起的是钢筋混凝土的新式民居。就像10年、20年前中国大小城市的改造一样，这个群山围绕的小村子也开始陷入“工地模式”。
童年浪花 在江河边长大的孩子几乎都有一个当仁不让的特长：善水。褚时健也不例外，他不仅从小就在南盘江和花鱼塘里扑腾出了上佳的游泳技术，五六岁已经可以一个猛子扎出老远，而且从七八岁就可以在南盘江和河滩上的鱼塘里捉鱼了。
少年故事 褚时健在乡村自由自在生活的十多年，其实正是中国社会风雨飘摇的十多年。特别是1937年卢沟桥事变后，日本人发动全面侵华战争，短短两三年间，中国的大部分国土相继沦陷
激情的青春十年 当上了游击队员 1948年夏天，褚时健回乡，在禄丰车站小学做了一名老师，同时也和褚时仁、褚时杰一起继续保持与共产党组织的联系，做一些传递情报的工作
战火纷飞 因为战斗力相较悬殊，所以游击队只能是靠打一枪换一个地方的办法，专找敌人薄弱的地方攻击，但更多时候，都是在防御和转移阵地。
迎来解放 1949年12月，国民党云南省主席卢汉在昆明宣布起义，云南正式拉开解放的序幕。1950年2月20日，陈赓、宋任穷、周保中率解放军第二野战军第四兵团进入昆明，24日，陈赓宣布云南全境解放。
生活的断层 跌入生活底层 “反右”运动中被打倒的人在“右派”身份确定后，只有一条路可走：下放到农场。农场名副其实，就是干农活儿的地方，必须过和农民一样的生活。
尾声 岁月像一条河 2015年，是褚时健和马静芬结婚60周年，被称为“钻石婚”的纪念年份。这简直是一份人生的奖赏,在中国离婚率愈益升高的当下，60年的婚姻，几乎就像一个前世之梦。一个甲子的相伴相随，褚时健和马静芬共同经历了国家和个人的各种风浪，共同面对过生死。他们两人已经不仅是夫妻，更是一对战友。尽管马静芬偶尔会对褚时健年轻时候的粗心抱怨上两句，但说到最后，她会说一句：“没有我就没有他，没有他也就没有我。”
作者致谢 这本书从2014年初夏开始采访，到今天完稿，历时18个月。封面上“作者”只能是我一个人的名字，但也只有我自己知道，这本书，包含了太多人的心力和体力。我当然首先要致谢王石先生，没有他就没有这本书。我自己细想下来，没有王石先生一直的鞭策和鼓励，也没有我写作工作的今天。从2006年我开始从事专业写作工作以来，他给我创造了很多写作的机会，并且不吝自己诸多人生和学习的体会和感悟，一一传递予我。知遇之恩，感谢非常。
最后，我当然要把最大的感谢致予褚时健先生。不仅是因为他慷慨、坦率面对我的各种提问，更重要的是，在倾听他的故事的过程里，他繁盛的人生经历，他的强大生命力，他对生活、对事业的一片赤子之心，也丰富了我对自己人生的思考。
]]></content>
  </entry>
  
  <entry>
    <title>Hugo 内置的 Chroma 语法高亮</title>
    <url>/post/syntax-highlighting.html</url>
    <categories><category>示例</category>
    </categories>
    <tags>
      <tag>语法</tag>
      <tag>高亮</tag>
      <tag>Chroma</tag>
    </tags>
    <content type="html"><![CDATA[Hugo 通过 Chroma 提供非常快速的语法高亮显示，现 Hugo 中使用 Chroma 作为代码块高亮支持，它内置在 Go 语言当中，速度是真的非常、非常快，而且最为重要的是它也兼容之前我们使用的 Pygments 方式。
以下通过 Hugo 内置短代码 highlight 和 Markdown 代码块方式分别验证不同语言的代码块渲染效果并能正确高亮显示，有关优化语法突出显示的更多信息，请参阅 Hugo 文档  。
编程语言 GO 199 200 201 202 203 204 205 206 207 208  func GetTitleFunc(style string) func(s string) string { switch strings.ToLower(style) { case &#34;go&#34;: return strings.Title case &#34;chicago&#34;: return transform.NewTitleConverter(transform.ChicagoStyle) default: return transform.NewTitleConverter(transform.APStyle)  } }   Java import javax.swing.JFrame; //Importing class JFrame import javax.swing.JLabel; //Importing class JLabel public class HelloWorld { public static void main(String[] args) { JFrame frame = new JFrame(); //Creating frame  frame.setTitle(&#34;Hi!&#34;); //Setting title frame  frame.add(new JLabel(&#34;Hello, world!&#34;));//Adding text to frame  frame.pack(); //Setting size to smallest  frame.setLocationRelativeTo(null); //Centering frame  frame.setVisible(true); //Showing frame  } } Python print &#34;Hello, world!&#34; Git 对比 *** /path/to/original &#39;&#39;timestamp&#39;&#39; --- /path/to/new &#39;&#39;timestamp&#39;&#39; *************** *** 1 **** ! This is a line. --- 1 --- ! This is a replacement line. It is important to spell -removed line +new line *** /path/to/original &#39;&#39;timestamp&#39;&#39; --- /path/to/new &#39;&#39;timestamp&#39;&#39; *************** *** 1 **** ! This is a line. --- 1 --- ! This is a replacement line. It is important to spell -removed line +new line 文件 Make 文件 CC=gcc CFLAGS=-I. hellomake: hellomake.o hellofunc.o $(CC) -o hellomake hellomake.o hellofunc.o -I. Markdown 文档 **bold** *italics* [link](www.example.com) 数据内容 JSON 数据 {&#34;employees&#34;:[ {&#34;firstName&#34;:&#34;John&#34;, &#34;lastName&#34;:&#34;Doe&#34;}, ]} XML 内容 &lt;employees&gt; &lt;employee&gt; &lt;firstName&gt;John&lt;/firstName&gt; &lt;lastName&gt;Doe&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt; SQL 查询 SELECT column_name,column_name FROM Table WHERE column_name = &#34;condition&#34; 除以上列举的代码高亮显示外，还支持诸如：C 语言、C++、HTML、CSS、Shell脚本等各主流的代码语言高亮显示，可自行测试效果。
]]></content>
  </entry>
  
  <entry>
    <title>支持 Emoji 表情</title>
    <url>/post/emoji-support.html</url>
    <categories><category>示例</category>
    </categories>
    <tags>
      <tag>表情</tag>
      <tag>emoji</tag>
    </tags>
    <content type="html"><![CDATA[Emoji 可以通过多种方式在 Hugo 项目中启用。
 emojify   方法可以直接在模板中调用, 或者使用 行内 Shortcodes  .
要全局使用 emoji, 需要在你的 网站配置  中设置 enableEmoji 为 true， 然后你就可以直接在文章中输入 emoji 的代码。
它们以冒号开头和结尾，并且包含 emoji 的 代码：
去露营啦! {:}tent: 很快就回来. 真开心! {:}joy: 呈现的输出效果如下:
去露营啦! ⛺ 很快就回来。
真开心! 😂
以下符号清单是 emoji 代码的非常有用的参考。
表情与情感 笑脸表情    图标 代码 图标 代码     😀 grinning 😃 smiley   😄 smile 😁 grin   😆 laughing satisfied 😅 sweat_smile   🤣 rofl 😂 joy   🙂 slightly_smiling_face 🙃 upside_down_face   😉 wink 😊 blush   😇 innocent      爱意表情    图标 代码 图标 代码     😍 heart_eyes 😘 kissing_heart   😗 kissing ☺️ relaxed   😚 kissing_closed_eyes 😙 kissing_smiling_eyes    吐舌头表情    图标 代码 图标 代码     😋 yum 😛 stuck_out_tongue   😜 stuck_out_tongue_winking_eye 😝 stuck_out_tongue_closed_eyes   🤑 money_mouth_face      国家和地区旗帜    图标 代码 图标 代码     🇦🇩 andorra 🇦🇪 united_arab_emirates   🇦🇫 afghanistan 🇦🇬 antigua_barbuda   🇦🇮 anguilla 🇦🇱 albania   🇦🇲 armenia 🇦🇴 angola   🇦🇶 antarctica 🇦🇷 argentina   🇦🇸 american_samoa 🇦🇹 austria   🇦🇺 australia 🇦🇼 aruba   🇦🇽 aland_islands 🇦🇿 azerbaijan   🇧🇦 bosnia_herzegovina 🇧🇧 barbados   🇧🇩 bangladesh 🇧🇪 belgium   🇧🇫 burkina_faso 🇧🇬 bulgaria   🇧🇭 bahrain 🇧🇮 burundi   🇧🇯 benin 🇧🇱 st_barthelemy   🇧🇲 bermuda 🇧🇳 brunei   🇧🇴 bolivia 🇧🇶 caribbean_netherlands   🇧🇷 brazil 🇧🇸 bahamas   🇧🇹 bhutan 🇧🇼 botswana   🇧🇾 belarus 🇧🇿 belize   🇨🇦 canada 🇨🇨 cocos_islands   🇨🇩 congo_kinshasa 🇨🇫 central_african_republic   🇨🇬 congo_brazzaville 🇨🇭 switzerland   🇨🇮 cote_divoire 🇨🇰 cook_islands   🇨🇱 chile 🇨🇲 cameroon   🇨🇳 cn 🇨🇴 colombia   🇨🇷 costa_rica 🇨🇺 cuba   🇨🇻 cape_verde 🇨🇼 curacao   🇨🇽 christmas_island 🇨🇾 cyprus   🇨🇿 czech_republic 🇩🇪 de   🇩🇯 djibouti 🇩🇰 denmark   🇩🇲 dominica 🇩🇴 dominican_republic   🇩🇿 algeria 🇪🇨 ecuador   🇪🇪 estonia 🇪🇬 egypt   🇪🇭 western_sahara 🇪🇷 eritrea   🇪🇸 es 🇪🇹 ethiopia   🇪🇺 eu european_union 🇫🇮 finland   🇫🇯 fiji 🇫🇰 falkland_islands   🇫🇲 micronesia 🇫🇴 faroe_islands   🇫🇷 fr 🇬🇦 gabon   🇬🇧 gb uk 🇬🇩 grenada   🇬🇪 georgia 🇬🇫 french_guiana   🇬🇬 guernsey 🇬🇭 ghana   🇬🇮 gibraltar 🇬🇱 greenland   🇬🇲 gambia 🇬🇳 guinea   🇬🇵 guadeloupe 🇬🇶 equatorial_guinea   🇬🇷 greece 🇬🇸 south_georgia_south_sandwich_islands   🇬🇹 guatemala 🇬🇺 guam   🇬🇼 guinea_bissau 🇬🇾 guyana   🇭🇰 hong_kong 🇭🇳 honduras   🇭🇷 croatia 🇭🇹 haiti   🇭🇺 hungary 🇮🇨 canary_islands   🇮🇩 indonesia 🇮🇪 ireland   🇮🇱 israel 🇮🇲 isle_of_man   🇮🇳 india 🇮🇴 british_indian_ocean_territory   🇮🇶 iraq 🇮🇷 iran   🇮🇸 iceland 🇮🇹 it   🇯🇪 jersey 🇯🇲 jamaica   🇯🇴 jordan 🇯🇵 jp   🇰🇪 kenya 🇰🇬 kyrgyzstan   🇰🇭 cambodia 🇰🇮 kiribati   🇰🇲 comoros 🇰🇳 st_kitts_nevis   🇰🇵 north_korea 🇰🇷 kr   🇰🇼 kuwait 🇰🇾 cayman_islands   🇰🇿 kazakhstan 🇱🇦 laos   🇱🇧 lebanon 🇱🇨 st_lucia   🇱🇮 liechtenstein 🇱🇰 sri_lanka   🇱🇷 liberia 🇱🇸 lesotho   🇱🇹 lithuania 🇱🇺 luxembourg   🇱🇻 latvia 🇱🇾 libya   🇲🇦 morocco 🇲🇨 monaco   🇲🇩 moldova 🇲🇪 montenegro   🇲🇬 madagascar 🇲🇭 marshall_islands   🇲🇰 macedonia 🇲🇱 mali   🇲🇲 myanmar 🇲🇳 mongolia   🇲🇴 macau 🇲🇵 northern_mariana_islands   🇲🇶 martinique 🇲🇷 mauritania   🇲🇸 montserrat 🇲🇹 malta   🇲🇺 mauritius 🇲🇻 maldives   🇲🇼 malawi 🇲🇽 mexico   🇲🇾 malaysia 🇲🇿 mozambique   🇳🇦 namibia 🇳🇨 new_caledonia   🇳🇪 niger 🇳🇫 norfolk_island   🇳🇬 nigeria 🇳🇮 nicaragua   🇳🇱 netherlands 🇳🇴 norway   🇳🇵 nepal 🇳🇷 nauru   🇳🇺 niue 🇳🇿 new_zealand   🇴🇲 oman 🇵🇦 panama   🇵🇪 peru 🇵🇫 french_polynesia   🇵🇬 papua_new_guinea 🇵🇭 philippines   🇵🇰 pakistan 🇵🇱 poland   🇵🇲 st_pierre_miquelon 🇵🇳 pitcairn_islands   🇵🇷 puerto_rico 🇵🇸 palestinian_territories   🇵🇹 portugal 🇵🇼 palau   🇵🇾 paraguay 🇶🇦 qatar   🇷🇪 reunion 🇷🇴 romania   🇷🇸 serbia 🇷🇺 ru   🇷🇼 rwanda 🇸🇦 saudi_arabia   🇸🇧 solomon_islands 🇸🇨 seychelles   🇸🇩 sudan 🇸🇪 sweden   🇸🇬 singapore 🇸🇭 st_helena   🇸🇮 slovenia 🇸🇰 slovakia   🇸🇱 sierra_leone 🇸🇲 san_marino   🇸🇳 senegal 🇸🇴 somalia   🇸🇷 suriname 🇸🇸 south_sudan   🇸🇹 sao_tome_principe 🇸🇻 el_salvador   🇸🇽 sint_maarten 🇸🇾 syria   🇸🇿 swaziland 🇹🇨 turks_caicos_islands   🇹🇩 chad 🇹🇫 french_southern_territories   🇹🇬 togo 🇹🇭 thailand   🇹🇯 tajikistan 🇹🇰 tokelau   🇹🇱 timor_leste 🇹🇲 turkmenistan   🇹🇳 tunisia 🇹🇴 tonga   🇹🇷 tr 🇹🇹 trinidad_tobago   🇹🇻 tuvalu 🇹🇼 taiwan   🇹🇿 tanzania 🇺🇦 ukraine   🇺🇬 uganda 🇺🇸 us   🇺🇾 uruguay 🇺🇿 uzbekistan   🇻🇦 vatican_city 🇻🇨 st_vincent_grenadines   🇻🇪 venezuela 🇻🇬 british_virgin_islands   🇻🇮 us_virgin_islands 🇻🇳 vietnam   🇻🇺 vanuatu 🇼🇫 wallis_futuna   🇼🇸 samoa 🇽🇰 kosovo   🇾🇪 yemen 🇾🇹 mayotte   🇿🇦 south_africa 🇿🇲 zambia   🇿🇼 zimbabwe     ]]></content>
  </entry>
  
  <entry>
    <title>Markdown 语法支持</title>
    <url>/post/markdown-syntax.html</url>
    <categories><category>示例</category>
    </categories>
    <tags>
      <tag>Markdown</tag>
      <tag>语法</tag>
    </tags>
    <content type="html"><![CDATA[仅以此篇文章来测试下在 NexT 主题中在通过 Hugo 引擎来建站时，是否支持 Markdown 文件内容中所写的各种语法，并展示下实际的效果。
标题样式 让我们从所有可能的标题开始，在 HTML 中 &lt;h1&gt;-&lt;h6&gt;元素分别表示六个不同级别的标题样式，其中 &lt;h1&gt; 为最大标题，&lt;h6&gt;为最小标题，效果如下：
标题 1 标题 2 标题 3 标题 4 标题 5 标题 6 段落格式 根据 W3C  定义的 HTML5 规范  ，HTML 文档由元素和文本组成。每个元素的组成都由一个 开始标记  表示，例如： &lt;body&gt; ，和 结束标记  表示，例如： &lt;/body&gt; 。（某些开始标记和结束标记在某些情况下可以省略，并由其他标记暗示。） 元素可以具有属性，这些属性控制元素的工作方式。例如：超链接是使用 a 元素及其 href 属性形成的。
Markdown 语法 ![图像说明](图像地址) HTML IMG 标签 &lt;img src=&#34;图像地址&#34; width=&#34;宽度&#34; height=&#34;高度&#34; /&gt; SVG 格式 &lt;svg&gt;xxxxxx&lt;/svg&gt; 
列表类型 有序列表  第一个元素 第二个元素 第三个元素  无序列表  列表元素 另一个元素 和其它元素  嵌套列表 借助 HTML 的 ul 元素来实现。
 第一项 第二项  第二项第一个子项目 第二项第二个子项目  第二项第二分项第一分项 第二项第二分项第二分项 第二项第二分项第三分项   第二项第三个子项目  第二项第三分项第一分项 第二项第三分项第二分项 第二项第三分项第三分项    第三项  自定义列表 通过 HTML 的 dl 元素还支持自定义列表（表格列表）。
 Hugo 目录结构 assets config.toml content data theme static Hugo 模板 基础模板 列表模板 单页模板  块引用 blockquote 元素表示从另一个源引用的内容，可以选择引用必须在 footer 或 cite 元素中，也可以选择使用注释和缩写等行内更改。
 引用文本 这一行也是同样的引用 同样你也在 blockquote 中使用 Markdown 语法书写
 带有引文的 Blockquote 元素效果。
 我的目标不是赚大钱,是为了制造好的电脑。当我意识到我可以永远当工程师时，我才创办了这家公司。
— 史蒂夫·沃兹尼亚克  根据 Mozilla 的网站记录，Firefox 1.0 于 2004 年发布，并取得了巨大成功。
表格 表格并不算是 Markdown 的核心要素，但 Hugo 同样支持它。
   ID 创建者 模型 年份     1 Honda Accord 2009   2 Toyota Camry 2012   3 Hyundai Elantra 2010    可以使用 : （英文格式冒号）来对表格内容进行对齐。
   表格 可以是 很酷     左对齐 居中 右对齐   左对齐 居中 右对齐   左对齐 居中 右对齐    同样也可以在表格中使用 Markdown 语法。
   表格 中 使用 Markdown 语法     斜体 粗体 中划线 代码块    Code &lt;!DOCTYPE html&gt; &lt;html lang=&#34;en&#34;&gt; &lt;head&gt; &lt;meta charset=&#34;UTF-8&#34;&gt; &lt;title&gt;Example HTML5 Document&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Test&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; &lt;!DOCTYPE html&gt; &lt;html lang=&#34;en&#34;&gt; &lt;head&gt; &lt;meta charset=&#34;UTF-8&#34;&gt; &lt;title&gt;Example HTML5 Document&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Test&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; 其它元素： abbr、sub、sup、kbd等等 GIF 是位图图像格式。
H2O
C6H12O6
Xn + Yn = Zn
按X获胜。或按CTRL+ALT+F显示 FPS 计数器。
比特作为信息论中的信息单位，也被称为 shannon ，以信息论领域的创始人 Claude shannon 的名字命名。
参考：
 来自 Mainroad 主题的 Basic Elements   内容 ]]></content>
  </entry>
  
  <entry>
    <title>友情链接</title>
    <url>/flinks.html</url>
    <categories>
    </categories>
    <tags>
    </tags>
    <content type="html"><![CDATA[如想交换本站友情链接，请在评论区留下你的站点信息，格式参考如下：
- name: VxWorks俱乐部 desc: VxWorks实时操作系统 avatar: https://www.vxworks.net/images/vxworks-club-logo.png link: https://www.vxworks.net ]]></content>
  </entry>
  
</search>